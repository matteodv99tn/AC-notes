\chapter{Controllability and reachability of linear systems} \label{ch:controllability}
	Chapter \ref{ch:solutions} (page \pageref{ch:solutions}) described the \textbf{solutions} of \textbf{linear system} (in both continuous and discrete-time case as well as time variant or invariant), but what we want to understand now is \textit{what we can achieve} by the system.
	
	Given two times $t_1 > t_0 \geq 0$, we denote with $\reach[t_0,t_1]$ the \de{$[t_0,t_1]$-reachable subset} (or \textbf{subspace}), a linear subspace defined as
	\begin{equation}
		\reach[t_0,t_1] = \left\{ \x_1 \in \mathds R^n \ : \ \exists \u \textrm{ such that } \int_{t_0}^{t_1} \stm(t_1,\tau) \B(\tau) u(\tau)\, d\tau = \x_1 \right\}
	\end{equation}
	Intuitively this set contains all states $\x_1$ that can be achieved by a linear system considering a zero-state initial condition. Each different input, using the variation of constants formula (\ref{eq:sol:varconstantcontinuous}) at page \pageref{eq:sol:varconstantcontinuous}, might generate a  different final state $\x(t_1)$ and $\reach$ contains all possible values of those values. We observe that the \textbf{reachable subset} is a \textbf{linear subspace}: given in fact an input $\overline \u$ leading to a certain state $\overline \x_1$, then in order to reach the state $k \overline \x_1$ (with $k \in \mathds R$) all we need to do is to chose the input $\u = k \overline \u$.
	
	Conversely we denote with $\ctr[t_0,t_1]$ the \de{$[t_0,t_1]$ controllable set} (\textbf{subspace}), the linear subspace defined as
	\begin{equation}
		\ctr[t_0,t_1] = \left\{ \x_0 \in \mathds R^n \ : \ \exists \u \textrm{ such that } \stm(t_1,t_0)\x_0 + \int_{t_0}^{t_1} \stm(t_1,\tau) \B(\tau) \u(\tau)\, d\tau = 0 \right\}
	\end{equation}
	In this case the definition is \textit{revered} and the controllable set contains all possible initial states $\x_0$ that can be driven, through a specific input $\u(t)$, to a final state $\x_1$ that's zero: the definition contains in fact also the free response of the system that's affected by the initial condition only.\\
	Another equivalent definition of the controllable set can be obtained by pre-multiplying the variation of constants formula by $\stm(t_1,t_0)^{-1}$, leading to the definition
	\[ \ctr[t_0,t_1] = \left\{ \x_0 \in \mathds R^n \ : \ \exists \u \textrm{ such that } \int_{t_0}^{t_1} \stm(t_0,\tau) \B(\tau) \u(\tau)\, d\tau = -\x_0 \right\} \]
	
	We say that a \de{system} is \de{controllable} if the dimension of the controllable subspace coincides with the dimension of the states, so mathematically
	\begin{equation} \label{eq:gram:controllable}
		\textrm{controllable system} \qquad \Leftrightarrow \qquad \ctr[t_0,t_1] = \mathds R^n
	\end{equation}
	However all this discussion will be further clarified in section \ref{sec:controllablesystems}, page \pageref{sec:controllablesystems}.
	
	\paragraph{Linear algebra recall} In order to ease the following definition of \textit{Gramians}, a recall of concept of linear algebra is recommended. Given a matrix $\mat W \in \mathds R^{m\times n}$ we call \de{image} of $\mat W$ the linear subspace defined as
	\begin{equation}
		\image{\mat W} = \left\{ \y \in \mathds R^m \ : \ \y = \mat W \x \textrm{ with } \x \in \mathds R^n \right\} \subseteq \mathds R^m
	\end{equation}
	As an important note, we have that the \textbf{dimension} of the image of any matrix coincides with its \textbf{rank}, the number of linearly independent column vectors composing the matrix. We define the \de{kernel} of $\mat W$, also known as \textbf{null space}, the set defined as
	\begin{equation}
		\kernel{\mat W} = \left\{ \x \in \mathds R^n \ : \ \mat W \x = 0 \right\} \subseteq \mathds R^n
	\end{equation}
	In the following pages it will be often used the \de{fundamental theorem of linear algebra} stating that for any matrix $\mat W \in \mathds R^{m\times n}$ it holds
	\begin{equation}
		\dimension{\image {\mat W}} + \dimension{\kernel{\mat W}} = n
	\end{equation}
	Lastly, given a linear subspace $\mathds V \subseteq \mathds R^n$, we call \textbf{orthogonal complement} of $\mathds V$ the set defined as
	\[ \mathds V^\perp = \left\{ \x \in \mathds R^n \textrm{ such that } \x^T \z = 0 \ \forall \z \in \mathds V \right\} \]
	As a remark, for every $m\times n$ matrix $\mat W$ it holds that
	\[ \image{\mat W} = \big(\kernel{\mat W^T})^\perp \hspace{2cm} \kernel{\mat W} = \big(\image{\mat W^T}\big)^\perp \]
	
\section{Gramians}
	Given two times $t_1 > t_0 \geq 0$, we denote with $\rgram,\cgram$ respectively the \de{reachability Gramians} and \de{controllability Gramians} of a continuous-time LTIV system the $n \times n$ symmetric matrices
	\begin{equation} \label{eq:gram:tvgramians}
	\begin{aligned}
		\rgram[t_0,t_1] & = \int_{t_0}^{t_1} \stm(t_1,\tau) \B(\tau) \B^T(\tau) \stm^T(t_1,\tau) \, d\tau \\
		\cgram[t_0,t_1] & = \int_{t_0}^{t_1} \stm(t_0,\tau) \B(\tau) \B^T(\tau) \stm^T(t_0,\tau) \, d\tau  
	\end{aligned}
	\end{equation}
	The absolute importance of such matrices is that they are directly linked with the reachability and controllability set: we will show in fact that
	\begin{equation} \label{eq:gram:temp1}
		\reach[t_0,t_1] = \image{\rgram[t_0,t_1]} \hspace{2cm} \ctr[t_0,t_1] = \image{\cgram[t_0,t_1]}
	\end{equation}
	Furthermore determining $\vett \eta_1$ ($\vett \eta_0$) for which it holds $\x_1 = \rgram[t_0,t_1] \vett \eta_1$ ($\x_0 = \cgram [t_0,t_1] \vett \eta_0$), then the \de{optimal input}  $\u^*(t)$ minimizing the energy of the system is in the form
	\begin{equation} \label{eq:gram:optimalinputreach}
		\u^*(t)  =\B^T(t) \stm^T(t_1,t) \vett \eta_1 \hspace{2cm} \forall t \in [t_0,t_1]
	\end{equation}
	(\ref{eq:gram:optimalinputreach}) holds considering reachability, but in case of controllability we have the similar result
	\begin{equation} \label{eq:gram:optimalinputrtr}
		\u^*(t)  =-\B^T(t) \stm^T(t_0,t) \vett \eta_0 \hspace{2cm} \forall t \in [t_0,t_1]
	\end{equation}
	
	\begin{proof} \label{pr:gram:temp1}
		The key point of what has just been presented is equation (\ref{eq:gram:temp1}) linking the reachable (controllable) set with the image of the respective Gramian; the proof presented now is made for the reachable case, but is dual for the controllable set.\\
		In particular we will firstly prove that $\image{\rgram} \subseteq \reach$ and secondly $\reach \subseteq \image{\rgram}$: having that the two must hold together, then it means that $\reach = \image{\rgram}$.
		\begin{enumerate}[\itshape a)]
			\item Taking $\x_1$ a state belonging to the image of the reachability gramian $\x_1 \in \image{\rgram}$, using the optimal control solution (\ref{eq:gram:optimalinputreach}) plugged in the variation of constants formula (\ref{eq:sol:varconstantcontinuous}), page \pageref{eq:sol:varconstantcontinuous}, considering a initial zero-state evaluates to
			\begin{align*}
				\x(t_1) & = \int_{t_0}^{t_1} \stm(t_1,\tau) \B(\tau) \B^T(\tau) \stm^T(t_1,\tau)\vett \eta_1 \, d\tau = \left(\int_{t_0}^{t_1} \stm(t_1,\tau) \B(\tau) \B^T(\tau) \stm^T(t_1,\tau) \, d\tau \right) \vett \eta_1 \\
				& = \rgram[t_0,t_1] \vett \eta_1 = \x_1
			\end{align*}
		
			\item For any state $\x_1 \in \reach[t_0,t_1]$ it's known by definition that at least one control input $\u^*$ exists satisfying $\x_1 = \int_{t_0}^{t_1} \stm(t_1,\tau) \B(\tau) \u^*(\tau) \, d\tau$. Showing that
			\[ \x_1 \in \image{\rgram} = \left(\kernel{\rgram^T}\right)^\perp = \left(\kernel{\rgram}\right)^T \]
			implies $\x_1^T \vett \eta_{1k} = 0$ for any vector $\vett \eta_{1k}\in \kernel{\rgram}$.
			
			Expanding so the definition of $\x(t)$ from the variation of constants formula gives
			\[ \x^T \vett \eta_{1k} = \int_{t_0}^{t_1} \stm(t_1,\tau) \B(\tau) \B^T(\tau) \stm^T(t_1,\tau)\vett \eta_{1k} \, d\tau \]
			Since $\vett \eta_{1k}$ belongs to the kernel and having $\big(\rgram \vett{\eta}_{1k})^T = \vett\eta_{1k}^T \rgram$ we have
			\begin{align*}
				\vett \eta_{1k}^T \rgram[t_0,t_1] \vett \eta_{1k} & = \int_{t_0}^{t_1} \vett \eta_{1k} \stm(t_1,\tau) \B(\tau) \B^T(\tau) \stm^T(t_1,\tau)\vett \eta_{1k} \, d\tau \\
				\vett \eta_{1k} \, 0 & = \int_{t_0}^{t_1} \big\| \B^T(\tau) \stm^T(t_1,\tau) \vett \eta_{1k} \big\|^2 \, d\tau = 0 
			\end{align*}
			Having an integrand that's always non-negative, in order to have a zero integral we must ensure $\B^T(\tau) \stm^T(t_1,\tau) \vett \eta_{1k} = 0$ for any time $\tau$, proving so that $\x_1$ is orthogonal to $\vett \eta_{1k}$.
			
		\end{enumerate}		
	\end{proof}
	\begin{theorem}
		Given two times $t_1 > t_0 \geq 0$:
		\begin{enumerate}[\itshape i)]
			\item for each state $\x_1 \in \reach[t_0,t_1]$ in the reachable set, the control input $\u^*$ described in (\ref{eq:gram:optimalinputreach}) transform the state from $\x_0 = 0$ to $\x_1 = \rgram[t_0,t_1] \vett \eta_1$ with the \textbf{minimum energy control}
			\[ \int_{t_0}^{t_1} |\u(\tau)|^2\, d\tau \]
			Moreover the minimum control energy evaluates to
			\begin{equation}
				\min_{\u} \int_{t_0}^{t_1} |\u(\tau)|^2\, d\tau = \vett \eta_1^T \rgram \vett \eta_1 
			\end{equation}
			\item similarly, for each state $\x_0 \in \ctr[t_0,t_1]$, the control $\u^* = - \B^T(t) \stm^T(t_0,t)\vett \eta_0$ transform the initial state $\x_0 \neq 0$ to the final state $\x_1 = 0$ with the minimum energy control that evaluates to $\vett \eta_0^T \cgram \vett \eta_0$.
		\end{enumerate}
	\end{theorem}
	The main advantage presented by this theorem is that Gramians $\rgram,\cgram$ can be used to estimate the energy required for \textit{moving the states}.
	
	\begin{proof}
		We can prove now \textit{i)}, but the same process can be applied with analogy to \textit{ii)}. Given $\u^*(t)$ the minimum energy control moving the initial state $\x_0 = 0$ into $\x_1 \in \reach[t_0,t_1]$ (defined in \ref{eq:gram:optimalinputreach}) and $\tilde \u(t)$ any other non-optimal input \textit{achieving the same goal}, so such that $\int_{t_0}^{t_1} \stm \B \tilde\u\, d\tau = \x_1$, their relative difference evaluates to
		\[ 0 = \int_{t_0}^{t_1} \stm(t_1,\tau) \B(\tau) \big(\u^*(\tau) - \tilde{\u}(\tau) \big)\, d\tau \tag{$\dagger$} \]
		Defining the difference $\u^*(t) -  \tilde \u(t)$ as $\vett v(t)$, we can compute the control energy of $\tilde \u$ as function of both $\u^*$ and $\vett v$ as
		\begin{align*}
			\int_{t_0}^{t_1} |\tilde \u(\tau)|^2\, d\tau & = \int_{t_0}^{t_1} \tilde \u^T(\tau) \tilde \u(\tau) \, d\tau = \int_{t_0}^{t_1} \big(\u^*(\tau) - \vett v(\tau)\big)^T\big(\u^*(\tau) - \vett v(\tau)\big) \, d\tau \\ 
			& = \int_{t_0}^{t_1} {\u^*}^T \u^*\, d\tau + \int_{t_0}^{t_1} \vett v^T \vett v\, d\tau + \int_{t_0}^{t_1} 2{\u^*}^T \vett v \, d\tau
		\end{align*}
		Knowing the optimal input $\u^*$ that's in the form $\B^T \stm^T \vett \eta_1$, then we can expand the definition of the energy of the control $\tilde \u$ as
		\begin{align*}
			\int_{t_0}^{t_1} |\tilde \u(\tau)|^2\, d\tau & = \vett \eta_1^T \int_{t_0}^{t_1} \stm \B \B^T\stm^T \, d\tau \ \vett \eta_1 + \int_{t_0}^{t_1} |\vett v|^2 \, d\tau + 2\vett \eta_1^T\cancel{\int_{t_0}^{t_1} \stm \B \vett v \, d\tau} \\
			& = \vett \eta_1^T \rgram \vett \eta_1 + \int_{t_0}^{t_1} |\vett v|^2\, d\tau
		\end{align*}
		where the mixed term has been cancelled because of ($\dagger$). Considering that $|\vett v|$ is a non-negative quantity it means that integrating this value over time results in an increment on the energy spent by the control with respect to the minimum value $\vett \eta_1^T \rgram \vett \eta_1$ determine by the optimal control.
	\end{proof}
	
\section{Continuous-time LTI systems: controllability and reachability matrix}
	Considering the specific case of a \textbf{continuous-time LTI} system, we had that the state transition matrix $\stm(t,t_0)$ reduced to the matrix exponential $e^{\A (t-t_0)}$: this allows us to simplify the definition of the \de{Gramians} (\ref{eq:gram:tvgramians}) for both reachable and controllable set to the form
	\begin{equation} \begin{split} \label{eq:gram:LTIwr}
		\rgram[t_0,t_1] & = \int_{t_0}^{t_1} e^{\A(t_1-\tau)} \B\B^T e^{\A^T(t_1-\tau)} \, d\tau = \int_{0}^{t_1-t_0} e^{\A t} \B\B^T e^{\A^Tt} \, d\tau \\
		\cgram[t_0,t_1] & = \int_{t_0}^{t_1} e^{\A(t_0-\tau)} \B\B^T e^{\A^T(t_0-\tau)} \, d\tau = \int_{0}^{t_1-t_0} e^{-\A t} \B\B^T e^{-\A^Tt} \, d\tau 
	\end{split} \end{equation}
	
	The study of the dynamics for what concerns reachability and observability of LTI system is furthermore simplified and can be performed through the definition of the \de{controllability} (or equivalently \textbf{reachability}) \de{matrix} $\R \in \mathds R^{n\times (kn)}$ defined as the \textit{concatenation} of the elements as follows:
	\begin{equation} \label{eq:gram:reachabilitymatrix}
		\R = \matrix{\B & \A\B & \A^2 \B & \dots & \A^{n-1} \B }
	\end{equation}
	
	\begin{theorem} \label{th:gram:LTIequivalence}
		Given any two times $t_1 > t_0 \geq 0$, then for a continuous-time LTI system it holds that
		\begin{equation} \label{eq:gram:temp2}
			\reach[t_0,t_1] = \image{\rgram[t_0,t_1]} = \image{\R} = \image{\cgram[t_0,t_1]} = \ctr[t_0,t_1]
		\end{equation}
	\end{theorem}
	(\ref{eq:gram:temp2}) implicitly states that for time-invariant system the concept of \textit{controllability} and \textit{reachability} are coinciding. A direct consequence of this theorem are the properties of \textbf{time reversibility}, meaning that a state is controllable if and only if it's reachable, and \textbf{time scaling} for which controllability/reachability do not depend on the time difference $t_1-t_0$ because there will always exists an input $\u(t)$ that allows to reach the desired state in an arbitrary small time.
	
	\begin{proof}
		To prove (\ref{eq:gram:temp2}) we can show that $\reach \subseteq \image{\R}$ and $\image \R \subseteq \image{\rgram}$, thus for (\ref{eq:gram:temp1}) we can ensure $\image \rgram = \reach = \image \R$:
		\begin{enumerate}[\itshape a)]
			\item Given a reachable state $\x_1 \in \reach[t_0,t_1]$, then it's already ensured that exists an input $\u(t)$ for which $\int_{t_0}^{t_1} e^{\A(t_1-\tau) } \B \u(\tau)\, d\tau = \x_1$. Exploiting Cayley-Hamilton theorem, page \pageref{th:cayley}, we can rewrite this integral as
			\[ \x_1 = \int_{t_0}^{t_1} \sum_{i=0}^{n-1} \alpha_i(t_1-\tau) \A^i \B \u(\tau)\, d\tau = \sum_{i=0}^{n-1} \A^i \B \int_{t_0}^{t_1} \alpha_i(t_1-\tau) \u(\tau)\, d\tau \]
			The last equality can be rewritten as a linear combination in the form
			\[ \x_1 = \underbrace{\matrix{\B & \A \B & \dots & \A^{n-1} \B}}_{=\R} \underbrace{\vector{ \int_{t_0}^{t_1} \alpha_0(t_1-\tau)\u(\tau)\, d\tau \\ \int_{t_0}^{t_1} \alpha_1(t_1-\tau)\u(\tau)\, d\tau \\ \vdots \\ \int_{t_0}^{t_1} \alpha_{n-1}(t_1-\tau)\u(\tau)\, d\tau }}_{\vett v}  \]
			From this expression we can clearly see that exists a vector $\vet v$ that pre-multiplied by the reachability matrix $\R$ gives the desired state $\x_1$, meaning that any reachable state $\x_1 \in \reach[t_0,t_1]$ is also inside the image of $\R$.
			
			\item In proof \ref{pr:gram:temp1} we showed that $\B^T\stm^T(t_1,\tau) \vett \eta_{1k} = 0$ for any vector $\vett \eta_{1k} \in \kernel{\rgram[t_0,t_1]}$ and for any time $\tau \in [t_0,t_1]$: applying this to the specific case of continuous-time system reduces to
			\[ \B^T e^{\A^T (t_1-\tau)} \vett \eta_{1k} = 0 \tag{$\dagger$} \]
			Deriving in time this expression transposed evaluates to $\frac d{dt}(\dagger)^T = -\vett \eta_{1k}^T \A e^{\A(t_1-\tau)} \B = 0$;  generalizing the concept to the $i$-th derivative in time gives
			\[ \frac{d^i}{dt^i}(\dagger)^T = (-1)^i \vett \eta_{1k}^T \A^i e^{\A(t_1-\tau)}\B = 0 \]
			Evaluating the derivative for $\tau = t_1$ results in $\vett \eta_{1k}^T \A^i \B = 0 \ \forall i$. For any reachable state $\x_i = \R \vett v \in \image{\rgram}[t_0,t_1]$ we can see that
			\begin{align*}
				\x_1^T \vett \eta_{1k} & = \vett \eta_{1k} \x_1 = \vett \eta_{1k} \R \vett v = \vett \eta_{1k}^T \matrix{ \B & \A \B & \dots & \A^{n-1} \B } \vet v \\ &
				= \matrix{\cancel{\vet \eta_{1k} \B} & \cancel{\vet \eta_{1k} \A \B} & \dots & \cancel{\vet \eta_{1k} \A^{n-1}\B} } \vet v
			\end{align*}
			Having $\x_1 \perp \vett \eta_{1k}$ for any vector $\vett \eta_{1k} \in \kernel{\rgram}$ then it means that $\x_1$ belong to the orthogonal subspace $\big(\kernel{\rgram}\big)^\perp$; having from linear algebra $\image{\mat W} = \big(\ker{\mat W^T}\big)^\perp$, but knowing also that $\rgram$ is a symmetric matrix, when we have that $\big(\kernel \rgram\big)^\perp = \image{\rgram}$, this $\image\R \subseteq \image \rgram$.
		\end{enumerate}
	\end{proof}
	
	As a matter of fact, combining (\ref{eq:gram:reachabilitymatrix}) with (\ref{eq:gram:temp2}) tells us that for LTI system both controllability and reachability are coincident sets that are characteristic of the dynamic of the system, in particular is determine just by the matrices $\A,\B$ (due to the characterization of the controllability matrix).
	
	Moreover considering (\ref{eq:gram:controllable}) we can further simplify the definition of \de{controllable system} by considering that the dimension of the controllable subspace coincides with the dimension of the image of the reachability matrix:
	\begin{equation}
		\textrm{controllable sys.} \qquad \Leftrightarrow \qquad \textrm{reachable sys.} \qquad \Leftrightarrow \qquad  \dim \big(\image{\R}\big) = n
	\end{equation}
	
\section{Extension to the discrete-time case}
	\paragraph{Time-varying case} Until now only the linear continuous-time case has been considered, but similar tools can be developed also for the discrete-time counter part, but in this case we have to take care of more subtle details in the analysis.
	
	Knowing the solution (\ref{eq:sol:varconstantdiscrete}), page \pageref{eq:sol:varconstantdiscrete}, of such system, given two times $t_1 > t_0 \geq 0$ we can define the \de{$[t_0,t_1]$-reachable} and \de{$[t_0,t_1]$-controllable subspaces} of the discrete-time LTV system the sets defined as
	\begin{equation}
	\begin{split}
		\reach[t_0,t_1] & = \left\{ \x_1 \ : \ \exists \u(t) \textrm{ with } t \in [t_0,t_1) \textrm{ such that } \x(t_1) = \sum_{\tau = 0}^{t_1-1} \stm(t_1,\tau + 1) \B(\tau)\u(\tau) \right\} \\
		\ctr[t_0,t_1] & = \left\{ \x_0 \ : \ \exists \u(t) \textrm{ with } t \in [t_0,t_1) \textrm{ such that } 0 = \stm(t_1,t_0)\x_0 + \sum_{\tau = 0}^{t_1-1} \stm(t_1,\tau + 1) \B(\tau)\u(\tau) \right\} 
	\end{split}
	\end{equation}
	Indeed the concept that the sets $\reach$ and $\ctr$ are representing are fundamentally the same: the first subspace contains all the possible states $\x_1$ that can be achieved by controls starting from a zero-state configuration, while in the second case the set contains all initial states $\x_0$ that can be driven to zero.
	
	Moreover we can observe that if the matrix $\A$ of the system is non singular for all times $t\in[t_0,t_1]$, then the state transition matrix in invertible and we can alternatively define the controllable subspace as
	\[ \ctr[t_0,t_1] = \left\{ \x_0 \ : \ \exists \vet v(t) = -\u(t) \textrm{ such that } \x_0 = \sum_{\tau = 0}^{t_1-1} \stm(t_0,\tau+1) \B(\tau) \vet v(\tau) \right\} \]	
	
	The \de{discrete-time Gramians} are equivalent to the continuous-time ones, with the exception of interchanging the integral with a summation, resulting in 
	\begin{equation} 	\begin{split}
		\rgram[t_0,t_1] & = \sum_{\tau = 0}^{t_1-1} \stm(t_1,\tau+1)\B(\tau)\B^T(\tau) \stm^T(t_1,\tau + 1)  \\		 
		\cgram[t_0,t_1] & = \sum_{\tau = 0}^{t_1-1} \stm(t_0,\tau+1)\B(\tau)\B^T(\tau) \stm^T(t_0,\tau + 1)
	\end{split} \end{equation}
	We have to pay attention to the fact that the controllability Gramian uses the \textit{backward in time} definition of the state transition matrix $\stm$: this operation can be performed if and only if $\A$ is non-singular in the domain $[t_0,t_1]$, otherwise the Gramian cannot be determined (and as consequence the state cannot be controlled).
	
	As in the continuous-time case, it still holds
	\[ \image{\rgram[t_0,t_1]} = \reach[t_0,t_1] \qquad \image{\cgram[t_0,t_1]} = \ctr[t_0,t_1] \hspace{2cm} \forall t_1> t_0 \geq 0 \]
	and considering a reachable (controllable) state $\x_1 = \rgram \vett \eta_1 \in \reach$ ($\x_0 = \cgram \vett \eta_0 \in \ctr$), then the control $\u^*(t) = \B^T(t) \stm^T(t_1,t+1) \vett \eta_1$ ($\u^*(t) = - \B^T \stm^T(t_0,t+1)\vett \eta_0$) moves $\x_0=0$ ($\x_0\neq 0$) into $\x_1 \neq 0$ ($\x_1 = 0$) with the minimum energy.
	
	\paragraph{Time-invariant case} Considering now a discrete-time LTI case, we have that the state transition matrix $\stm(t_1,\tau)$ collapses to the computation of a matrix power $\A^{t_1-\tau}$: this so simplifies the discrete-time Gramians to
	\begin{equation}
		\rgram[t_0,t_1] = \sum_{s=0}^{t_1-t_0-1} \A^s \B\B^T \big(\A^T\big)^s \hspace{1.5cm} \cgram[t_0,t_1] = \sum_{s=0}^{t_1-t_0-1} \A^{-s-1} \B\B^T \big(\A^T\big)^{-s-1}
	\end{equation}
	Theorem \ref{th:gram:LTIequivalence} still holds if and only if we assume that $\A$ is always invertible in $[t_0,t_1]$ (in order to have a proper definition of the controllability set) and if we impose that $t_1 \geq t_0 + n$: this is a very important bound limiting the time scaling property and is due to the fact that to reach all possible state we have to ensure, by Cayley-Hamilton, that all matrix power up to the $n$-th order exists.
	
\section{Full-state feedback and single-input eigenvalue assignment} \label{sec:fullstatefeed}
	Given a \textbf{LTI} system of which we can measure all his state $\x$, then we can assign as input $\u$ a linear combination of them in the form $\u = -\K \x$, where $\K$ is a $m\times n$ matrix; note that this  preamble is the same as the one of the linear quadratic regulator described in page \pageref{sec:LQR}. With this imposition we obtain a new \textbf{dynamic equation} of the system that's called \textit{in \de{full-state feedback}}:
	\begin{equation}
		\dx = \A \x + \B \u = \A \x - \B\K \x = \big(\A-\B\K\big) \x
	\end{equation}
	
	Considering now a \textbf{single-input controllable system} (for which $\reach = \ctr = \mathds R^n$) characterized by the pair of matrices $\A,\B$ (that are the lonely one relevant for the computation of the reachability matrix $\R$), then for each set of \de{desired eigenvalues} $\lambda_1,\dots, \lambda_n$ there always exists a $1\times n$ matrix $\K$ such that
	\[ \A_{cl} = \A - \B \K \]
	has those eigenvalues; usually $\A_{cl}$ is referred as the \de{closed-loop matrix}. The goal of the next paragraph is to describe a \textit{procedure} for the so called \de{single-input eigenvalue assignment}.
	
	\paragraph{Controllable canonical form} Given a controllable LTI system with characteristic polynomial of the form
	\[ p_\A(s) = s^n + \alpha_{n-1}s^{n-1} + \dots + \alpha_1 s + \alpha_0 \]
	at page \pageref{eq:dyn:controllablecanonical} we introduced the controllable canonical form whose matrices $\A_{ctr}, \B_{ctr}$ realizing the system were
	\[ \A_{ctr} = \matrix{ 0 \\ \vdots & & \I_{(n-1)\times(n-1)} \\ 0 \\ - \alpha_0 & - \alpha_1 & \dots & - \alpha_{n-1}} \qquad \B_{ctr}= \matrix{0\\ \vdots \\ 0 \\ 1} \]
	
	Knowing that the characteristic polynomial of the closed-loop matrix can be regarded as 
	\[ p_{\A_{cl}}(s) = \big(s-\lambda_1\big) \dots \big(s-\lambda_n\big) = s^n + \beta_{n-1}s^{n-1} + \dots + \beta_1 s + \beta_0 \]
	Equating the controllable canonical form of the closed-loop matrix $\A_{cl}$ with the system $\A - \B \K$ allows to obtain a simple relation to compute the matrix $\K$, because as we can see expanding the product $\B \K$ all we obtain is 
	\[ \matrix{ 0 \\ \vdots & & \I \\ 0 \\ - \beta_0 & - \beta_1 & \dots & - \beta{n-1}} = \matrix{ 0 \\ \vdots & & \I \\ 0 \\ - \alpha_0 - k_0 & - \alpha_1 - k_1 & \dots & - \alpha_{n-1} - k_{n-1}} \]
	If we considered so the controllable form of the system we could have easily determined the matrix $\K$ as
	\begin{equation}
		\K = \matrix{\beta_0 - \alpha_0 & \beta_1 - \alpha_1 & \dots & \beta_n-\alpha_n}
	\end{equation}

	\paragraph{More general case} In the more general case $\A,\B$ are not in controllable canonical form, but are \textit{generic} full matrices; however we can observe that the \textbf{reachability matrix} acts as a \textbf{similarity transformation} to transform any system into it's \textbf{observable canonical form} 
	\[ \A_{ob} = \matrix{ 0 & \dots & 0 & - \alpha_0  \\ &&& -\alpha_1 \\ &  \I_{(n-1)\times(n-1)} && \vdots \\ &&& -\alpha_{n-1}} \qquad \B_{ob} = \matrix{1\\ 0 \\ \vdots \\ 0} \]
	where the transformed matrices $\A_{ob}, \B_{ob}$ are obtained through the algebraic equivalence similar to (\ref{eq:dynsys:algebraicequivalence}) in the form
	\begin{equation} \label{eq:gram:temp3}
		\A_{ob} = \R^{-1} \A \R \qquad \B_{ob} = \R^{-1} \B
	\end{equation}
	\begin{proof}
		To prove the effectiveness of (\theequation) we can show that indeed $\R \A_{ob} = \A \R$. Computing the product $\R \A_{ob}$ considering the block matrices in the form
		\[ \R \A_{ob} = \matrix{ \\ \B  & \A \B & \dots & \A^{n-1} \B \\ &} \matrix{\begin{array}{ccc | c}
			& 0 & &- \alpha_0 \\ \hline
			&&& - \alpha_1 \\ 
			& \I &&\vdots \\ &&&-\alpha_{n-1}
		\end{array}} = \matrix{\begin{array}{ccc | c}
			&&&\\
			\A \B&  \dots & \A^{n-1} \B & \x \\ &&&
		\end{array}}\]
		where the vector $\x$ is the linear combination defined as $-\alpha_0\B - \alpha_1 \A\B - \dots-\alpha_{n-1} \A^{n-1}\B$; observe that for SISO system all products $\A^i\B$ evaluates to a vector. Collecting $\B$ we have $\x = (-\alpha_0 \I - \alpha_1 \A- \dots - \alpha_{n-1} \A^{n-1})\B$ and recalling the Cayley-Hamilton theorem (page \pageref{th:cayley}) we can consider the term in parenthesis exactly as $\A^n$, so we can rewrite
		\[ \R \A_{ob} = \matrix{\A\B & \dots & \A^{n-1}\B & \A^n \B} = \A \matrix{\B & \A\B & \dots & \A^{n-1} \B} = \A \R \]
		Similarly, by performing the matrix multiplication, we can show that $\B_{ob} =\R^{-1}\B$ by simply proving that $\R \B_{ob} = \B$.
	\end{proof}
	
	Moreover for any observable canonical form exists a similarity transformation matrix $\mat M$ that allows to obtain the controllable canonical form as $\mat M^{-1} \A_{ob}\mat M = \A_{ctr}$ and $\mat M^{-1} \B_{ob} = \B_{ctr}$. In particular $\mat M$ is a symmetric matrix in the form form
	\begin{equation} \label{eq:gram:temp4}
		\mat M = \matrix{ \alpha_1 & \alpha_2 & \dots & \alpha_{n-1} & 1 \\
		\alpha_2 & &  & \reflectbox{$\ddots$} \\
		\vdots & & \reflectbox{$\ddots$} \\
		\alpha_{n-1} & \reflectbox{$\ddots$} \\ 1}
	\end{equation}
	
	\begin{proof}
		To show that (\ref{eq:gram:temp4}) is the similarity transformation transforming the observable into the canonical form  we can start computing the product $\mat M \A_{ctr}$ considering the block matrices
		\[ \mat M \A_{ctr} = \matrix{\begin{array}{ccc|c}
				& \vet \alpha^T && 1 \\ \hline &&& 0 \\ & \mat M_{21} && \vdots \\ &&& 0
		\end{array}} \matrix{\begin{array}{c|ccc}
			0 \\ \vdots && \I \\ 0 \\ \hline - \alpha_0 & & -\vet \alpha^T &
		\end{array}} =  \matrix{\begin{array}{c|ccc}
			-\alpha_0 & 0 & \dots & 0 \\ \hline 0 \\ \vdots && \mat M_{21} \\ 0
		\end{array}} \]
		where $\mat M_{21}$ is the $(n-1)\times(n-1)$ submatrix of $\mat M$ and $\vet \alpha$ is the column vector $(\alpha_1,\dots , \alpha_{n-1})$. We observe that the result of the product is a symmetric matrix, so we can say that
		\[ \mat M \A_{ctr} = \big(\mat M \A_{ctr}\big)^T = \A_{ctr}^T \mat M^T = \A_{ob} \mat M \]
		proving the algebraic equivalence between the two representations.
	\end{proof}
	
	Combining what has been said so far, considering the transformation matrix $T =\R\mat M$ allows to directly compute the controllable canonical form of any pair $(\A,\B)$ as
	\[ \T^{-1} \A \T = \A_{ctr} \qquad \T^{-1} \B= \B_{ctr} \]
	We can so summarize the procedure for the \textbf{single-input eigenvalue assignment} as
	\begin{enumerate}
		\item given the original pair $(\A,\B)$ and the desired eigenvalues $\lambda_i$ of the closed-loop system, compute their characteristic polynomials in the form
		\[ p_\A(s) = s^n + \alpha_{n-1}s^{n-1} + \dots + \alpha_1 s + \alpha_0 \qquad p_{\A_{cl}}(s) = s^n + \beta_{n-1}s^{n-1} + \dots + \beta_1 s + \beta_0 \]
		\item compute the feedback matrix $\K_{ctr}$ in the controllable canonical form as
		\[ \K_{ctr} = \matrix{\beta_0-\alpha_0 & \dots & \beta_n-\alpha_n} \]
		\item build the controllability matrix $\R$ (\ref{eq:gram:reachabilitymatrix}) and the transformation $\mat M$ as in (\ref{eq:gram:temp4}) and compute $\T = \R \mat M$ and it's inverse;
		\item determine the feedback matrix $\K$ for the initial pair $(\A,\B)$ as
		\[ \K = \K_{ctr} \T^{-1} \]		
	\end{enumerate}
	As final check one can verify that indeed $\A-\B\K$ gives the desired eigenvalues.
	
	\paragraph{Ackerman method} Another way to compute the feedback matrix $\mat K$ is through the \de{Ackerman equation} defined as
	\begin{equation}
		\K = \matrix{0 & \dots & 0} \R^{-1} p_{\A_{cl}}(\A)
	\end{equation}
	
\section{Controllable systems} \label{sec:controllablesystems}
	Given a \textbf{linear system} $\Sigma$ with dynamics $\dx/\xp = \A(t)\x + \B(t) \u$, then given two times $t_1>t_0 \geq 0$ we say that the pair $(\A,\B)$ is \de{reachable} on the time interval $[t_0,t_1]$ if the reachable subspace has \textit{maximum dimension}, meaning $\reach[t_0,t_1] = \mathds R^n$; similarly the pair $(\A,\B)$ is \de{controllable} if $\ctr[t_0,t_1] = \mathds R^n$. The underlying idea of this definition is that for such system we can choose particular inputs $u(t)$ that allows us to reach/control all possible state of $\Sigma$ in $\mathds R^n$.
	
	From now on, where not otherwise specified, this section will consider the simplified condition of \textbf{linear time-invariant} system (and for discrete-time sys. we also assume that $\A$ is invertible) in order to have the coincidence between controllability and reachability.
	
	For LTI systems the condition of controllability/reachability collapses to the study of the rank of the controllability matrix, in particular
	\begin{equation} \label{eq:gram:temp5}
		\textrm{controllable sys.} \qquad \Leftrightarrow \qquad \textrm{reachable sys.} \qquad \Leftrightarrow \qquad \rank{\R} = n
	\end{equation}
	\begin{proof}
		Knowing that $\R$ is a $n\times mn$ matrix, if it happens that $n$ of it's rows are linearly independent then we are sure that $\image{\R} = \mathds R^n$: this furthermore implies $\image{\rgram[t_0,t_1]} = \mathds R^n$ probing the controllability of the system.
	\end{proof}
	The intuition behind the idea of \textbf{\textit{controllable system}} is that \textit{the input can reach the dynamic of all the states}. Considering as example the system
	\[ \begin{cases}
		\dot x_1 = x_1 \\ \dot x_2 = - 2x_2 + x_1 + u
	\end{cases} \]
	we can intuitively say that the system is not controllable: the input $u$ in fact affects only the dynamic of the second state $x_2$, while there's no way to act on the state $x_1$ (that's indeed autonomous). We can rigorously prove this intuition by building the matrices $\A= \matrix{1 & 0 \\ 1 & - 2}$ and $\B= \matrix{0 \\ 1}$: once we compute $\R = \matrix{ 0 & 0 \\ 1 & - 2}$ we see that the controllability matrix is not full rank, hence the system is not controllable.
	
	Considering instead a similar system whose state-space representation is
	\[ \begin{cases}
		\dot x_1 = x_2 \\ \dot x_2 = - 2x_2 + x_1 + u
	\end{cases} \]
	we see that $u$ cannot directly modify the dynamic of the first state $x_1$, however it can still be driven by controlling the state $x_2$ (appearing in the right-hand side of the first dynamic equation), thus there's a way to \textit{move} both states as desired. Having this time $\A = \matrix{0 & 1 \\ 1 & -2}, \B = \matrix{0\\1}$ the reachability matrix $\R=\matrix{0 & 1 \\ 1 & -2}$ has full rank, proving that the system is controllable.
	
\subsection{Controllability tests}
	The scope now is to determine some \textit{tests} that can be used to prove the controllability/reachability (or not) of the system without the need of computing the reachability matrix $\R$ (that in general for \textit{big} system can be numerically expensive).
	
	In order to do so, let's recall one concept from linear algebra: given a $n\times n$ matrix $\A$, the linear subspace $\mathcal V \subseteq \mathds R^n$ is said \de{$\A$-invariant} if $\forall \vet v \in \mathcal V$ it holds that $\A \vet v \in \mathcal V$. Subsequent properties are that
	\begin{enumerate}[\itshape i)]
		\item given a subspace $\mathcal V \neq \{0\}$ for which we can define a matrix $\V \in \mathds R^{n\times k}$ whose $k$ columns are a basis of $\mathds V$, then there exists a matrix $\overline \A\in \mathds R^{k\times k}$ such that
		\[ \A \vet V = \vet V \overline \A \]
		Calling in fact $\vet v_i\in \mathds R^n, i=1\dots k$, the columns of $\V$, is the associate linear space $\mathcal V$ is $\A$-invariant then it means that also $\A \vet v_i \in \mathcal V$ for any $i$. At the same time we have that $\A\vet v_i$ can be regarded as a linear combination $\overline a_{1i} \vet v_i + \overline a_{2i} \vet v_2 + \dots + \overline a_{ki} \vet v_k$, so
		\[ \A \matrix{\vet v_1 & \dots & \vet v_k} = \matrix{\vet v_1 & \dots & \vet v_k} \matrix{\overline a_{11} & \dots & \overline a_{1k} \\ \vdots & \ddots \\ \overline a_{k1} & & \overline a_{kk} } \] 
		
		\item given $\A\in \mathds R^{n\times n}$ and an $\A$-invariant linear subspace $\mathcal V$, then the matrix $\V$ (made by the basis of $\mathcal V$) contains at least one eigenvalue of $\A$.
		
		Considering in fact the eigenvalue-eigenvector pair $(\lambda, \overline{\vet v})$ of the matrix $\overline \A$, so satisfying $\overline \A \overline {\vet v} = \lambda \overline{\vet v}$, then if $\mathcal V$ is $\A$-invariant for \textit{i)} we equivalently have $\A \V \overline{\vet v} = \V \overline\A \overline{\vet v}$; since $\overline{\vet v}$ is an eigenvector of $\overline \A$, then such equality can be rewritten as 
		\[ \A \V \overline{\vet v} = \lambda \V \overline{\vet v} \]
		Substituting the vector $\V \overline{\vet v}$ with the variable $\x$, we finally reach the \textit{standard form} of the eigenvalue statement $\A \x = \lambda \x$, proving so that $\lambda$ is also an eigenvector of $\A$.
		
	\end{enumerate}
	
	\paragraph{Eigenvector test} The \de{eigenvector test} \textbf{for controllability} states that {\itshape a LTI system is controllable if and only if all the eigenvectors $\x$ of the matrix $\A^T$ are not in the kernel of $\B^T$.}
	\begin{proof}
		The proof of such test can be performed in two step: firstly by showing that if the pair $(\A,\B)$ is controllable, then the eigenvector of $\A^T$ are not in the kernel of $\B^T$ and then the reversed statement:
		\begin{enumerate}[\itshape a)]
			\item By contradiction let's assume that exists an eigen-pair $(\x,\lambda) \neq 0$ such that $\A^T \x = \lambda \x$ and for which $\B^T\x = 0$, exploiting the definition (\ref{eq:gram:reachabilitymatrix}) of the controllability matrix transposed, then we have that
			\[ \R^T \x = \matrix{\B^T \\ \B^T\A^T \\ \vdots \\ \B^T \big(\A^{n-1}\big)^T} \x = \vector{\B^T\x \\ \B^T\A^T\x \\ \vdots \\ \B^T \big(\A^{n-1}\big)^T\x} = \vector{0 \\ \B^T \lambda \x \\ \vdots \\ \B^T \lambda^{n-1} \x} = \vector{0 \\ \lambda \B^T\x \\ \vdots \\ \lambda^{n-1} \B^T \x} = \vector{0\\0\\ \vdots \\ 0} \]
			Shown that $\x$ is in the kernel of $\R^T$, then we are sure that $\dim \kernel{\R^T} \geq 1$ thus for the fundamental theorem of linear algebra we can also say that
			\[ \dim \image{\R^T} < n \]
			However as initial hypothesis, having the pair $(\A,\B)$ controllable requires a reachability matrix that's full rank, so it must have $\dim \image{\R^T} = \dim \image{\R} = \rank{\R}$: this arise the contradiction proving so that if $\x$ in an eigenvector of $\A^T$, then surely it cannot belong to the kernel of $\B^T$.
			
			\item Let us consider now the eigenvector $\x$ satisfying $\A^T \x = \lambda \x$ and $\B^T \x \neq 0$, then we can prove by contradiction that the pair $(\A,\B)$ is controllable. Assuming in fact that $(\A,\B)$ is not controllable, by (\ref{eq:gram:temp5}) we can say that $\rank{\R} = \rank{\R^T} < n$, this by the fundamental theorem of linear algebra $\dim \kernel{\R^T} > 0$.\\
			It happens that $\ker{\R^T}$ is $\A^T$-invariant (meaning $\R^T \x = \R^T\A^T \x = 0$), because considering 
			\[ \R^T \x = \matrix{\B^T \\ \B^T\A^T \\ \vdots \\ \B^T \big(\A^{n-1}\big)^T} \x = \vector{\B^T\x \\ \B^T\A^T\x \\ \vdots \\ \B^T \big(\A^{n-1}\big)^T\x} = 0 \]
			if further implies
			\[ \R^T\A^T \x = \vector{\B^T\A^T\x \\ \B^T(\A^2)^T\x \\ \vdots \\ \B^T \big(\A^{n}\big)^T\x} = \matrix{0 \\ \vdots \\ 0 \\ \B^T(\A^n)^T} \x = 0 \]
			Using Cayley-Hamilton theorem, page \pageref{th:cayley}, we can in fact decompose the last block of the matrix as
			\[ \B^T(\A^n)^T \x = \B^T \left( \sum_{k=0}^{n-1} \alpha_k \A^k \right)^T \x = \sum_{k=0}^{n-1} \alpha_k \B^T(\A^k)^T\x = 0 \]
			Showed that $\kernel{\R^T}$ is $\A^T$-invariant, then $\A^T\x \in \kernel{\R^T}$ contains al least one eigenvector of $\A^T$ (due to property \textit{ii)} of the $\A$-invariant definition). Since $\R^T \x=0$ implies $\B^T\x = 0$, this contradicts the initial statement for which we should have had $\B^T\x \neq 0$, meaning that $(\A,\B)$ cannot be non-controllable (hence must be controllable).			
		\end{enumerate}
	\end{proof}
	
	\paragraph{Popov-Belevitch-Hotus PBH test} Another method that can be used to prove controllability of the system is the so called \de{PBH test}, named by its invetor Popov-Belevitch-Hotus, stating that {\itshape the pair $(\A,\B)$ is controllable if and only if the rank of the matrix $\matrix{\A-\lambda\I& \B}$ is equal to $n$ for any value of $\lambda$:}
	\begin{equation} \label{eq:gram:PBHcontrol}
		(\A,\B) \textrm{ controllable} \qquad \Leftrightarrow \qquad \rank{\matrix{\A-\lambda \I & \B}} = n \quad \forall \lambda \in \mathds C
	\end{equation}
	
	Note that for almost all values of $\lambda$ condition (\ref{eq:gram:PBHcontrol}) is always satisfied (because $\A$ is always full-rank), but the only \textit{problematic points} are the eigenvalues $\lambda$ of $\A$ that are making the matrix $\A-\lambda\I$ rank-deficient.
	
	\begin{proof}
		The PBH test can be proven by applying the fundamental theorem of linear algebra to the matrix $\matrix{\A-\lambda\I&\B}^T$ and requiring that the dimension of it's kernel must be null:
		\[ \dim \kernel{\matrix{\A^T-\lambda \I \\ \B^T}} = n - \rank{\matrix{\A-\lambda\I & \B}} = 0 \]
		This means that
		\[ \matrix{\A^T-\lambda\I \\ \B^T} \x = \matrix{\A^T \x - \lambda\I\x \\ \B^T\x} \neq 0 \qquad \forall \x \neq 0, \forall \lambda \in \mathds C \]
		In the particular case when $(\lambda,\x)$ is a eigen-pair of $\A^T$, so such that $\A^T\x = \lambda\x$, in order to have a non-zero matrix we must have $\B^T\x \neq 0$: each eigenvector of $\A^T$ so do not belong to $\ker \B^T$ and so for the eigenvalue test previously shown implies controllability.		
	\end{proof}

	\paragraph{Lyapunov controllability test} Given a continuous/discrete-time LTI system characterized by a Hurwitz/Schur matrix $\A$ (so assuming that the system is exponentially stable), then the pair $(\A,\B)$ is controllable if and only if exists a symmetric positive-definite matrix $\mat W = \mat W^T > 0$ such that
	\begin{equation} \label{eq:gram:temp6}
		(CT):\quad \A \mat W + \mat W \A^T = - \B\B^T \hspace{2cm} (DT): \quad \A \mat W \A^T - \mat W = - \B \B^T
	\end{equation}
	Moreover the unique solution $\mat W$ can be computes as $\lim_{t_1-t_0\rightarrow \infty} \rgram[t_0,t_1]$, so
	\begin{equation} \label{eq:gram:temp7}
		(CT):\quad \mat W = \int_0^\infty e^{\A t} \B\B^T e^{\A^Tt}\, dt \hspace{2cm} (DT):\quad \mat W = \sum_{\tau = 0}^\infty \A^\tau\B\B^T(\A^T)^\tau
	\end{equation}
	
	\begin{proof}
		To prove the Lyapunov test for controllability we have to recall that the norm squared of a complex vector $\vet v \in \mathds C^n$ can be computed as $|\vet v|^2 = {\vet v^*}^T \vet v$. With this premise, we can firstly show that if (\ref{eq:gram:temp6}) holds then $(\A,\B)$ is controllable and then vice-versa:
		\begin{enumerate}[\itshape a)]
			\item We show now that if exists $\mat W$ positive-definite satisfying (\ref{eq:gram:temp6}), then $(\A,\B)$ is controllable. Exploiting the eigenvector test, if $(\lambda,\x)$ is an eigen-pair of $\A^T$ then it implies $\B^T\x = 0$. By pre-multiplying by $\x^*$ and post-multiplying by $\x$ equation \ref{eq:gram:temp6} results in
			\begin{align*}
				\x^*\big(\A \mat W + \mat W \A^T\big)\x = \x^*\A \mat W \x + \x^*\mat W \A^T\x = \big(\A^T \x\big)^* \mat W\x + \x^* \mat W\big(\A^T \x\big) & = -\x^*\B\B^T\x \\
				\big(\lambda \x\big)^* \mat W \x + \x^* \mat W\big(\lambda \x\big) = \lambda^*\x^* \mat W \x + \lambda \x^* \mat W \x & = - |\B^T\x|^2 \\
				2\Re \lambda \x^* \mat W \x & = 
			\end{align*}
			With the hypothesis of $\A$ being Hurwitz, then it means that $\Re \lambda < 0$; moreover having $\mat W$ positive-definite tells us that the overall term $2\Re\lambda \x^*\mat W\x$ is negative for each eigenvector $\x \neq 0$ of $\A^T$ (with eigenvalue $\lambda$). This means that the norm $|\B^T\x|^2$ is non-zero for all $\x\neq 0$, so by the eigenvector test this implies that the pair $(\A,\B)$ is controllable.
			
			\item We show now that if the pair $(\A,\B)$ is controllable, then exists a positive-definite matrix $\mat W$ satisfying (\ref{eq:gram:temp6}), Calling the product $\B\B^T$ as $\overline \Q$ and $\A^T = \overline \A$, what we obtain is a formulation in the form
			\[ \mat W \overline \A + \overline \A^T = - \overline \Q \]
			This formulation coincides with the Lyapunov equality (\ref{eq:ly:lyapequality}), page \pageref{eq:ly:lyapequality}, from which we can compute the solution $\mat W$ as
			\[ \mat W = \int_{t_0}^{t_1} e^{\overline \A t} \overline \Q e^{\overline \A^T t}\, dt \]
			This solution is based upon the fact that $\overline \Q$ is positive-definite, condition that until now wasn't ensured but it holds so by the assumption that $(\A,\B)$ is a controllable pair. Considering in fact
			\[ \x^T \mat W \x = \int_0^\infty \x^T e^{\A t} \B \B^T e^{\A^Tt} \x\, dt = \int_0^\infty \big| \B^T e^{\A^T t} \x \big|^2\, dt \geq 0 \]
			proving so that the so obtained matrix $\mat W$ is positive definite.
		\end{enumerate}
	\end{proof}
	
	\begin{theorem}
		Given an autonomous LTI system $\dx/\xp = \A \x$m then the following conditions/statements are equivalent:
		\begin{enumerate}[\itshape i)]
			\item the system is asymptotically stable;
			\item the system is exponentially stable;
			\item matrix $\A$ is Hurwitz/Schur (meaning that there aren't \textit{bad eigenvalues});
			\item for every symmetric positive-definite matrix $\Q = \Q^T > 0$ there exists a matrix $\P$ such that
			\[ (CT): \quad \A^T \P + \P \A = - \Q \hspace{2cm} (DT): \quad \A^T\P\A - \P = \Q \]
			Moreover the unique solution of the problem is described by $\P = \int_0^\infty e^{\A^T t} \Q e^{\A t}\, dt$ for continuous-time systems and $\P = \sum_{k=0}^\infty \big(\A^T\big)^k \Q \A^k$ for the discrete-time case;
			\item always exists a symmetric positive-definite matrix $\P= \P^T > 0$ such that
			\[ (CT): \quad \A^T \P + \P \A < 0 \hspace{2cm} (DT): \quad \A^T\P\A - \P < 0 \]
			\item for each matrix $\B$ such that $(\A,\B)$ is a controllable pair exists a positive-definite matrix $\mat W = \mat W^T > 0$ solving
			\[ (CT):\quad \A \mat W + \mat W \A^T = - \B\B^T \hspace{2cm} (DT): \quad \A \mat W \A^T - \mat W = - \B \B^T \]
			The unique solution of $\mat W$ is presented in (\ref{eq:gram:temp7}).
		\end{enumerate}
	\end{theorem}
	
\subsection{Feedback stabilization with Lyapunov test}
	Considering a controllable continuous-time LTI characterized by a controllable pair $(\A,\B)$, we can show also the system $(\B, -\mu\I - \A)$ is controllable for any given $\mu$. We have in fact that both $-\mu\I-\A$ and $\A$ are sharing an eigenvector: given the eigen-pair $(\lambda,\x)$ of $\A$ for which $\A\x = \lambda \x$, subtructing $-\mu \I\x$ on both sides evaluates to the expression
	\[ \big(-\mu \I - \A\big)  \x= \big(-\lambda - \mu\big) \x \]
	This shows that $\x$ is an eigenvector of both $\A$ and $-\mu \I - \A$ (while their eigenvalues are different and are $\lambda$ and $-\mu - \lambda$ respectively) and thus in both cases it satisfies the requirement $\B^T\x \neq 0$ (having initially impose that $(\A,\B)$ is controllable). Picking a coefficient $\mu$ \textit{large enough} allows us to \textit{shift} all the eigenvalues of $\A$ into the \textit{good region} for the eigenvalues (in particular there are infinite values of $\mu$ for which $-\mu \I - \A$ is Hurwitz); having that this system must satisfy the (\ref{eq:gram:temp6}), so $(-\mu \I - \A)\mat W + \mat W(-\mu\I - \A)^T = -\B \B^T$, we can also observe by computing the products that
	\[ \A \mat W + \mat W \A^T + \B\B^T = -2\mu \mat W \]
	Defining $\P = \mat W^{-1}$ and pre- and post-multiplying this expression by $\P$ gives
	\[ \P \A + \A^T \P - \P\B\B^T\P = -2\mu \P \]
	Defining now with $\K = \frac{\B^T\P}{2}$ we can observe that the term $\P\B\B^T\P$ can be rewritten as $\frac{(\B^T\P)^T}{2} \B^T\P + \P\B \frac{\B^T\P}{2} = \K^T \B^T \P + \P\B\K$, thus collecting $\P$ in the previous equation gives
	\[ \P\big(\A-\B\K\big) + \big(\A^T - \K^T\B^T\big) \P = \P\big(\A-\B\K\big) + \big(\A^T  -\B \K\big)^T \P = -2\mu \P \tag{$\star$} \]
	Having both $\mu$ and $\P$ positive-definite, when it means that $\Q=2\mu\P$ is also positive-definite: with this assumption we see that ($\star$) is indeed in the same form of the Lyapunov equality (\ref{eq:ly:lyapequality}) for which by theorem \ref{th:lyap:continuoustime}, page \pageref{th:lyap:continuoustime}, implies that $\A -\B\K$ determine an exponentially stable system. 
	
	\begin{theorem}
		Given a continuous-time LTI system $\dx = \A \x+ \B \u$ based on the controllable pair $(\A,\B)$, then $\forall \mu > 0$ there exists a full-state feedback $\u = -\K \x$ that places all eigenvalues of the closed-loop system $\dx = (\A - \B\K)\x$ on the complex semi-plane characterized by $\Re s \leq - \mu$. In particular we call $\mu$ the \de{convergence rate} of the system.
		
		Similarly for discrete-time LTI plants $\xp = \A \x + \B\u$ a pair $(\A,\B)$ is controllable if and only if $\forall \mu \in(0,1)$ there exists a full-state feedback $\u = - \K\x$ that makes the closed-loop matrix $\A_{cl} = \A- \B\K$ Schur (so $|\lambda_i| < 1$ for all eigenvalues).
	\end{theorem}
	Note that this two conditions are a double implication, meaning that if the pair is controllable, then we can asses whichever convergence rate we want through a full-state feedback, but also if we can ensure any convergence rate then the pair $(\A,\B)$ must have been controllable in the first place.
	
\subsection{Controllable decomposition}
	What we would to do now is to use the \textbf{similarity transformations} seen at page \pageref{eq:dynsys:algebraicequivalence} to \textit{split} the \textbf{controllable states} from the \textbf{non-controllable} ones. In this case we consider as transformation the \textit{mapping} of the states $\tilde \x = \T^{-1} \x$ leading to a dynamics in the form
	\[ \dot{\tilde\x}/\tilde\x^+ = \tilde \A \tilde\x + \tilde \B \u = \T^{-1}\A\T\tilde \x + \T^{-1}\B\u\]
	\begin{theorem}
		The pair $(\A,\B)$ is controllable if and only if the pair $(\T^{-1}\B, \T^{-1}\A\T)$ is controllable for any non-singular $n\times n$ matrix $\T$.
	\end{theorem}
	The main takeaway from this theorem is that controllability is a property of the system and do not depend on the realization chosen (if a system is not controllable it doesn't exists a state remapping function that turns it controllable).
	\begin{proof}
		To prove this theorem we simply need to compute the reachability matrix of the equivalent system that's
		\begin{align*}
			\tilde \R & = \matrix{\tilde \B & \tilde \A\tilde\B &\dots & \tilde \A^{n-1}\tilde \B} = \matrix{\T^{-1} \B & \T^{-1}  \A\T\T^{-1} \B & \dots & \T^{-1} \A \T \T^{-1} \A \T \dots \T^{-1}  \B  } \\
			& = \matrix{\T^{-1} \B & \T^{-1}  \A\B & \dots & \T^{-1}  \A^{n-1}\B } = \T^{-1}\matrix{\A & \A\B & \dots & \A^{n-1}\B} \\
			& = \T^{-1} \R
		\end{align*}
		Being $\T$ (and so $\T^{-1}$) non-singular, then the rank of the matrices after the similar transformations remains unchanged, in particular $\rank{\tilde \R} = \rank \R= n$: having that both reachability matrix have maximum rank (being $(\A,\B)$ controllable), then surely system $(\T^{-1}\B, \T^{-1}\A\T)$ is controllable. 
	\end{proof}

	Given now a generic LTI system $\dx /\xp = \A\x + \B u$: constructing $\V$ as the $n\times \overline n$ matrix show columns are forming a base of the reachable subspace $\reach$ of the system (where in particular $\overline n$ is the the dimension of the reachable set), then the following holds:
	\begin{enumerate} [\itshape i)]
		\item the reachable set $\reach = \image{\R}$ is $\A$-invariant, meaning that exists a $\overline n\times \overline n$ matrix $\A_c$ such that
		\[ \A \V = \V \A_c \tag{$\dagger$} \]
		\item the image of the matrix $\B$ is contained in the reachable set, mathematically $\image{\B} \subset \reach$, and the columns of $\B$ can be regarded as a linear combination of the columns of $\V$ determined with the multiplication by the $\overline n \times m$ matrix $\B_c$:
		\[ \B= \V \B_c \]
	\end{enumerate}
	\begin{proof}
	\begin{enumerate}[\itshape a)]
		\item To prove \textit{i)}, if we consider a vector $\x \in \image \R$, then it means that exists a vector $\vet \beta\in\mathds R^n$ satisfying $\x = \R \vet \beta$, thus
		\[ \x = \R \vet \beta \matrix{\B&\A\B &\dots & \A^{n-1}\B} \vet \beta = \sum_{k=0}^{n-1}\A^k\B \beta_k \]
		Pre-multiplying everything by $\A$ gives
		\[ \A\x = \A\R\vet \beta = \sum_{k=0}^{n-1} \A^{k+1} \B \beta_k = \sum_{k=0}^{n-2} \A^{k+1} \B \beta_k + \A^n \B \beta_{n-1} \]
		Exploiting Cayley-Hamilton theorem, page \pageref{th:cayley}, we can rewrite the last term as
		\[ \A^n \B \beta_{n-1} = \sum_{i=0}^{n-1} \alpha_i \A^i \B \beta_{n-1} \]
		Calling now $\tilde{\vet \beta}$ the difference $\vet \beta-\vet \alpha$ allows us to rewrite the product $\A\x$ as
		\[ \A\x = \sum_{k=0}^{n-1}\A^k \B \tilde\beta_k = \R \tilde{\vet \beta} \]
		This proves that for any vector $\x$ in the reachable sub-set, than also $\A\x$ lies in the same set, proving the $\A$-invariance of the matrix $\V$ (that's representing in fact $\image{\R}$).
		
		\item The proof of \textit{ii)} is straightforward: it's obvious that $\image\B \subseteq \reach$ by observing that
		\[ \image{\B} \subseteq \image{\R} = \image{\matrix{\B & \A\B & \dots & \A^{n-1}\B}} = \reach \]
		thus $\B$ can always be regarded as a linear combination of a basis of the reachable subset, hence as the product $\V \B_c$.
	\end{enumerate}
	\end{proof}
	Given so the basis of $\R$ determining the matrix $\vet V$ and calling $\V_c\in \mathds R^{n\times(n-\overline n)}$ it's \textbf{completion} of $\mathds R^n$ (so such that the columns of $\matrix{\V & \V_c}$ are a basis of $\mathds R^n$), then the \de{transformation matrix} $\T$ defined as
	\begin{equation}
		\T = \matrix{\V & \V_c}
	\end{equation}
	allows us to perform the \de{controllable decomposition}, so to separate the controllable states from the unobservable ones.
	
	Computing in fact the product
	\[\A\T= \matrix{\A \V & \A \V_c} \stackrel{(\dagger)}{=} \matrix{ \V \A_c & \A \V_c }\]
	Recalled that $\A_c$ is a $\overline n\times \overline n$ matrix, we can also observe that the product $\A\T$ can be further decomposed as
	\[ \A \T = \matrix{ \matrix{\V & \V_c} \matrix{\A_c \\ 0} & \A \V_c } = \matrix{\T \matrix{\A_c \\ 0} & \T\T^{-1}\A\V_c } = \T \matrix{\A_c & \multirow{2}{*}{$\T^{-1} \A \V_c$} \\ 0  } \]
	In particular $\T^{-1} \A\V_c$ is a $n\times (n-\overline n)$ matrix that can be decomposed into two submatrices $\A_{12} \in \mathds R^{\overline n\times(n-\overline n)}$ and $\A_u \in \mathds R^{(n-\overline n)\times(n-\overline n)}$ in such a manner that it allows to rewrite $\A \T$ as
	\[ \A\T = \T \matrix{\A_c & \A_{12} \\ 0 & \A_u} \]
	So finally
	\begin{equation} \label{eq:gram:controllabledecomp}
		\tilde \A = \T^{-1}\A\T = \matrix{\A_c & \A_{12} \\ 0 & \A_u}
	\end{equation}
	In particular what we have is that only the last $n-\overline n$ rows, so the block $\matrix{0&\A_u}$, is representing the dynamic of the \textbf{non-controllable states}: knowing that $\B$ can be regarded as $\V \B_c$, this also implies 
	\[ \B = \matrix{\V&\V_c}\matrix{\B_c\\0} = \T \matrix{\B_c \\ 0} \]
	thus for the similarity transformation
	\[ \tilde \B= \T^{-1}\B = \cancel{\T^{-1}\T} \matrix{\B_c\\0} \]
	This shows that in the transformed matrix, only the first $\overline n$ states can be accessed by the input (as $\B_c$ is a $\overline n \times m$ matrix), showing that the last $n-\overline n$ remains untouched by the inputs $\u$, proving that they are \textbf{non-controllable}. In particular the sub-matrix $\A_u$ in (\ref{eq:gram:controllabledecomp}) is the matrix ruling the \textbf{dynamics} of the \textbf{uncontrollable \textit{part}} of the system.
	
	Summarizing, we call \de{controllable decomposition} of the pair $(\A,\B)$ the matrices
	\begin{equation} \label{eq:gram:controllabledecomposition}
		\tilde \A = \T^{-1}\A\T = \matrix{\A_c & \A_{12} \\ 0 & \A_u} \hspace{2cm} \tilde \B = \T^{-1}\B = \matrix{\B_c \\ 0}
	\end{equation}
	\begin{theorem}
		Each LTI system characterized by a pair $(\tilde \B,\tilde \A)$ in the form $\left(\matrix{\B_c \\ 0},\matrix{\A_c & \A_{12} \\ 0 & \A_u} \right)$ satisfies:
		\begin{enumerate}[\itshape i)]
			\item the reachable subspace is given by
			\[ \image{\tilde \R} = \image{\matrix{\I_{\overline n\times \overline n} \\ 0}} \]
			\item the pair $(\B_c,\A_c)$ is reachable/controllable.
		\end{enumerate}
	\end{theorem}
	
	
	
	