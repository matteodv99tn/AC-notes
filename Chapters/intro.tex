\chapter{Dynamical systems and linear systems theory}

	This course will treat the \de{automatic control} of \de{dynamical systems}, hence it's necessary to firstly understand what a \textit{dynamical system} is and how it's described. In general such systems can be regarded as \textit{black box} that for a given \textbf{input} $u(t)$ determines an \textbf{output} $y(t)$; systems can be \textbf{continuous} or \textbf{discrete-time} depending on the type of \textit{time axis scale}, but also \textbf{hybrid} systems exists (as it will be studied).
	
	\paragraph{Rabbits} In order to understand behaviours of dynamical system, let's consider the example of pairs of rabbit. If we describe with $y(t)$ the total number of rabbits couple that one person has (where $t$ is the time expressed in months), we can consider that initially a person start with no animals ($y(0) = 0$) and goes at $t=1$ at an animal shop to buy a couple of small rabbits (hence $y(1) = 1$). In the first month of life they grow up and from \textit{babies} they become \textit{adults}, but the person still has $y(2) = 1$ couple of rabbit. In the next month they procreate a new couple of rabbits and so we have $y(3) = 2$ rabbit pairs. Assuming that no rabbit dies, we can describe the evolution of the system as
	\begin{equation} \label{eq:dyn:rabbitdiscrete}
		y(n) = y(n-2) + y(n-1)
	\end{equation}
	where the term $y(n-2)$ relates to the elder population and $y(n-1)$ describes the newly borne couple from each pair of elder pair. Equation \ref{eq:dyn:rabbitdiscrete} is a \textbf{finite difference equation} that allows to recursively define the number of rabbit pairs depending on the time $t$ assuming to know the \textbf{initial conditions} $y(0)$ and $y(1)$.
	
	For such system we can define it's \de{states} $x_1(t) = y(t)$ and $x_2 = y(t-1)$ determining the vector $x(t)$:
	\begin{equation}
		x(t) = \begin{pmatrix}
			x_1(t) \\ x_2(t)
		\end{pmatrix}
	\end{equation}
	Given so the states $x(t)$, we can compute their evolution $x(t+1)$ as
	\[ x(t+1) = \vet{x_1(t+1) \\ x_2(t+1)} = \vet{y(t+1) \\ y(t)} = \vet{y(t-1) + y(t) \\ y(t)} = \vet{x_2(t) + x_1(t) \\ x_1(t)}\]
	Using linear algebra the evolution of the state can be regarded as a linear combination of the actual states, in fact
	\begin{equation} \label{eq:dyn:temp4}
		x(t+1) = \mat{1 & 1 \\ 1 & 0} \vet{x_1(t) \\ x_2(t)} = \A x(t)
	\end{equation}
	Having the eigenvalues of the matrix $\A$ equal to $\lambda_{1,2} = \frac{1 \pm \sqrt 5}{2}$ we will prove that the continuous explicit function that express the rabbit pairs $y$ as function of $t$ is
	\[ y(t) = \frac1{\sqrt{5}} \left[ \left(\frac{1 + \sqrt 5}{2}\right)^t - \left(\frac{1 - \sqrt 5}{2}\right)^t \right] \]
	
	\paragraph{Classification of system} This example allowed to intrude the first classification of system: they in fact can be \de{linear} if the states variation and output can be regarded as linear combination of the actual states and inputs, meaning that they can be expressed as
	\begin{equation} \label{eq:dyn:temp1}
	\begin{cases}
		x(t+1) = \A x(t) + \B u(t) \\ y(t) = \C x(t) + \D u(t)
	\end{cases}
	\end{equation}
	where $\A$ is the \textbf{dynamic matrix}, $\B$ the \textbf{input} m., $\C$ the \textbf{output} m. and $\D$ is the \textbf{feed-through} (instantaneous) m.; both $u,x$ and $y$ are in general vectors. To simplify the notation usually the dependency on the time $t$ is dropped and the increment for \textbf{discrete-time} systems (where $t\in \mathds Z$) is described as $x^+$ (values of the state at the next step), hence equation \ref{eq:dyn:temp1} is rewritten as
	\begin{equation} \label{eq:dyn:LTIdiscrete}
		\begin{cases}
			x^+ = \A x + \B u \\ y = \C x + \D u \\ x(t_0) = x_0
		\end{cases}
	\end{equation}
	For \textbf{continuous-time} systems (where $t\in \mathds R$) the concept of increment is represented by the derivative $\dot x = \frac{dx}{dt}$, hence
	\begin{equation} \label{eq:dyn:LTIcontinuous}
		\begin{cases}
			\dot x = \A x + \B u \\ y = \C x + \D u \\ x(t_0) = x_0
		\end{cases}
	\end{equation}
	Note that for all the system is very important to define the \textbf{initial condition} $x(t_0) = x_0$ of the states because the response of the output is strictly related to them.
	
	System that don't exhibits a linear behaviour are therefore called \de{non-linear} and the input/state/output relations are described by functions:
	\begin{equation}
	\begin{cases}
		\delta x = f(x,u) \\
		y = h(x,y) \\
		x(t_0) = x_0
	\end{cases}
	\end{equation}
	\begin{note}
		the term $\delta x$ is the generalization of $\dot x$ for continuous-time or $x^+$ for discrete-time systems in order to make the notation more generalized and will be used throughout the whole book.
	\end{note}
	
	Until now the notation were referred to \de{time-invariant} systems where the matrix $\A,\B\,\C,\D$ (for linear sys.) or the functions $f,h$ (for non-linear sys.) were not dependent on time, however \de{time-variant} systems also exists that exhibiting an explicit relation with time, hence can be regarded as
	\begin{equation}
		\begin{cases}
			\delta x = \A(t) x + \B(t) u \\ y = \C(t) x + \D(t) u \\ x(t_0) = x_0
		\end{cases}
		\qquad \textrm{or} \qquad 
		\begin{cases}
			\delta x = f(x,u,t) \\
			y = h(x,y,t) \\
			x(t_0) = x_0
		\end{cases}
	\end{equation}
	for both linear and non-linear case.\\
	By a mathematical point of view a system if it satisfy that
	\[ \begin{cases}
		x(t_0 + T) = x_0 \\ u(t-T) 
	\end{cases} \qquad \Rightarrow \quad y(t-T) \hspace{2cm} \forall t \geq t_0 + T \]
	
	\textbf{ESEMPI DEI DIVERSI TIPI DI SISTEMI}
	
	\begin{example}{: linear time-variant dynamical system}
		Let's consider the model of a 3 year course of an university with the goal of study the behaviour of students. Considering the graphs that follows, we denote as $u$ the number of students registering at the first year, while $\alpha_i \in [0,1]$ is the relative value of students that passes to the next year/graduates; $y$ finally represent the number of student graduated. In this case the states $x_i$ are representing the number of student in each year of  the course. This models assumes that no student drops the course (more complex models can indeed consider such phenomena).
		\begin{center}
			\tikzfigure{university}
		\end{center}
		
		We can see that this dynamical system is discrete-time (the system is considered to evolved each year at the start of the academic year); analysing the graph we observe that each next year the number of student in each \textit{class} ($x_i$) depends on the number of the one passed from the previous course ($\alpha_i x_{i-1}$) and the one that didn't passed ($(1-\alpha_i)x_i$), hence we have that
		\[\begin{cases}
			x_1^+ & = (1-\alpha_1)x_1 + u \\
			x_2^+ & = (1-\alpha_2)x_2 + \alpha_1 x_1 \\
			x_3^+ & = (1-\alpha_3)x_3 + \alpha_2 x_2 \\
			y & = \alpha_3 x_3
		\end{cases}\]	
		In this case if we assume that $\alpha_i$ are constant value, the model that we obtain doesn't suit the real behaviour of the course where problems might happen (for example a \textit{evil teacher} reduces the rate of passed students one year, reducing $\alpha_i$),	hence we have a system that's time-variant ($\alpha_i$ depends on time $t$) while the model is still linear, because it can be expressed as
		\begin{align*}
			x^+ = \vet{x_1^+ \\ x_2^+ \\ x_3^+} & = \mat{1-\alpha_1(t) & 0 & 0 \\ \alpha_1(t) & 1-\alpha_2(t) & 0 \\ 0 & \alpha_2(t) & 1-\alpha_3(t)} \vet{x_1 \\ x_2 \\ x_3} + \vet{1 \\0 \\0 } u = \A(t)x + \B(t)u \\
			y & = \mat{0 & 0 & \alpha_3(t)} \vet{x_1 \\ x_2 \\ x_3} + \mat{0} u = \C(t)x + \D(t) u
		\end{align*}
	\end{example}
	
\section{Linear time-invariant systems and transfer functions}
	As equations \ref{eq:dyn:LTIdiscrete} and \ref{eq:dyn:LTIcontinuous} stated, \de{linear time-invariant} \textbf{LTI} systems can be described in matrix form as
	\[ \begin{cases}
		\delta x = \A x + \B u \qquad x(0) = x_0\\ y = \C x + \D u 
	\end{cases} \]
	where the matrix $\A,\B,\C,\D$ are constant (hence independent from time due to the time-invariancy requirement). Having the system time invariant the initial condition can be expressed as $x(0)$ (because it doesn't depend on the choice of $t_0$, but on any general initial condition).
	
	Continuous-time LTI systems can be easily analysed in the domain of the complex variable $s$ using the \textbf{Laplace transform} $\mathscr L$, an operator defined
	\[ X(s) = \laplace{x(t)} = \int_0^\infty x(t) e^{-st}\, dt \hspace{2cm} s \in \mathds C \]
	An important property of such operator is that converts differential equations into algebraic ones in the variable $s$, in fact we can observe that
	\[ \dot x(t) \mapsto s X(s) - x(0) \]
	The Laplace transform has also the useful property of being linear, hence the input-output relation of LTI system can be transformed according to Laplace, hence
	\begin{equation}
		(eq.\ref{eq:dyn:LTIcontinuous}) \mapsto \begin{cases}
			sX(s) - x(0) = \A X(s) + \B U(s) \\ Y(s) = \C X(s) + \D U(s)
		\end{cases}
	\end{equation}
	The first equation can be solved in order to explicit the transform of the states $X(s) = \laplace{x(s)}$ as function of the transform of the input $U(s) = \laplace{u(t)}$ determining
	\begin{align*}
		sX(s) - \A X(s) & = \B U(s) + x(0) \\
		(s\I-\A) X(s) & = \B U(s) + x(0) \\
		X(s) & = (s\I - \A)^{-1} \B U(s) + (sI- \A)^{-1} x(0) 
	\end{align*}
	\begin{note}
		In the first step in order to collect $sX(s)$ and $\A X(s)$ has been necessary to multiply $s$ by the identity matrix $\I$: $s$ is in fact a scalar while $\A$ is a matrix and no operation is compatible between this elements (addition is defined only for matrix with same sizes).
	\end{note} \noindent
	With this result obtained we can compute the transform of the output $Y(s) = \laplace{y(t)}$ as function of the lonely $U(s)$ as 
	\begin{equation}
		Y(s) = \underbrace{\left(\C (s\I - \A)^{-1} \B + \D\right)}_{= \hat \G(s)} U(s) + \underbrace{\C(s\I-\A)^{-1} \D}_{\hat \psi(s)} x(0)
	\end{equation}
	In this equation $\hat \G(s)$ represent the \de{transfer function} of the system (however more generally it's a matrix). Observing that the output in the Laplace domain can be so simplified to the form $Y(s) = \hat G(s) U(s) + \hat \psi(s) x(0)$ and so by using property of the (inverse) Laplace transform we can rewrite the output as the convolution of the input sequence and the \de{impulse response} $G(t) = \mathscr L^{-1}\{\hat G(s)\}$ of the system:
	\begin{equation}
		y(t) = (G*x)(t) + \psi(t) x(0)
	\end{equation}
	where the convolution is defined as
	\[ (G*x)(t) = \int_0^t G(t)x(t-\tau)\, d\tau \]
	
	Considering the case of discrete-time LTI  systems instead of the Laplace we have to use the \textbf{Z transform} $\Z$ that maps discrete time sequences into a domain in the complex variable $z$; knowing that $X(z) = \ztransf{x(t)}$ (with $t\in \mathds Z$) the analogous property of differentiation in time is that $x(t+1) = x^+ \mapsto z X(z) - z x(0)$, hence
	\begin{equation}
		(eq.\ref{eq:dyn:LTIdiscrete} )\mapsto \begin{cases}
			z X(z) - z x(0) = \A X(z) + \B U(z) \\ Y(z) = \C X(z) + \D U(z)
		\end{cases}
	\end{equation}
	As in the continuous case it's possible to compute the transform of the output $Y(z) = \ztransf{y(t)}$ as function of the spectrum $U(z) = \ztransf{x(t)}$ as
	\begin{equation}
		Y(z) = \underbrace{\left(\C (z\I - \A)^{-1} \B + \D\right)}_{= \hat \G(z)} U(z) + \underbrace{\C(z\I-\A)^{-1} \D z}_{\hat \psi(z)} x(0)
	\end{equation}

	\paragraph{Theorem} A fundamental theorem for continuous-time linear time-invariant systems states that \textit{the \textbf{transfer function} $\hat \G(s)$ and so the \textbf{impulse response} $\G(t)$ of such systems is computed as}
	\begin{equation}
		\hat \G(s) = \left(\C (s\I - \A)^{-1} \B + \D\right) \hspace{2cm} \G(s) = \antilaplace{\left(\C (s\I - \A)^{-1} \B + \D\right)}
	\end{equation}
	
\section{Interconnected systems}
	The control of dynamical systems is based on \de{interconnections} between, and such connection can be summarized as: in parallel, in series, in feedback. In this section a proof of transfer function of interconnected system is given in the case of linear time-invariant system (where the concept of transfer function is \textit{well defined}). Such system represented as 
	\[ \begin{cases}
		\delta x = \A x + \B u \qquad x(0) = x_0\\ y = \C x + \D u 
	\end{cases} \]
	are so characterized by the 4 matrix $\A,\B,\C,\D$ by the \textit{big matrix notation}
	\[ \vet{\delta x \\ \hline y} = \begin{bmatrix}
		\begin{array}{c | c}
			\A & \B \\ \hline \C & \D
		\end{array} 
	\end{bmatrix} \vet{x \\ u }\]
	
	\subsection*{Parallel connection}
		\begin{figure}[b!]
			\centering
			\tikzfigure{parallel}
			\caption{black box representation of a parallel connection between two systems.} \label{fig:dyn:parallel}
		\end{figure}
		Considering the parallel connection of the systems $\Sigma_1,\Sigma_2$ (figure \ref{fig:dyn:parallel}), the overall transfer function of the system $\Sigma$ can be computed considering that the output $Y(\cdot)$ in the Laplace/Z domain is the sum of the contribution of the single stages that can be computed so independently
		\begin{equation}
		\begin{aligned}
			Y &  = Y_1 + Y_2 = \hat \G_1 U_1 + \hat \G_2 U_2 = \hat \G_1 U + \hat G_2 U \\
			& = \underbrace{\big(\hat \G_1  + \hat \G_2\big)}_{=\hat \G} U
		\end{aligned}
		\end{equation}
		
		While connection two or more systems we have that the overall states $x$ of the system $\Sigma$ is the union of the individual systems $x_i$, hence in this case
		\[ x = \vet{x_1 \\ x_2} \]
		More formally the we have that the variation of the states $\delta x$ of the system $\Sigma$ can be regarded as
		\[ \delta x = \A_1 x_1 + \B_1 u_1 + \A_2 u_2 + \B_2 u_2 = \mat{\A_1 & 0 \\ 0 & \A_2} \vet{x_1 \\ x_2} + \mat{\B_1 \\ \B_2} u \]
		and similarly the output in the time domain
		\[ y = \C_1 x_1 + \D_1 u_1 + \C_2 x_2 + \D_2 x_2 = \mat{\C_1 & \C_2} \vet{x_1 \\ x_2} + \vet{D_1+D_2} u \]
		The associated \textit{big matrix} is so
		\begin{equation} 			
			\vet{\delta x_1 \\ \delta x_2 \\ y} = \mat{\begin{array}{c c | c}
					\A_1 & 0 & \B_1 \\ 0 & \A_2 & \B_2 \\ \hline \C_1 & \C_2 & \D_1 + \D_2
			\end{array}} \vet{x_1 \\ x_2 \\ u}
		\end{equation}
	
	\subsection*{Series (cascade) connection}
		A series connection (figure \ref{fig:dyn:series}) between two system is implemented by feeding the input of $\Sigma_2$ with the output of $\Sigma_1$; in this case the input-output of the overall system can be regarded in the Laplace/Z transform as
		\begin{equation}
		\begin{aligned}
			Y & = \hat \G_2 U_2 = \hat \G_2 Y_1 = \hat \G_2 \hat \G_1 U_1 \\ & = \hat \G_2 \hat \G_1  U = \hat \G U
		\end{aligned}
		\end{equation}
		Note that in this case the order of the transfer function $\hat \G= \hat \G_2 \hat \G_1$ is important due to the multiplication of matrix; in fact in order to make the series connection possible we need to have that the length of the vector $y_1$ is equal to the one of $x_2$: this makes possible to have a matrix $\hat \G$ the can be multiplied by $U$ and returns a proper $Y$.
	
		Carefully representing this system in the space of the states, we obtain that
		\begin{align*}
			\delta x_1 & = \A_1 x_1 + \B_1 u \\
			y_1 & = \C_1 x_1 + \D_1 u = u_2 \\
			\delta x_2 & = \A_2 x_2 + \B_2 u_2 = \A_2 x_2 + \B_2\big( \C_1 x_1 + \D_1 u \big) \\
			y= y_2 & = \C_2 x_2 + \D_2 u_2 = \C_2 x_2 + \D_2 \big(\C_1 x_1 + \D_1 u\big)
		\end{align*}
		hence the \textit{big matrix} is
		\begin{equation}
			\mat{ \begin{array} {c c | c}
					\A_1 & 0 & \B_1 \\ 
					\B_2 \C_1 & \A_2 & \B_2 \D_1 \\ \hline 
					\D_2 \C_1 & \C_2 & \D_2\D_1
			\end{array} }
		\end{equation}
		
		\begin{figure}[bht]
			\centering
			\tikzfigure{series}
			\caption{black box representation of a series connection between two systems.} \label{fig:dyn:series}
		\end{figure}
	
	\subsection*{Feedback connection}	
		\begin{figure}[bt]
			\centering
			\tikzfigure{feedback}
			\caption{black box representation of a series feedback connection of a system.} \label{fig:dyn:feedback}
		\end{figure}
		
		A feedback connection (figure \ref{fig:dyn:feedback}) of a system is realised by subtracting to the input $u$ of the system it's output $y$. Performing the analysis in the frequency domain we have have in fact the solution
		\[ Y = \hat\G_1 U_1 = \hat\G_1 (U-Y) \]
		We see that this expression is implicit in $Y$, but we can still obtain the solution
		\begin{equation}
		\begin{split}
			Y + \hat\G_1 Y = Y(\I + \hat \G_1)& =\hat\G_1 U \\
			Y &= (\I + \hat \G_1)^{-1} \hat \G_1 U
		\end{split}
		\end{equation}
		This solution is obtained by inverting the matrix $\I + \hat \G_1$ and this operation is possible if the matrix is \textit{\de{well-posed}}, condition that happens every time the matrix $\I + \D_1$ is invertible. By performing the analysis in the states space we have in fact 
		\begin{align*}
			y & = \C_1 x_1 + \D_1 u_1 = \C_1 x_1 + \D_1 (u-y) \\
			(\I + \D_1) y & = \C_1 x_1 + \D_1 u \\
			y & = (\I + \D_1)^{-1}\C_1 x_1 +(\I + \D_1)^{-1} \D_1 u
		\end{align*}
		From this expression is possible to observe why the well-posedness condition is related to having $\I + \D_1$ invertible. Expliciting the variation of the states we have instead we have
		\begin{align*}
			\delta x & = \A_1 x_1 + \B_1 u_1 = \A_1x_1 + \B_1 (u-y) \\
			& = \A_1 x_1 + \B_1 u - \B_1 \left( (\I + \D_1)^{-1}\C_1 x_1 +(\I + \D_1)^{-1} \D_1 u \right) \\
			& = \left( \A_1 - \B_1 (\I+\D_1)^{-1} \C_1 \right) x_1 + \left( \B_1 - \B_1 (\I+\D_1)^{-1} \D_1 \right)u
		\end{align*}
		From this states space expansion we can define the \textit{big matrix} associated to a feedback connection as
		\begin{equation}
			\mat{ \begin{array} {c | c}
				\A_1 - \B_1(\I+\D_1)^{-1} \C_1 & \B_1\left(\I - (\I+\D_1)^{-1} \D_1\right) \\ \hline 
				(\I+\D_1)^{-1}\C_1 & (\I+\D_1)^{-1} \D_1
			\end{array} } = 
			\mat{ \begin{array} {c | c}
				\A_1 - \B_1(\I+\D_1)^{-1} \C_1 & \B_1 (\I+\D_1)^{-1} \\ \hline 
				(\I+\D_1)^{-1}\C_1 & \D_1 (\I+\D_1)^{-1} 
		\end{array} }
		\end{equation}
	
\section{Realization theory}
	Given a rational polynomial $g(s)$ so defined as the ratio between two polynomial $n(s)/d(s)$ (this expression indeed represent the case of a one dimensional transfer function), such function is said
	\begin{itemize}
		\item \textbf{strictly proper} if the degree $\#n(s)$ of the denominator is less then the degree $\#d(s)$ od the denominator; this means that
		\[ \lim_{s\rightarrow \infty} g(s) = 0 \]
		\item \textbf{proper} if in general we have $\#n(s) \leq \#d(s)$, but more specifically it has to be
		\[ \lim_{s\rightarrow \infty} g(s) \neq \infty \]
		\item every time we have $\#n(s) > \#d(s)$, the rational polynomial is defined as \textbf{improper} and has
		\[ \lim_{s\rightarrow \infty} g(s) = \infty \]
	\end{itemize}	
	A rational polynomial $G(s)$ is \textbf{biproper} if both $g$ and it's inverse $g^{-1}$ are propers. Extending this concepts to a transfer function $\hat \G(s)$ (that's in general a matrix), the system is (strictly) proper if all it's entries are (strictly) proper; if one entry is improper, then $\hat \G(s)$ is also improper.
	
	In general it's proven that for linear time-invariant systems the transfer function $\hat G(s)$ is composed by a strictly proper and a proper term as
	\begin{equation} \label{eq:dyn:temp2}
		\hat \G(s) = \underbrace{\C(s\I - \A)^{-1}\B}_\textrm{strictly proper} + \underbrace{\D}_\textrm{proper}
	\end{equation}
		
	\paragraph{Realization} Given a transfer function $\hat \G(s)$, we say that the linear time-invariant system in the form
	\[ \begin{cases}
		\delta x = \A x + \B u \qquad x(0) = x_0\\ y = \C x + \D u 
	\end{cases} \] 
	is a \de{realization} of the transfer function $\hat G$ if equation \ref{eq:dyn:temp2} holds. \\
	This states that a LTI system characterized by the matrices $\A,\B,\C,\D$ is the realization of $\hat G(s)$ if it happens that $\hat G(s) = \C(s\I - \A)^{-1}\B + \D$. We also define two realization as \de{zero state equivalent} if they realise the same transfer function $\hat G(s)$.
		
	\subsection*{Realizations and transfer functions}
		An important theorem states that a transfer function $\hat \G(s)$ can be \textbf{realised} by an LTI system in the \textit{standard form} \textbf{if and only if} the rational function $\hat \G(s)$ is \textbf{proper}.
		
		\paragraph{Proof pt. 1} We can prove the \textit{only if} conditions stating
		\[ \mat{\begin{array}{c|c}
				\A & \B \\ \hline \C & \D
		\end{array}} \mapsto \hat \G(s) \]
		the proof is straightforward: as \ref{eq:dyn:temp2} states, given the constant matrices $\A,\B,\C,\D$ the resulting transfer function is strictly proper for the term $\C(s\I-\A)^{-1}\B$ and becomes proper only in the case when $\D\neq 0$.
		
		\paragraph{Proof pt. 2} More complex is proving the \textit{if} condition that starting from the transfer function computes the \textit{characteristic} matrices of LTI systems:
		\[ \hat \G(s) \mapsto \mat{\begin{array}{c|c}
				\A & \B \\ \hline \C & \D
		\end{array}}  \]
		We want now to prove that if the transfer function $\hat G(s)$ is proper then it state space representation exists. Determining the non-strictly proper element $\D$ is straightforward, in fact it can be computed simply as the limit
		\[ \mathcal D = \lim_{s\rightarrow \infty} \hat \G(s) \]
		We can so build the strictly proper transfer function as
		\[ \hat \G_{sp}(s) = \B(s\I - \A)^{-1}\C = \Gs - \D\]
		Building the polynomial $d(s)$ as the least common denominator of all the entries in the matrix $\hat \G_{sp}(s)$ and writing him in a form
		\[ d(s) = s^n + \alpha_1 s^{n-1} + \alpha_2 s^{n-2}+ \dots + \alpha_{n-1} s + \alpha_n \]
		we can rewrite the strictly proper part of the transfer function as a combination of matrices $N_i$ in the following way:
		\[ \hat \G_{sp}(s) = \frac{N(s)}{d(s)} = \frac{N_1 s^{n-1} + N_2 s^{n-2} + \dots + N_{n-1} s + N_n }{d(s)} \]
		With that said we have all the elements to compute the state space representation of the system: it can be proven in fact that the LTI has a state space representation in the form
		\begin{equation} \label{eq:dyn:Gtostaterep}
			\mat{\begin{array}{ c c c c | c}
				-\alpha \I_k & - \alpha_2 \I_k & \dots & - \alpha_n \I_k & \I_k \\
				\I_k & 0 & \dots & 0 & 0 \\
				0 & \ddots & & 0 & \vdots \\
				0 & 0& \I_k & 0 & 0 \\ \hline
				N_1 & N_2 & \dots & N_m & \D
			\end{array}}
		\end{equation}
		where $\I_k$ is the $k\times k$ identity matrix; this notation is called \de{controllable canonical form}.
		
		\begin{example}{: state representation from a transfer function}
			Given a system characterized by a transfer function
			\[ \Gs = \mat{\frac{4s-10}{s+1} & \frac{3}{s+2} \\ \frac{1}{s+2} & \frac{4}{s+1} } \]
			in order to compute one it's representation the first thing to do is extract the output matrix $\D$ as
			\[ \D = \lim_{s\rightarrow \infty} \Gs = \mat{4 & 0 \\ 0 & 0} \]
			And so we can compute the strictly  proper part as
			\begin{align*}
				\hat G_{sp}(s) & = \Gs - \D = \mat{\frac{-14}{s+1} & \frac{3}{s+2} \\ \frac{1}{s+2} & \frac{4}{s+1} } \\
				& = \frac 1 {(s+1)(s+2)} \mat{-14(s+2) & 3 (s+1) \\ 1(s+1) & 4(s+2)} = \frac 1{s^2 + 3s + 2} \mat{-14 s - 28 & 3s + 3 \\ s+1 & 4s + 8} \\
				& = \frac{\mat{-14 & 3 \\ 1 & 4}s + \mat{-28 & 3 \\ 1 & 8} }{s^2 + 3s +2}
			\end{align*}
			Using equation \ref{eq:dyn:Gtostaterep} we can compute the state representation of the system using the yet computed denominator and matrices $N_i$ providing the result
			\[ \mat{\begin{array}{c c c c | c c}
				-3 & 0 & -2 & 0 & 1 & 0 \\
				0 & -3 & 0 & -2 & 0 & 1 \\
				1 & 0 & 0 & 0 & 0 & 0 \\
				0 & 1 & 0 & 0 & 0 & 0 \\ \hline
				-14 & 3 & -28 & 3 & 4 & 0 \\ 
				1 & 4 & 1 & 8 & 0 & 0
			\end{array}} \]
			By computing $\C (s\I - \A)^{-1} \B + \D$ using the matrices yet obtain we can indeed see that, after some simplifications, that we obtain the original transfer function $\Gs$.
		\end{example}
		
		\paragraph{Proof pt. 3} All that's left now is to prove that equation \ref{eq:dyn:Gtostaterep} indeed is a realization of the proper transfer function $\Gs$. In order to do so we consider we consider the sub-matrix $z(s)$ defined as
		\[ z(s) = \mat{z_1^t & z_2^t & \dots & z_n^t} = (s\I - \A)^{-1} \B \qquad \Rightarrow \qquad (s\I-\A) z(s) = \B \]
		Considering the \textit{lower part} of the matrices $\A$ and $\B$ (the last 3 rows in equation \ref{eq:dyn:Gtostaterep}), the multiplication results in the set of equalities
		\[ s z_2 -z_1 = 0 \qquad sz_3 - z_2 = 0 \qquad \dots \qquad sz_n = z_{n-1} \]
		Considering so that $z_n = \frac 1 s z_{n-1}$ but also $z_{n-1} = \frac 1 s z_{n-2}$ up to $z_2 = \frac 1 2 z_1$ we can determine the generic vector $z_k$ as
		\begin{equation} \label{eq:dyn:temp3}
			z_k = \frac 1{s^{k-1}} z_1
		\end{equation}
		Considering so now the multiplication $(s\I - \A)z(s) = \B$ using the top row of $\A,\B$ respect to the formulation in equation \ref{eq:dyn:Gtostaterep}, we have that
		\[ (s+\alpha_1) z_1 + \alpha_2 z_2 + \dots + \alpha_n z_n = \I_k \]
		hence for relation \ref{eq:dyn:temp3}
		\[ \left( s + \alpha_1 + \frac{\alpha^2}{s} + \dots + \frac{\alpha_{n-1}}{s^{n-2}} + \frac{\alpha_n}{s^{n-1}} \right) z_1 = \I_k \]
		Since the polynomial in the left-hand side is given by the ratio $\frac{d(s)}{s^{n-1}}$ we can define the sub-matrix $z$ as
		\[ z(s) = \mat{z_1 \\ z_2 \\ \vdots \\ z_n} = \frac 1 {d(s)} \mat{s^{n-1}\I_k \\ s^{n-2}\I_k \\ \vdots \\ \I_k} \]
		hence
		\begin{align*}
			\C(s\I-\A)^{-1} \B & = \C z(s) = \frac 1{d(s)} \mat{N_1 & N_2 & \dots  & N_n} \mat{s^{n-1}\I_k \\ s^{n-2}\I_k \\ \vdots \\ \I_k} \\
			& = \hat \G_{sp}(s)
		\end{align*}
		
		\subsubsection{LTI SISO system case and multiple realizations}		
		 A SISO (single-input single-output) LTI system is characterized by a transfer function that's simply a rational polynomial in the form
		\[ \Gs = \frac{\beta_1 s^{n-1} + \beta_2 s^{n-2} + \dots + \beta_{n-1} s + \beta_ m}{s^n + \alpha_1 s^{n-1} + \alpha_2 s^{n-2}+ \dots + \alpha_{n-1} s + \alpha_n } \]
		Recalling the results of equation \ref{eq:dyn:Gtostaterep}, a realization of this kind of system is characterized by the matrices
		\[ \A = \mat{-\alpha_1 & - \alpha_2 & \dots & - \alpha_{n-1} & - \alpha_n \\
		1 & 0 & \dots & 0 & 0 \\
		0 & 1 & \dots & 0 & 0 \\ 
		\vdots & \vdots & \ddots & \vdots & \vdots \\
		0 & 0 & \dots & 1 & 0 } \qquad \B = \mat{1 \\ 0 \\ \vdots \\ 0 \\ 0} \qquad \C \mat{\beta_1 & \beta_2 & \dots & \beta_{n-1} & \beta_n} \]
		Figure \ref{fig:dyn:controllablecanonical} shows how a system described in a controllable canonical representation (as in this case) can be graphed.
		
		\begin{figure}[bht]
			\centering \resizebox{0.9\linewidth}{!}{\tikzfigure{controlform}}
				\caption{graph representation of an LTI SISO system expressed in the controllable canonical form; the $1/s$ blocks in practise are representing integrators of the input.} \label{fig:dyn:controllablecanonical}
		\end{figure}
		
		In general this is not the lonely allowable representation of the transfer function $\Gs$, but there exists infinitely many ones. Considering $T$ as a non-singular matrix, we can define a new vector of the states $\tilde x = T x$ (hence $x = T^{-1} \tilde x$) that allows us to determine a new state space representation of the system, in fact we can build
		\begin{equation*}
		\begin{cases}
			\delta \tilde x = T \, \delta x  = T \A x + T \B u = T A T^{-1} \tilde x + T\B u = \tilde \A \tilde x + \tilde \B u \\
			\tilde y = \C x + \D u = \C T^{-1}\tilde x + \D u = \tilde \C \tilde x + \D u
		\end{cases}
		\end{equation*}
		We can so state that the systems
		\begin{equation} \label{eq:dyn:algebraicequivalence}
			\mat{\begin{array}{c|c}
					\tilde \A & \tilde \B \\ \hline \tilde \C & \D
			\end{array}} = \mat{\begin{array}{c|c}
				T\A T^{-1} & T \B \\ \hline \C T^{-1} & \D
			\end{array}} \qquad \textrm{and} \qquad \mat{\begin{array}{c|c}
				\A & \B \\ \hline \C & \D
			\end{array}}
		\end{equation}
		are \de{algebraically equivalent} and are indeed representations of the same transfer function $\Gs$. In this case the non-singular matrix $T$ is referred as \textbf{\textit{similarity/equivalence transformation}} and allows to describe the system into a different set of states. This operation can be useful because it might allow to express a system in a more suitable way for numerical computations.
		
		An example of algebraically equivalent representation is the \de{observable canonical form} described as
		\begin{equation}
		\mat{\begin{array}{ c c c c | c}
			-\alpha_1 & 1 & 0 & 0 & \beta_1 \\
			\vdots & 0 & \ddots & 0 & \vdots  \\
			-\alpha_{n-1} & 0 & 0 & 1 & \beta_{n-1} \\
			-\alpha_n & 0 & \dots & 0 & \beta_n \\ \hline
			1 & 0 & \dots & 0 
		\end{array}}
		\end{equation}
		The graph representation of such system is shown in figure \ref{fig:dyn:observablecanonical}.
	
		\begin{figure}[bht]
			\centering \resizebox{0.9\linewidth}{!}{\tikzfigure{observableform}}
			\caption{graph representation of an LTI SISO system expressed in the controllable canonical form; the $1/s$ blocks in practise are representing integrators of the input.} \label{fig:dyn:observablecanonical}
		\end{figure}
	
\section{Solution of linear time-varying systems}
	\subsection*{Continuous-time linear time-variant systems}
	Given a continuous-time linear time-variant  system (CT-LTV), it's solution must be a function of the time $t$ and must be differentiable (in order to be the solution of the differential equations). In particular the zero input response of the system expressed as
	\[ \dot x = \A(t) x \hspace{2cm} x(t_0 ) = x_0 \]
	can be computed using the \de{Peano-Baker series}, a matrix $\Phi(t,t_0) \in \mathds R^{n\times n}$ defined as
	\begin{equation} \label{eq:dyn:ct-statetransition}
	\begin{split}
		\Phi(t,t_0) = \I + & \int_{t_0}^t \A(s_1)\, ds_1 + \int_{t_0}^t \A(s_1) \int_{t_0}^{s_1} \A(s_2)\, ds_2\, ds_1 + \\ & \hspace{1cm}+ \int_{t_0}^t \A(s_1) \int_{t_0}^{s_1} \A(s_2) \int_{t_0}^{s_2} \A(s_3)\, ds_3\, ds_2\, ds_1 + \dots
	\end{split}
	\end{equation}
	The matrix $\Phi$ is mostly referred as the \de{state transition matrix} and is characterized by the following property:
	\begin{enumerate}[\itshape i)]
		\item it has that $\Phi(t,t) = \I$ for any $t\in \mathds R$; this result come just by the definition of the state transition matrix: considering in fact that for the integrals $\int_a^a f(x)\, dx = 0$, then the evaluation for $t=t_0$ cancels all the terms unless the identity matrix;
		
		\item a second important property is associated to the derivation of $\Phi$; knowing that $\frac d{dt} \int_a^t f(\tau) \, d\tau = f(\tau)$, them we can show that
		\begin{align*}
			\frac d{dt} \Phi(t,t_0) & = 0 + \A(t) + \A(t) \int_{t_0}^t \A(s_2) \, ds_2 + \A(t) \int_{t_0}^{t} \A(s_2) \int_{t_0}^{s_2} \A(s_3)\, ds_3\, ds_2 + \dots \\
			& = \A(t) \Phi(t,t_0)
		\end{align*}	
	\end{enumerate}
	
	With this property states we can compute and prove that the zero input response solution of a continuous-time linear system is determined as
	\begin{equation} \label{eq:dyn:zeroinputres}
		x(t) = \Phi(t,t_0) x_0 \hspace{2cm} \forall t 
	\end{equation}
	This solution is also proven to be unique (but we won't prove that). Equation \ref{eq:dyn:zeroinputres} can be proven considering that $x(t_0)$ gives indeed $\Phi(t_0,t_0) x_0 = \I x_0 = x_0$ using property \textit{i)}. To generally prove that this formulation is also solution to the original state equation $\dot x(t) = \A(t) x(t)$ we can just take the derivative of the suggested solution $\Phi(t,t_0) x_0$ respect to time and using property \textit{ii)} we indeed have
	\[ \dot x(t) = \frac{d}{dt}\Big( \Phi(t,t_0) x_0 \Big) = \A(t) \phi(t,t_0) x_0 = \A(t) x(t) \]
	
	In order now to further extend the solution in case of non-zero inputs it's mandatory to define other 2 properties for the state transition matrix:	
	\begin{itemize}
		\item[\textit{iii)}] each $i$-th column of the matrix $\Phi(t,t_0)$ is the unique solution at the problem
		\[ \dot x = \A(t) x \qquad \textrm{with } x(t_0) = \vet{0 \\ \vdots \\ 0 \\ 1 \\ 0 \\ \vdots \\ 0} = e_i \] 
		where the \textit{one} is placed in the $i$-th position of the vector $x_0$, meaning that's the $i$-th canonical base $e_i$. This property is very useful for computation because it allow to decompose the study of the solution $x(t)$ by independently solving all the states;		
		
		\item[\textit{iv)}] it holds the \textbf{semi-group property} stating that 
		\[ \Phi(t,s) \Phi(s,\tau)= \Phi(t,\tau) \hspace{2cm} \forall t,s,\tau \in \mathds R \]
		We can in fact consider that, given the initial state $x_0$ at time $t_0$, the state at a second timespan $t_1$ is regarded as $x(t_1) = \Phi(t_1,t_0) x_0$. Considering now a second time $t_2$ we have that
		\begin{align*}
			x(t_2) = \Phi(t_2,t_1) x(t_1) & = \Phi(t_2,t_1) \Phi(t_1,t_0) x_0  \\
			& \Phi(t_2,t_0) x_0
		\end{align*}
		Note that the semi-group property doesn't set any requirement in the \textit{position} of the times $t,s,\tau$ that can in fact be also \textit{back in time}.
		
		\item[\textit{v)}] due to the semi-group property yet stated we have that the state transition matrix $\Phi(t,t_0)$ is always invertible (hence is non-singular) and it's inverse is equal to $\Phi(t,s) = \Phi(s,t)$. We can in fact see that 
		\[ \Phi(t,s) \Phi(s,t) = \Phi(t,t) = \I \hspace{2cm} \forall t,s \in \mathds R \] 
	\end{itemize}
	
	\paragraph{Addition of the inputs} The more general unique solution of a CT-LTV system defined by the state space representation
	\[ \begin{cases}
		\dot x = \A(t) x + \B(t) u \hspace{1.4cm} x(t_0) = x_0\\ 
		y = \C(t) x + \D(t) u
	\end{cases} \]
	is determined by equation
	\begin{equation}
	\begin{split}
		x(t) & = \overbrace{\Phi(t,t_0) x_0}^\textrm{zero-input res.} + \overbrace{\int_{t_0}^t \Phi(t,\tau) \B(\tau) u(\tau) \, d\tau}^\textrm{zero-state response} \\
		y(t) & = \underbrace{\C(t) \Phi(t,t_0) x_0}_\textrm{homogeneous res.} + \underbrace{ \int_{t_0}^t \C(\tau) \Phi(t,\tau) \B(\tau) u(\tau)\, d\tau + \D(t) u(t) }_\textrm{forced response}
	\end{split}
	\end{equation}
	TO prove that the provided solution works (we won't prove that's unique, but it happens so) we can simply derive in time the state solution and watch that matches the state equation $\dot x = \A(t) x + \B(t) u$. In order to do so we have to consider the following property in the derivation of composed function in integration, in fact
	\[ \frac{d}{dt} \int_{t_0}^t f(t,\tau)\, d\tau = f(t,t) + \int_{t_0}^t \frac{d}{dt}f(t,\tau)\, d\tau \]
	With this stated we have
	\begin{align*}
		\dot x(t) & = \frac{d}{dt} \left( \Phi(t,t_0) x_0 + \int_{t_0}^t \Phi(t,\tau) \B(\tau) u(\tau) \, d\tau \right) \\
		& = \A(t) \Phi(t,t_0) x_0 + \Phi(t,t) \B(t)u(t) + \int_{t_0}^t \frac{d\Phi}{dt} \B(\tau) u(\tau)\, d\tau \\
		& = \A(t)\Phi(t,t_0) x_0 \B(t) u(t) +  \int_{t_0}^t \A(t) \Phi(t,\tau) \B(\tau) u(\tau)\, d\tau \\
		& = \A(t) \underbrace{\left( \Phi(t,t_0) + \int_{t_0}^t \Phi(t,\tau) + \B(\tau) u(\tau)\, d\tau \right)}_{=x(t)} + \B(t)u(t)
	\end{align*}
	
	\subsection*{Discrete-time linear time-variant systems}
	Given a discrete-time linear time-variant DT-LTV system, the unique solution of the homogeneous term $x^+ = \A(t) x$ subject to an initial state $x(t_0) = x_0$ is determined by the expression
	\begin{equation}
		x(t) = \Phi(t,t_0) x_0 \hspace{2cm} x_0 \in \mathds R^n, \forall t \geq t_0
	\end{equation}
	\begin{note}
		In this case the solution is valid for time $t$ grater or equal to $t_0$; in the continuous time case, equation \ref{eq:dyn:zeroinputres} is valid also for $t < t_0$ due to the semi-group property; for discrete-time system the matrix $\Phi(t,t_0)$ can sometimes be singular, meaning that it cannot be inverted as it will be shown.
	\end{note} \noindent
	In this case $\Phi(t,t_0)$ is the \de{discrete-time state transition matrix} defined as
	\begin{equation} \label{eq:dyn:peanodiscrete}
		\Phi(t,t_0) = \begin{cases}
			\I & t = t_0 \\
			\A(t-1) \A(t-2) \dots \A(t_0+1) \A(t_0) \hspace{1cm} & t > t_0
		\end{cases}
	\end{equation}
	Such definition can be inductively retrieved: considering in fact that the state at the step $t_0+1$ can be computed as $x(t_0 + 1) + \A(t_0) x_0$ but also $x(t_0 + 2) = \A(t_0+1) x(t_1) = \A(t_0+2) \A(t_0+1) x_0$. Inductively it's proven how the discrete-time state transition matrix was defined. This formulation should be extended also to the \textit{backward times}, for $t < t_0$, in fact considering
	\[ x(\underbrace{t+1}_{=t_0}) = \A(t) x(t) \hspace{1.4cm} \Rightarrow \hspace{1.4cm} x(t) = \A^{-1}(t_0 - 1) x(t_0) \]
	This operation is feasible if and only if the matrix $\A(t_0)$ is non-singular that hence can be inverted, but this condition isn't stated a priori.
	
	As for the continuous time case, the general solution of the discrete-time linear time-variant systems of the form
	\[ \begin{cases}
		x^+ = \A(t) x + \B(t) u \hspace{1.4cm} x(t_0) = x_0\\ 
		y = \C(t) x + \D(t) u
	\end{cases} \]
	is determined by the functions
	\begin{equation}
	\begin{split}
		x(t) & = \Phi(t,t_0) x_0 + \sum_{\tau=0}^{t-1} \Phi(t,\tau+1) \B(\tau) u(\tau) \\
		y(t) & =  \C(t) \Phi(t,t_0) x_0 + \sum_{\tau=0}^{t-1} \C(\tau) \Phi(t,\tau+1) \B(\tau) u(\tau)  + \D(t)u(t)
	\end{split}
	\end{equation}

\section{Solutions to linear time-invariant systems}
	\subsubsection*{Discrete-time case}
	Starting with the case of discrete-time case, if the system is time invariant than it means that the matrices $\A,\B,\C,\D$ does not depends on time. Considering the definition of Peano-Bakers series for discrete-time systems (equation \ref{eq:dyn:peanodiscrete}), knowing that $\A(t)$ is constant (and is so regarded as $\A$), then it means that the state transition matrix can be regarded as
	\begin{equation}
		\Phi(t,t_0) = \A^{t-t_0} \hspace{2cm} \forall t\geq t_0
	\end{equation}
	where conventionally $\A^0 = \I$. With that we can rewrite the solutions of the DT-LTI system as
	\begin{equation}
	\begin{split}
		x(t) & = \A^{t-t_0} x_0 +  \sum_{\tau=0}^{t-1} \A^{t-\tau-1} \B u(\tau) \\
		y(t) & = \C \A^{t-t_0} + \sum_{\tau=0}^{t-1} \C \A^{t-\tau -1} \B u(\tau) + \D u(t)
	\end{split}
	\end{equation}
	We refer to $\A^t$ (with $t\in \mathds Z$) as the \de{matrix power} and it will be the goal in the definition of the solution is to identify a \textit{clever} way to compute it's value (because numerically it's not feasible to perform matrix multiplication at each iteration); such problem will be fixed later with the definition of the Jordan normal form for matrices.
	
	\subsubsection{Continuous-time case}
	Considering now the continuos-time case for LTI systems, the definition of the Peano-Baker series (equation \ref{eq:dyn:ct-statetransition}, page \pageref{eq:dyn:ct-statetransition}) can be simplified: the matrices $\A$ can in fact be taken outside the integral terms resulting in
	\begin{align*}
		\Phi(t,t_0) & = \I + \A(t-t_0) + \A^2 \frac{(t-t_0)^2}{2} + \A^3 \frac{(t-t_0)^3}{2\cdot 3} + \dots \\ & = \sum_{k=0}^{\infty} \A^k \frac{(t-t_0)^k}{k!}
	\end{align*}
	This definition resembles the exponential; considering in fact the Taylor series expansion of the exponential is $e^x = \sum_{k=0}^\infty \frac 1{k!}x^k$, we can determine the state transition matrix as the \de{exponential matrix}
	\begin{equation}
		\Phi(t,t_0) = e^{\A(t-t_0)}
	\end{equation}
	hence the unique solution of a CT-LTI system is provided by the expression
	\begin{equation}
	\begin{split}
		x(t) & = e^{\A(t-t_0)} x_0 + \int_{t_0}^t e^{\A(t-\tau)} \B u(\tau) \, d\tau \\
		y(t) & = \C e^{\A(t-t_0)} x_0 + \int_{t_0}^t \C e^{\A(t-t_0)} \B u(\tau)\, d\tau + \D u(t)
	\end{split}
	\end{equation}
	
	Relevant properties of the exponential matrix are
	\begin{enumerate}[\itshape i)]
		\item that each $i$-th column of the matrix $e^{\A t}$ is the unique solution of the problem
		\[ \dot x(t) = \A x(t) \hspace{2cm} \textrm{with } x(0)= e_i \]
		\item it still holds the semi-group property stating that $\Phi(t,\tau) \Phi(\tau,s) = \Phi(t,s)$ ($\forall t,s,\tau$); by an application point of view it means that for the exponential matrix
		\[ e^{\A t} e^{\A s} = e^{\A(t+s)} \]
		\item it's always invertible, in fact from the semi-group property we have that $e^{\A t} e^{-\A t} = e^{\A 0} = \I$ and so it means that $e^{\A t}$ is always non-singular (hence invertible) and its inverse is defined as
		\[ \big(e^{\A t} \big)^{-1} = e^{-\A t} \]
		\item for any matrix $\A \in \mathds R^{n\times n}$ it holds the following \textit{"commutative"} property:
		\[ \A e^{\A t} = e^{\A t} \A \hspace{2cm} \forall t \]
		
		\item there exists $n$ functions $\alpha_i(t)$ such that
		\[ e^{\A t} = \sum_{k=0}^{n-1} \alpha_k (t) \A^k \]
		
	\end{enumerate}
	
	\paragraph{Cayley-Hamilton theorem} The \textbf{Cayley-Hamilton theorem} states that { \itshape for each matrix $\A \in \mathds R^{n\times n}$ with characteristic polynomial
	\[ p_\A(s) = \det (s\I -\A) = s^n + a_1 s^{n-1} + \dots + a_{n-1} s + a_n \] then  $\A$ is a matrix solution of this polynomial, meaning that
	\[ \A^n + a_1 \A^{n-1} + \dots a_{n-1} \A + a_n \I = 0 \] }
	In this case $0$ is a $n\times n$ matrix of zeros. By reversing this quantity we can indeed see that
	\[ \A^n = - \big( a_1 \A^{n-1} + \dots a_{n-1} \A + a_n \I \big) \]
	and so by multiplying both sides of the equation by $\A$ allows to compute the exponential $\A^{n+1}$ as
	\begin{align*}
		\A^{n+1} & = -\A^n a_1 - a_2 \A^{n-1} - \dots - a_{n-1}\A^2 - a_n \A  \\
		& = a_1\big(a_1 \A^{n-1} + \dots + za_{n-1} \A + a_n \I \big) - a_2 \A^{n-1} - \dots - a_{n-1}\A^2 - a_n \A \\
		& = (a_1^2 - a_2) \A^{n-1} + (a_1 a_2 - a_3) \A^{n-2} + \dots + (a_1 a_{n-1} - a_n) \A + a_1 a_n \I - \dots -a_{n-1} \A^2
	\end{align*}
	In general this proposition can be extended and any exponential of a matrix $\A \in \mathds R^{n\times n}$ can be regarded as a linear combination of the first $n$ exponentials of matrix $\A$:
	\begin{equation} \label{eq:lin:cayley}
		\A^h = \sum_{k=0}^{n-1} c_{h,k} \A^k
	\end{equation}

	\subsection{Jordan normal form}
		As was show in equation \ref{eq:dyn:algebraicequivalence} (page \pageref{eq:dyn:algebraicequivalence}), using a transformation matrix $T$ a linear system characterized by a state space representation
		\[ \begin{cases}
			\delta x = \A x + \B u \\ y = \C x + \D u
		\end{cases} \]
		can be transformed in a algebraically equivalent representation that could have resulted in a easier computation model. From an ideal point of view the \textit{best} solution we can get after transformation is a \textbf{diagonal state matrix} in the form
		\[ T \A T^{-1} = \mat{ \lambda_1 & 0 & \dots & 0 \\
		0 & \lambda_2 & \ddots  & \vdots \\ 
		\vdots & \ddots & \ddots & 0 \\
		0 & \dots & 0 & \lambda_n } \]
		where $\lambda_i$ are the eigenvalues of the \textit{original} matrix $\A$ (values that are always shared in each algebraic equivalent representation of the same system).
		
		A problem of this kind of formulation is that in practise not all matrices $\A$ are diagonalizable, and in particular this happens when the geometric multiplicity doesn't match the algebraic multiplicity of the related eigenvalues.
		
		A solution of this problem is provided by Jordan with a theorem stating that { \itshape for each matrix $\A \in \mathds C^{n\times n}$ with eigenvalues $\lambda_1, \dots, \lambda_m \in \mathds C$ there exists an non-singular (hence invertible) matrix $T \in \mathds C^{n\times n}$ such that
		\begin{equation}
			\J = T \A T^{-1} = \mat{ J_1 & 0 & \dots & 0 \\
				0 & J_2 & \ddots  & \vdots \\ 
				\vdots & \ddots & \ddots & 0 \\
				0 & \dots & 0 & J_l } \quad \in \mathds C^{n\times n} 
		\end{equation} where $J_i$ are matrices in the form
		\begin{equation}
			J_i = \mat{\lambda_i & 1 & 0 & \dots & 0 \\
			0 & \lambda_i & 1 & \dots & 0 \\
			0 & 0 & \lambda_i & \dots & 0 \\
			\vdots & \vdots & \ddots & \vdots \\
			0 & 0 & 0 & \dots & \lambda_i } \quad \in \mathds C^{n_i\times n_i}
		\end{equation} }
	
		The matrix $\J$ is known as the \de{Jordan normal form} of $\A$ and is proven to be unique (upon rearrangement of the sub-matrices $J_i$). Note that in general sub-matrices $J_i,J_j$ (with $i\neq j$) might be constructed with the same eigenvalue $\lambda_k$ but what might change are their dimension ($n_i\neq n_j$). \\		
		The number of \textit{blocks} in the Jordan normal form are $l \leq n$ and are associated to each linearly independent eigenvectors of $\A$ (is proven that at least one eigenvector exists for each matrix $\A$ and there can be maximum $n$ linearly independent ones).
		
		A matrix is so called \textbf{semi-simple} (or \textbf{diagonalizable}) if its Jordan normal form is diagonal. In particular given a matrix $\A \in \mathds C^{n\times n}$ the 3 following statements are equivalents:
		\begin{itemize}
			\item $\A$ is semi-simple;
			\item $\A$ has $n$ linearly independent eigenvectors;
			\item $p(\A) \neq 0$ for all non-zero polynomials $p(s)$ having a degree less then $n$.
		\end{itemize}
	
	\subsubsection{Matrix power}
		The power matrix value can be \textit{easily} determined considering the Jordan normal form of the matrix on which we want to compute the power $\A^t$; in particular we can see that
		\[ \A^ t = \underbrace{ \overbrace{T^{-1} \J T}^{=\A}  T^{-1} \J T \dots T^{-1} \J T }_{t \textrm{ times } }  \]
		Observing that \textit{in the middle} the terms $T T^{-1}$ cancels out (they evaluate to the identity matrix $\I$) what we obtain is that
		\begin{equation} \label{eq:lin:powermatrix}
			\A^t = T^{-1} \J^t T
		\end{equation}
		where the power of a matrix in Jordan forms evaluates to
		\begin{equation} \label{eq:lin:Ji}
			\J^t = \mat{J_1^t \\ & J_2^t \\ && \ddots \\ &&& J_l^t}
		\end{equation} 
		where
		\begin{equation} J_i^t = \mat{ 
			\lambda_i^t & t \lambda_i^{t-1} & \frac{t! \lambda_i^{t-2 }}{(t-2)! 2!} & \frac{t! \lambda_i^{t-3 }}{(t-3)! 3!} & \dots & \frac{t! \lambda_i^{t-n_i+1}}{(t-n_i+1)!(n_i-1)!} \\
			0&\lambda_i^t & t \lambda_i^{t-1} &  \frac{t! \lambda_i^{t-2 }}{(t-2)! 2!} &  \dots & \frac{t! \lambda_i^{t-n_i+2}}{(t-n_i+2)!(n_i-2)!}\\
			\vdots & \ddots &\lambda_i^t &  t \lambda_i^{t-1} & \dots & \frac{t! \lambda_i^{t-n_i+3}}{(t-n_i+3)!(n_i-3)!} \\
			\vdots && \ddots & \ddots & \ddots & \vdots\\
			\vdots&&& \ddots &\lambda_i^t &  t \lambda_i^{t-1} &\\
			0& \dots& \dots & \dots& 0&\lambda_i^t &  \\			
		 } \end{equation}
		
		\paragraph{Region of convergence of the system} Considering that the homogeneous response of the discrete time system $x(t+1) = \A x(t)$ (with $x(0) = x_0$) is determines as
		\[ x(t)= \A^t x_0 \]
		In order to have a convergent solution we want to ensure that $\lim_{t\rightarrow\infty} \A^t$ tends to zero (or at least it doesn't diverge), in the sense that all it's entry are zero (or non-diverging). Considering that the matrix power $\A^t$ can be regarded as $T^{-1}\J T$, the limit is intrinsecally transmitted to the Jordan normal form matrix $\J$ for which we can state
		\[ \lim_{t\rightarrow\infty} \J^t = 0 \qquad \Leftrightarrow \qquad \lim_{t\rightarrow\infty} J_i^t = 0 \quad \forall i  \]
		Considering the formal definition of the sub-matrix power $J_i^t$ shown in equation \ref{eq:lin:Ji}, we observe that each entry of the matrix is made by the eigenvalue $\lambda_i$ to the power of $t$ and a polynomial of $t$. Considering that the exponential asymptotically grows/decreases faster then polynomial we observe that whenever $|\lambda_i| \leq 1$ then all entries converges to 0. We can so state that if all eigenvalues $\lambda_i$ of the state matrix $\A$ have a magnitude less then 1, then the system converges to a null solution.
		
		In contrary if we consider $|\lambda_i|>1$ the exponential $\lambda_i^t$ explodes to infinity for $t\rightarrow \infty$, hence the systems behaviour diverges. All that's left to analyse is the case where $\J^t$ (for $\rightarrow \infty$) doesn't diverge and this happens when $|\lambda_i| = 1$ (where $1^t \xrightarrow{t\rightarrow \infty} 1$), but in order to ensure that the matrix exponential doesn't diverge we have to impose that $n_i = 1$ (in this case we don't have entries with diverging polynomials). In general we can so state that
		\[ \lim_{t\rightarrow\infty} \A^t \neq \infty \quad \Leftrightarrow \quad \lim_{t\rightarrow\infty} J_i^t \neq \infty  \  \forall i \quad \Leftrightarrow \quad |\lambda_i| \leq 1 \textrm{ and if } |\lambda_i|=1  \Rightarrow n_i=1 \]
		
		\paragraph{Fibonacci sequence} Recalling the Fibonacci state representation reported in equation \ref{eq:dyn:temp4} (page \pageref{eq:dyn:temp4}), we have that the problem was described as
		\[ x^+ = \mat{1 & 1 \\ 1 & 0} x \hspace{2cm} y = \mat{1 & 0} x \hspace{2cm} x(0) = \vet{0 \\ 1} \]
		In this case the polynomial characteristic of the state matrix $\A \in \mathds R^{2\times 2}$ evaluates as $p_\A(s) = s^2-s-1$ whose roots (hence eigenvalues of $\A$) are $s_{1,2} = \frac{1\pm \sqrt 5}{2}$. Having two distinct eigenvalues this means that the Jordan normal form of $\A$ is the diagonal matrix
		\[ \J= \mat{\frac{1+\sqrt 5}{2} & 0 \\ 0 & \frac{1-\sqrt 5}{2}} \]
		In order now to determine the transformation matrix $T \in \mathds R^{2\times 2}$ we need to solve the linear system $T\A = \J T$, hence
		\[ \mat{t_1 & t_2 \\ t_3 & t_4} \mat{1 & 1 \\ 1 & 0} = \mat{\frac{1+\sqrt 5}{2} & 0 \\ 0 & \frac{1-\sqrt 5}{2}} \mat{t_1 & t_2 \\ t_3 & t_4} \qquad \Rightarrow \qquad \begin{cases}
			t_1 + t_2  & = \frac{1+\sqrt 5}{2} t_1 \\
			t_1  & = \frac{1+\sqrt 5}{2} t_2 \\
			t_3 + t_4 & = \frac{1-\sqrt 5}{2} t_3 \\
			t_3 & =  \frac{1-\sqrt 5}{2 } t_4
		\end{cases}   \]
		hence
		\[ t_1 = \frac{1+\sqrt 5}{2}t_2 \hspace{2cm} t_3 = \frac{1-\sqrt 5}{2} t_4 \hspace{2cm} \forall t_2,t_4 \]
		In order not to have a singular matrix we have to chose value $t_2,t_4 \neq 0$; picking for simplicity $t_2 = t_4 = 2$ the transformation matrix we obtain is
		\[ T = \mat{ 1 + \sqrt 5 & 2 \\ 1- \sqrt 5 & 2 } \hspace{1.2cm} \Rightarrow \hspace{1.2cm} T^{-1} = \frac{1}{4\sqrt 5} \mat{ 2 & - 2 \\ - (1-\sqrt 5) & 1 + \sqrt 5 } \]
		Using equation \ref{eq:lin:powermatrix} we can compute the matrix power $\A^t$ as $T^{-1} \J T$; to ease the computation we can see that $1+\sqrt 5 = 2 \lambda_1$ and $1-\sqrt 5 = 2 \lambda_2$ we can write the equation as
		\begin{align*}
			\A^t & = \frac{1}{4\sqrt 5} \mat{ 2 & - 2 \\ -2\lambda_2 & 2\lambda_1} \mat{ \lambda_1^t & 0 \\ 0 & \lambda_2^t}  \mat{ 2\lambda_1 & 2 \\ 2\lambda_2 & 2 } = \frac 1{\sqrt 5} \mat{ 1 & - 1 \\ -\lambda_2 & \lambda_1} \mat{ \lambda_1^t & 0 \\ 0 & \lambda_2^t}  \mat{ \lambda_1 & 1 \\ \lambda_2 & 1 } \\
			& = \frac 1{\sqrt 5} \mat{ 1 & - 1 \\ -\lambda_2 & \lambda_1} \mat{ \lambda_1^{t+1} & \lambda_1^t \\\lambda_2^{t+1} & \lambda_2^t } = \frac 1 {\sqrt 5} \mat{\lambda_1^{t+1} - \lambda_2^{t+1} & \lambda_1^t - \lambda_2^t \\ - \lambda_1^{t+1} \lambda_2 + \lambda_1 \lambda_2^{1+t} & - \lambda_1^t \lambda_2 + \lambda_1 \lambda_2^t}
		\end{align*}
		Computing the homogeneous solution of the system as $y(t) = \C\A^t x_0$, given the particular formulation of the matrices $\C$ and $x_0$ means that the output $y(t)$ is determined by the second entry in the first column, hence
		\begin{align*}
			y(t) & =  \mat{1 & 0} \frac 1 {\sqrt 5} \mat{\lambda_1^{t+1} - \lambda_2^{t+1} & \lambda_1^t - \lambda_2^t \\ - \lambda_1^{t+1} \lambda_2 + \lambda_1 \lambda_2^{1+t} & - \lambda_1^t \lambda_2 + \lambda_1 \lambda_2^t} \vet{0 \\ 1}  = \frac 1{\sqrt 5}\left(\lambda_1^t - \lambda_2^t\right) \\ & = \frac 1{\sqrt 5} \left[ \left( \frac{1 + \sqrt 5}{2} \right)^t - \left( \frac{1 - \sqrt 5}{2} \right)^t \right] 
		\end{align*}
			
	
	\subsubsection{Matrix exponential}
		If we want to compute the exponential $e^{\A t}$ of the matrix $\A$ with Jordan normal form $\J$ we can use the fact that the exponential of such Jordan form is
		\begin{equation} \label{eq:lin:jordanexponential}
			e^{\J t} = \mat{ e^{J_1 t} && 0\\ & \ddots \\ 0 & &  e^{J_lt} }
		\end{equation}
		where $e^{J_it}\in \mathds R^{n_i\times n_i}$ is defined as 
		\begin{equation}
			e^{J_i t} = e^{\lambda_i t} \mat{ 1 & t & \dots & \frac{t^{n_i-1}}{(n_i-1)!} \\
			& \ddots & \ddots & \vdots \\
			&& \ddots & t	\\ &&& 1	}
		\end{equation}
		
		Considering now the Cayley-Hamilton theorem (equation \ref{eq:lin:cayley}, page \pageref{eq:lin:cayley}) we have that the matrix exponential can be regarded as a linear combination of the first $n$ matrix powers as
		\[ e^{\A t} = \sum_{k=0}^{n-1} \alpha_k(t) \A^k \]
		Knowing so that $\A= T^{-1} \J T$ we can rewrite
		\begin{equation} \label{eq:lin:exponentialmatrix}
		\begin{split}
			e^{\A t} & = \sum_{k=0}^{n-1} \alpha_k(t) T^{-1} \J^k T = T^{-1} \left( \sum_{k=0}^{n-1} \J^k \right) T \\
			& = T^{-1} e^{\J t} T
		\end{split}
		\end{equation}
		on which we can compute the formal definition of the Jordan form exponential as shown in equation \ref{eq:lin:jordanexponential}.
		
		\paragraph{Region of convergence} Considering so that the exponential $e^{\J t}$ is the unique solution of the problem $\dot x(t) = \J x(t)$ with $x(0) = \I$, we can so analyse the asymptotic behaviour of the system (as was done for the discrete-time ones). In general the eigenvalues an be complex ($\lambda_i = \alpha_i + j \beta_i \in \mathds C$) and we can show that the sub-matrix exponentials $e^{J_it}$ converges for $t\rightarrow \infty$ if and only if the real part $\alpha_i$ of the eigenvalues is strictly negative; at contrary if $\alpha_i > 0$ what we obtain is that the term $e^{\lambda_i t}$ premultiplying the whole matrix diverges and hence $\J$.
		
		In the case where $\alpha_i = 0$ we have the case of discrete-time systems: if the dimension $n_i$ is unitary then $e^{J_i}$ is a constant (non-infinite) 1-dimensional matrix, while if $n_i>1$ the upper triangular terms polynomially explodes to $\infty$:
		\[ \lim_{t\rightarrow\infty} e^{\A t} \neq \infty \quad \Leftrightarrow \quad \lim_{t\rightarrow\infty} e^{J_i t} \neq \infty  \  \forall i \quad \Leftrightarrow \quad \Re{\lambda_i} \leq 0 \textrm{ and if } \Re{\lambda_i} = 0 \Rightarrow n_i=1 \]
		
		\paragraph{Example} Given the continuous time LTI system characterized by the state matrix
		\[ \A= \mat{ 5 & 3 \\ -6 & -4} \]
		in order to compute it's exponential $e^{\A t}$ we have to firstly determine it's Jordan normal form $\J$. This can be achieved by firstly determining the eigenvalues of the matrix; knowing that the characteristic polynomial is $p(\lambda) = (5 - \lambda)(-4-\lambda) + 6\cdot 3 = \lambda^2 - \lambda - 2$ we can compute it's roots and hence determine $\J$:
		\[ \lambda_{1,2} = \frac{1\pm \sqrt 9}{2} \hspace{1.2cm} \Rightarrow \hspace{1.2cm} \J= \mat{ 2 & 0 \\ 0 & -1} \]
		With this result we have to compute the transformation matrix $T$ by imposing that $\J T = T A$ and solving the related linear system in the unknowns entries of $T$:
		\[ \mat{ 2 & 0 \\ 0 & -1} \mat{t_1 & t_2 \\ t_3 & t_4} = \mat{t_1 & t_2 \\ t_3 & t_4} \mat{ 5 & 3 \\ -6 & -4} \]
		\[ \begin{cases}
			2 t_1 & = 5 t_1 - 6t_2 \\
			2 t_2 & = 3 t_1 - 4 t_2 \\
			-t_3 & = 5 t_3 - 6 t_4 \\
			-t_4 & = 3t_3 - 4 t_4
		\end{cases} \qquad \Rightarrow \quad \begin{cases}
			2t_2 = t_1 \\
			t_3 = t_4
		\end{cases} \]
		Choosing as free parameters $t_1 = t_3 = 1$ for simplicity we obtain the transformation matrix
		\[ T = \mat{2 & 1 \\ 1 & 1} \hspace{1.2cm} \Rightarrow \hspace{1.2cm} T^{-1} = \mat{  1 & - 1 \\ - 1 & 2} \]
		We can finally compute the exponential matrix $e^{\A t}$ as $T^{-1} e^{\J t} T$ (equation \ref{eq:lin:exponentialmatrix}):
		\begin{align*}
			e^{\A t} & = \mat{  1 & - 1 \\ - 1 & 2} \mat{e^{2t} & 0 \\ 0 & e^{-t} } \mat{2 & 1 \\ 1 & 1} = \mat{  1 & - 1 \\ - 1 & 2}  \mat{ 2 e^{2t} & e^{2t} \\ e^{-t} & e^{-t} } \\
			& = \mat{2 e^{2t} -e^{-t} & e^{2t} - e^{-t} \\ -2 e^{2t} + 2 e^{-t} & -e^{2t} + 2 e^{-to} }
		\end{align*}
		
\section{Internal / Lyapunov stability}
	\paragraph{Norms for vector} In order to start the discussion regarding the stability of linear systems, we have to recall the norms for vector and matrices. Given a vector $x\in \mathds R^n$, we define it's \de{norm} $x\mapsto |x|$ an operation that satisfy the following properties:
	\begin{align*}
		i) \qquad & |x|\geq 0 \ \forall x\in  \mathds R^n \quad \textrm{and} \quad |x|=0 \ \Leftrightarrow \ x = 0 \\
		ii) \qquad & |x+y| \leq |x|+|y| \qquad \forall x,y\in \mathds R^n \\
		iii) \qquad & |ax| =|a|\, |x| \qquad \forall x\in \mathds R^n,a\in \mathds R 
	\end{align*}
	Matching this definition we can define different kinds of norms:
	\begin{itemize}
		\item the \textit{one norm} $|\cdot|_1$ defined as
		\begin{equation}
			|x|_1 = \sum_{i=1}^{n} |x_i|
		\end{equation}
		\item the \textit{two norm} $|\cdot|_2$ (euclidean norm) defined as
		\begin{equation} \label{eq:lin:euclideannorm}
			|x|_2 = \sqrt{ \sum_{i=1}^n |x_i|^2 }
		\end{equation}
		\item the \textit{infinity norm} $|\cdot|_\infty$ defined as
		\begin{equation}
			|x|_\infty = \max_{i=1,\dots, n} |x_i|
		\end{equation}
		\item this previous concepts can be generalized by the concept of \textit{p norm} $|\cdot|_p$ defined as
		\begin{equation}
			|x|_p = \sqrt[p]{ \sum_{i=1}^n |x_i|^p}
		\end{equation}
	\end{itemize}	
	All this norms are \de{equivalents}: for any norm $p_1,p_2 \in [1,\infty)$ there exists two constants $c_1,c_2\in \mathds R$ such that
	\[ c_1 |x|_{p_1} \leq |x|_{p_2} \leq c_2 |x|_{p_1} \hspace{1cm} \forall x \in \mathds R^n \]
 
	\paragraph{Norms for matrix} The definition of norm can be extended also in case of matrices; in this case the 3 property of the norm must be respected but given a matrix $A \in \mathds R^{m\times n}$ with $m$ rows and $n$ columns we can compute its norm $\| A\|$ in different ways:
	\begin{itemize}
		\item \textit{one norm} $\|\cdot\|_1$ defined as
		\begin{equation}
			\|A\|_1 = \max_{j=1,\dots, n} \sum_{i=1}^m |a_{ij}|
		\end{equation}
		If we consider the matrix as the \textit{concatenation} of $n$ column vectors $v_i \in \mathds R^m$, then this norm can be regarded as the maximum one norm $|v_i|_1$ among such vectors.
		
		\item \textit{infinity norm} $\|\cdot\|_\infty$ defined as
		\begin{equation}
			\|A\|_\infty = \max_{i = 1,\dots,m} \sum_{j=1}^n |a_{ij}|
		\end{equation}
		\item \textit{two norm} $\|\cdot\|_2$ defined as
		\begin{equation}
			\|A\|_2 = \sigma_{\max}\{ A \} = \sqrt{\lambda_{max} \{ A^t A \}  }
		\end{equation}
		We denote with $\sigma\{A\}$ the \textbf{singular values} of the matrix $A$ and are computed as the square root of the eigenvalues of the square matrix $A^t A \in \mathds R^{n\times n}$; in particular the two norm corresponds to the maximum singular value of the matrix $A$. Considering that a vector can be regarded as a matrix $\mathds R^{m\times 1}$ this definitions coincide with the euclidean norm for vectors (equation \ref{eq:lin:euclideannorm}).
		
		\item \textit{Frobenius norm} $\|\cdot\|_F$ defined as
		\begin{equation}
			\|A\|_F = \sqrt{ \sum_{i=0}^m \sum_{j=0}^n a_{ij}^2 } = \sqrt{ \sum_{i=0}^n \sigma_i^2\{A\} }
		\end{equation} 
		Also in this case if we consider a vector as a matrix $\mathds R^{m\times 1}$ the computation of the Frobenius norm coincides with the euclidean/two norm (equation \ref{eq:lin:euclideannorm}) but in general for arbitrarily large matrices $\|A\|_2 \leq \|A\|_F$.
	\end{itemize}
	
	\begin{example}{: two vs Frobenius norm}
		Considering the matrix
		\[ A = \mat{ 2 & 0 \\ 0 & 1} \] 
		in order to determine both two and Frobenius norm we have to compute the singular values of the matrix, and to do so we have to compute the matrix $A^tA$ that results in
		\[ A^tA = \mat{4 & 0 \\ 0 & 1} \]
		Being this matrix diagonal, the two eigenvalues are $\lambda_1 = 4$ and $\lambda_2 = 1$ that, squared, results in the singular values $\sigma_1 = 2$, $\sigma_2 = 1$. We can so compute the two norms as
		\[ \| A\|_2 = \sigma_{max} = \sigma_1 = 2 \qquad \neq \qquad \|A\|_F = \sqrt{\sigma_1^2 + \sigma_2^2} = \sqrt{4 + 1} = \sqrt 5 \]
		We just showed with an example that in general Frobenius and two norms doesn't usually coincides for matrices of arbitrary dimensions.
	\end{example}
		
	Also matrix norms are \textbf{equivalent}, meaning that for any norm $p_1,p_2 \in \{ 1,2,F,\infty\}$ there exists two constants $c_1,c_2\in \mathds R$ such that
	\[ c_1 \|A\|_{p_1} \leq \|A\|_{p_2} \leq c_2 \|A\|_{p_1} \qquad \forall A \in \mathds V \]
	Matrix norms are also \textbf{sub-multiplicative}, meaning that
	\[ \|A B\|_p \leq \|A\|_p \|B\|_p \qquad \forall A,B\in \mathds V,\ \forall p \in \{1,2,F,\infty\} \]
	
	\paragraph{Induced norm} Given a matrix $A \in \mathds R^{m\times n}$ and a vector $x \in \mathds R^{n}$, the product $A x$ results in a $\mathds R^n$ vector on which we can compute the norm. Considering so the sub-multiplicative property we have that
	\[ | A x|_p \leq \|A\|_p |x|_p \qquad \Rightarrow \qquad \|A\|_p \geq \frac{|A x|_p}{|x|_p} \]
	For $p\in \{ 1,2,\infty\}$ we have that the associated norms are \textbf{subordinate}, meaning that we can compute the norm $\|A\|_p$ as
	\begin{equation}
		\|A\|_p = \max_{x\neq0} \frac{|Ax|_p}{|x|_p}
	\end{equation}
	Note that the Frobenius norm isn't subordinate, hence this relation can't be applied.
	
	\subsection*{Lyapunov stability for linear system}
	Given a linear system (both discrete/continuous-time, time-varying/invariant) described as
	\[ \begin{cases}
		\delta x = \A x + \B u \qquad x(t_0) = x_0 \\ y= \C x + \D u
	\end{cases} \]
	we say that
	\begin{itemize}
		\item is \de{Lyapunov} (or \textit{marginally}) \de{stable} if for each initial state $x_0$ there exists a constant $M\in \mathds R$ (that depends of course on $x_0$) such that the zero input response satisfies
		\[ |x(t)| \leq M \qquad \forall t\geq t_0,t_0 \geq 0 \]
		\item is (Lyapunov) \de{asymptotically stable} if it's Lyapunov stable and all solutions satisfy
		\[ \lim_{t\rightarrow \infty} |x(t)|=0 \qquad \forall x_0 \]
		\item is (Lyapunov) \de{exponentially stable} if it's asymptotically stable and exists constants $c>0, \lambda > 0, \mu \in [0,1)$ such that
		\begin{align*}
			& |x(t)| \leq ce^{-\lambda(t-t_0)} |x_0| \qquad \forall x_0,t_0 &&  \textrm{for continuous-time systems} \\
			& |x(t)| \leq c\mu^{t-t_0} |x_0| \qquad \forall x_0,t_0 &&  \textrm{for discrete-time systems} 
		\end{align*}
		\item a system is \de{unstable} if it's not Lyapunov stable.
	\end{itemize}
	We can see that this conditions are always more stringent, hence we can say that
	\[ \textrm{exponential stability} \quad \Rightarrow \quad \textrm{asymptotic stability} \quad \Rightarrow \quad \textrm{Lyapunov stability} \]
	From what concerns Lyapunov, stability is analysed regarding the homogeneous (zero input) response and, as was previously described, the solution can be described using the state transition matrix $\Phi$, hence we can consider:
	\[ |x(t)| = |\Phi(t,t_0)x_0| \leq \|\Phi(t,t_0)\|\, |x_0| \qquad \forall x_0 \]
	From what concerns the Lyapunov stability this means that the norm of the state transition matrix must be bounded, mathematically $\|\Phi(t,t_0)\| \leq M_\Phi$; the same idea holds for both the asymptotic ($\lim_{t\rightarrow \infty} \|\Phi(t,t_0) = 0$) and exponential stability, where $\|\Phi(t,t_0)\| \leq c e^{-\lambda(t-t_0)}$ (for continuous-time systems, but the same can be transliterated for discrete-time ones).
	
	We indeed see that \textbf{Lyapunov stability} only depends on the state transition matrix $\Phi(t,t_0)$, and not the initial condition $x_0$, hence is an \textbf{intrinsic property of the system} independently on the inputs and can be ensured by the analysis of the behaviour of the state transition matrix.
	
	\subsubsection{Linear time invariant systems}
		Considering continuous-time time-invariant linear systems, we proved that the state transition matrix $\Phi(t,t_0)$ can be computed as the matrix exponential $e^{\A(t-t_0)}$ and, as was described, we showed that such matrix converged if $\Re{\lambda_i} < 0 \forall i$. In particular for continuous-time LTI systems we have 
		\begin{align*}
			\textrm{asymptotic stability} \quad & \Leftrightarrow \quad \Re{ \lambda_i\{\A\} } < 0 \ \forall i \\
			\textrm{exponential stability} \quad & \Leftrightarrow \quad \Re{ \lambda_i\{\A\} } < 0 \ \forall i \\
			\textrm{Lyapunov stability} \quad & \Leftrightarrow \quad \Re{ \lambda_i\{\A\} } \leq 0 \ \forall i \textrm{ and if } \Re{\lambda_j} = 0 \ \Rightarrow \ n_j = 0
		\end{align*} 
		We see so that for linear time-invariant systems asymptotic and exponential stability are \textit{coincident} concepts, meaning that they both happen at the same time.\\
		Considering instead discrete-time LTI systems the state transition matrix $\Phi(t,t_0)$ is computed as the power $\A^{t-t_0}$ and, as was previously discussed, we can conclude that
		\begin{align*}
			\textrm{asymptotic stability} \quad & \Leftrightarrow \quad |\lambda_i\{\A\} < 1 \ \forall i \\
			\textrm{exponential stability} \quad & \Leftrightarrow \quad |\lambda_i\{\A\} < 1 \ \forall i \\
			\textrm{Lyapunov stability} \quad & \Leftrightarrow \quad |\lambda_i\{\A\} \leq 1 \ \forall i \textrm{ and if } |\lambda_j| = 1 \ \Rightarrow \ n_j = 0
		\end{align*} 
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
		