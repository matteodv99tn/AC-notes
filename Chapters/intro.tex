\chapter{Dynamical systems and linear systems theory}

	This course will treat the \de{automatic control} of \de{dynamical systems}, hence it's necessary to firstly understand what a \textit{dynamical system} is and how it's described. In general such systems can be regarded as \textit{black box} that for a given \textbf{input} $u(t)$ determines an \textbf{output} $y(t)$; systems can be \textbf{continuous} or \textbf{discrete-time} depending on the type of \textit{time axis scale}, but also \textbf{hybrid} systems exists (as it will be studied).
	
	\paragraph{Rabbits} In order to understand behaviours of dynamical system, let's consider the example of pairs of rabbit. If we describe with $y(t)$ the total number of rabbits couple that one person has (where $t$ is the time expressed in months), we can consider that initially a person start with no animals ($y(0) = 0$) and goes at $t=1$ at an animal shop to buy a couple of small rabbits (hence $y(1) = 1$). In the first month of life they grow up and from \textit{babies} they become \textit{adults}, but the person still has $y(2) = 1$ couple of rabbit. In the next month they procreate a new couple of rabbits and so we have $y(3) = 2$ rabbit pairs. Assuming that no rabbit dies, we can describe the evolution of the system as
	\begin{equation} \label{eq:dyn:rabbitdiscrete}
		y(n) = y(n-2) + y(n-1)
	\end{equation}
	where the term $y(n-2)$ relates to the elder population and $y(n-1)$ describes the newly borne couple from each pair of elder pair. Equation \ref{eq:dyn:rabbitdiscrete} is a \textbf{finite difference equation} that allows to recursively define the number of rabbit pairs depending on the time $t$ assuming to know the \textbf{initial conditions} $y(0)$ and $y(1)$.
	
	For such system we can define it's \de{states} $x_1(t) = y(t)$ and $x_2 = y(t-1)$ determining the vector $x(t)$:
	\begin{equation}
		x(t) = \begin{pmatrix}
			x_1(t) \\ x_2(t)
		\end{pmatrix}
	\end{equation}
	Given so the states $x(t)$, we can compute their evolution $x(t+1)$ as
	\[ x(t+1) = \vet{x_1(t+1) \\ x_2(t+1)} = \vet{y(t+1) \\ y(t)} = \vet{y(t-1) + y(t) \\ y(t)} = \vet{x_2(t) + x_1(t) \\ x_1(t)}\]
	Using linear algebra the evolution of the state can be regarded as a linear combination of the actual states, in fact
	\begin{equation} \label{eq:dyn:temp4}
		x(t+1) = \mat{1 & 1 \\ 1 & 0} \vet{x_1(t) \\ x_2(t)} = \A x(t)
	\end{equation}
	Having the eigenvalues of the matrix $\A$ equal to $\lambda_{1,2} = \frac{1 \pm \sqrt 5}{2}$ we will prove that the continuous explicit function that express the rabbit pairs $y$ as function of $t$ is
	\[ y(t) = \frac1{\sqrt{5}} \left[ \left(\frac{1 + \sqrt 5}{2}\right)^t - \left(\frac{1 - \sqrt 5}{2}\right)^t \right] \]
	
	\paragraph{Classification of system} This example allowed to intrude the first classification of system: they in fact can be \de{linear} if the states variation and output can be regarded as linear combination of the actual states and inputs, meaning that they can be expressed as
	\begin{equation} \label{eq:dyn:temp1}
	\begin{cases}
		x(t+1) = \A x(t) + \B u(t) \\ y(t) = \C x(t) + \D u(t)
	\end{cases}
	\end{equation}
	where $\A$ is the \textbf{dynamic matrix}, $\B$ the \textbf{input} m., $\C$ the \textbf{output} m. and $\D$ is the \textbf{feed-through} (instantaneous) m.; both $u,x$ and $y$ are in general vectors. To simplify the notation usually the dependency on the time $t$ is dropped and the increment for \textbf{discrete-time} systems (where $t\in \mathds Z$) is described as $x^+$ (values of the state at the next step), hence equation \ref{eq:dyn:temp1} is rewritten as
	\begin{equation} \label{eq:dyn:LTIdiscrete}
		\begin{cases}
			x^+ = \A x + \B u \\ y = \C x + \D u \\ x(t_0) = x_0
		\end{cases}
	\end{equation}
	For \textbf{continuous-time} systems (where $t\in \mathds R$) the concept of increment is represented by the derivative $\dot x = \frac{dx}{dt}$, hence
	\begin{equation} \label{eq:dyn:LTIcontinuous}
		\begin{cases}
			\dot x = \A x + \B u \\ y = \C x + \D u \\ x(t_0) = x_0
		\end{cases}
	\end{equation}
	Note that for all the system is very important to define the \textbf{initial condition} $x(t_0) = x_0$ of the states because the response of the output is strictly related to them.
	
	System that don't exhibits a linear behaviour are therefore called \de{non-linear} and the input/state/output relations are described by functions:
	\begin{equation}
	\begin{cases}
		\delta x = f(x,u) \\
		y = h(x,y) \\
		x(t_0) = x_0
	\end{cases}
	\end{equation}
	\begin{note}
		the term $\delta x$ is the generalization of $\dot x$ for continuous-time or $x^+$ for discrete-time systems in order to make the notation more generalized and will be used throughout the whole book.
	\end{note}
	
	Until now the notation were referred to \de{time-invariant} systems where the matrix $\A,\B\,\C,\D$ (for linear sys.) or the functions $f,h$ (for non-linear sys.) were not dependent on time, however \de{time-variant} systems also exists that exhibiting an explicit relation with time, hence can be regarded as
	\begin{equation}
		\begin{cases}
			\delta x = \A(t) x + \B(t) u \\ y = \C(t) x + \D(t) u \\ x(t_0) = x_0
		\end{cases}
		\qquad \textrm{or} \qquad 
		\begin{cases}
			\delta x = f(x,u,t) \\
			y = h(x,y,t) \\
			x(t_0) = x_0
		\end{cases}
	\end{equation}
	for both linear and non-linear case.\\
	By a mathematical point of view a system if it satisfy that
	\[ \begin{cases}
		x(t_0 + T) = x_0 \\ u(t-T) 
	\end{cases} \qquad \Rightarrow \quad y(t-T) \hspace{2cm} \forall t \geq t_0 + T \]
	
	\textbf{ESEMPI DEI DIVERSI TIPI DI SISTEMI}
	
	\begin{example}{: linear time-variant dynamical system}
		Let's consider the model of a 3 year course of an university with the goal of study the behaviour of students. Considering the graphs that follows, we denote as $u$ the number of students registering at the first year, while $\alpha_i \in [0,1]$ is the relative value of students that passes to the next year/graduates; $y$ finally represent the number of student graduated. In this case the states $x_i$ are representing the number of student in each year of  the course. This models assumes that no student drops the course (more complex models can indeed consider such phenomena).
		\begin{center}
			\tikzfigure{university}
		\end{center}
		
		We can see that this dynamical system is discrete-time (the system is considered to evolved each year at the start of the academic year); analysing the graph we observe that each next year the number of student in each \textit{class} ($x_i$) depends on the number of the one passed from the previous course ($\alpha_i x_{i-1}$) and the one that didn't passed ($(1-\alpha_i)x_i$), hence we have that
		\[\begin{cases}
			x_1^+ & = (1-\alpha_1)x_1 + u \\
			x_2^+ & = (1-\alpha_2)x_2 + \alpha_1 x_1 \\
			x_3^+ & = (1-\alpha_3)x_3 + \alpha_2 x_2 \\
			y & = \alpha_3 x_3
		\end{cases}\]	
		In this case if we assume that $\alpha_i$ are constant value, the model that we obtain doesn't suit the real behaviour of the course where problems might happen (for example a \textit{evil teacher} reduces the rate of passed students one year, reducing $\alpha_i$),	hence we have a system that's time-variant ($\alpha_i$ depends on time $t$) while the model is still linear, because it can be expressed as
		\begin{align*}
			x^+ = \vet{x_1^+ \\ x_2^+ \\ x_3^+} & = \mat{1-\alpha_1(t) & 0 & 0 \\ \alpha_1(t) & 1-\alpha_2(t) & 0 \\ 0 & \alpha_2(t) & 1-\alpha_3(t)} \vet{x_1 \\ x_2 \\ x_3} + \vet{1 \\0 \\0 } u = \A(t)x + \B(t)u \\
			y & = \mat{0 & 0 & \alpha_3(t)} \vet{x_1 \\ x_2 \\ x_3} + \mat{0} u = \C(t)x + \D(t) u
		\end{align*}
	\end{example}
	
\section{Linear time-invariant systems and transfer functions}
	As equations \ref{eq:dyn:LTIdiscrete} and \ref{eq:dyn:LTIcontinuous} stated, \de{linear time-invariant} \textbf{LTI} systems can be described in matrix form as
	\[ \begin{cases}
		\delta x = \A x + \B u \qquad x(0) = x_0\\ y = \C x + \D u 
	\end{cases} \]
	where the matrix $\A,\B,\C,\D$ are constant (hence independent from time due to the time-invariancy requirement). Having the system time invariant the initial condition can be expressed as $x(0)$ (because it doesn't depend on the choice of $t_0$, but on any general initial condition).
	
	Continuous-time LTI systems can be easily analysed in the domain of the complex variable $s$ using the \textbf{Laplace transform} $\mathscr L$, an operator defined
	\[ X(s) = \laplace{x(t)} = \int_0^\infty x(t) e^{-st}\, dt \hspace{2cm} s \in \mathds C \]
	An important property of such operator is that converts differential equations into algebraic ones in the variable $s$, in fact we can observe that
	\[ \dot x(t) \mapsto s X(s) - x(0) \]
	The Laplace transform has also the useful property of being linear, hence the input-output relation of LTI system can be transformed according to Laplace, hence
	\begin{equation}
		(eq.\ref{eq:dyn:LTIcontinuous}) \mapsto \begin{cases}
			sX(s) - x(0) = \A X(s) + \B U(s) \\ Y(s) = \C X(s) + \D U(s)
		\end{cases}
	\end{equation}
	The first equation can be solved in order to explicit the transform of the states $X(s) = \laplace{x(s)}$ as function of the transform of the input $U(s) = \laplace{u(t)}$ determining
	\begin{align*}
		sX(s) - \A X(s) & = \B U(s) + x(0) \\
		(s\I-\A) X(s) & = \B U(s) + x(0) \\
		X(s) & = (s\I - \A)^{-1} \B U(s) + (sI- \A)^{-1} x(0) 
	\end{align*}
	\begin{note}
		In the first step in order to collect $sX(s)$ and $\A X(s)$ has been necessary to multiply $s$ by the identity matrix $\I$: $s$ is in fact a scalar while $\A$ is a matrix and no operation is compatible between this elements (addition is defined only for matrix with same sizes).
	\end{note} \noindent
	With this result obtained we can compute the transform of the output $Y(s) = \laplace{y(t)}$ as function of the lonely $U(s)$ as 
	\begin{equation}
		Y(s) = \underbrace{\left(\C (s\I - \A)^{-1} \B + \D\right)}_{= \hat \G(s)} U(s) + \underbrace{\C(s\I-\A)^{-1} \D}_{\hat \psi(s)} x(0)
	\end{equation}
	In this equation $\hat \G(s)$ represent the \de{transfer function} of the system (however more generally it's a matrix). Observing that the output in the Laplace domain can be so simplified to the form $Y(s) = \hat G(s) U(s) + \hat \psi(s) x(0)$ and so by using property of the (inverse) Laplace transform we can rewrite the output as the convolution of the input sequence and the \de{impulse response} $G(t) = \mathscr L^{-1}\{\hat G(s)\}$ of the system:
	\begin{equation}
		y(t) = (G*x)(t) + \psi(t) x(0)
	\end{equation}
	where the convolution is defined as
	\[ (G*x)(t) = \int_0^t G(t)x(t-\tau)\, d\tau \]
	
	Considering the case of discrete-time LTI  systems instead of the Laplace we have to use the \textbf{Z transform} $\Z$ that maps discrete time sequences into a domain in the complex variable $z$; knowing that $X(z) = \ztransf{x(t)}$ (with $t\in \mathds Z$) the analogous property of differentiation in time is that $x(t+1) = x^+ \mapsto z X(z) - z x(0)$, hence
	\begin{equation}
		(eq.\ref{eq:dyn:LTIdiscrete} )\mapsto \begin{cases}
			z X(z) - z x(0) = \A X(z) + \B U(z) \\ Y(z) = \C X(z) + \D U(z)
		\end{cases}
	\end{equation}
	As in the continuous case it's possible to compute the transform of the output $Y(z) = \ztransf{y(t)}$ as function of the spectrum $U(z) = \ztransf{x(t)}$ as
	\begin{equation}
		Y(z) = \underbrace{\left(\C (z\I - \A)^{-1} \B + \D\right)}_{= \hat \G(z)} U(z) + \underbrace{\C(z\I-\A)^{-1} \D z}_{\hat \psi(z)} x(0)
	\end{equation}

	\paragraph{Theorem} A fundamental theorem for continuous-time linear time-invariant systems states that \textit{the \textbf{transfer function} $\hat \G(s)$ and so the \textbf{impulse response} $\G(t)$ of such systems is computed as}
	\begin{equation}
		\hat \G(s) = \left(\C (s\I - \A)^{-1} \B + \D\right) \hspace{2cm} \G(s) = \antilaplace{\left(\C (s\I - \A)^{-1} \B + \D\right)}
	\end{equation}
	
\section{Interconnected systems}
	The control of dynamical systems is based on \de{interconnections} between, and such connection can be summarized as: in parallel, in series, in feedback. In this section a proof of transfer function of interconnected system is given in the case of linear time-invariant system (where the concept of transfer function is \textit{well defined}). Such system represented as 
	\[ \begin{cases}
		\delta x = \A x + \B u \qquad x(0) = x_0\\ y = \C x + \D u 
	\end{cases} \]
	are so characterized by the 4 matrix $\A,\B,\C,\D$ by the \textit{big matrix notation}
	\[ \vet{\delta x \\ \hline y} = \begin{bmatrix}
		\begin{array}{c | c}
			\A & \B \\ \hline \C & \D
		\end{array} 
	\end{bmatrix} \vet{x \\ u }\]
	
	\subsection*{Parallel connection}
		\begin{figure}[b!]
			\centering
			\tikzfigure{parallel}
			\caption{black box representation of a parallel connection between two systems.} \label{fig:dyn:parallel}
		\end{figure}
		Considering the parallel connection of the systems $\Sigma_1,\Sigma_2$ (figure \ref{fig:dyn:parallel}), the overall transfer function of the system $\Sigma$ can be computed considering that the output $Y(\cdot)$ in the Laplace/Z domain is the sum of the contribution of the single stages that can be computed so independently
		\begin{equation}
		\begin{aligned}
			Y &  = Y_1 + Y_2 = \hat \G_1 U_1 + \hat \G_2 U_2 = \hat \G_1 U + \hat G_2 U \\
			& = \underbrace{\big(\hat \G_1  + \hat \G_2\big)}_{=\hat \G} U
		\end{aligned}
		\end{equation}
		
		While connection two or more systems we have that the overall states $x$ of the system $\Sigma$ is the union of the individual systems $x_i$, hence in this case
		\[ x = \vet{x_1 \\ x_2} \]
		More formally the we have that the variation of the states $\delta x$ of the system $\Sigma$ can be regarded as
		\[ \delta x = \A_1 x_1 + \B_1 u_1 + \A_2 u_2 + \B_2 u_2 = \mat{\A_1 & 0 \\ 0 & \A_2} \vet{x_1 \\ x_2} + \mat{\B_1 \\ \B_2} u \]
		and similarly the output in the time domain
		\[ y = \C_1 x_1 + \D_1 u_1 + \C_2 x_2 + \D_2 x_2 = \mat{\C_1 & \C_2} \vet{x_1 \\ x_2} + \vet{D_1+D_2} u \]
		The associated \textit{big matrix} is so
		\begin{equation} 			
			\vet{\delta x_1 \\ \delta x_2 \\ y} = \mat{\begin{array}{c c | c}
					\A_1 & 0 & \B_1 \\ 0 & \A_2 & \B_2 \\ \hline \C_1 & \C_2 & \D_1 + \D_2
			\end{array}} \vet{x_1 \\ x_2 \\ u}
		\end{equation}
	
	\subsection*{Series (cascade) connection}
		A series connection (figure \ref{fig:dyn:series}) between two system is implemented by feeding the input of $\Sigma_2$ with the output of $\Sigma_1$; in this case the input-output of the overall system can be regarded in the Laplace/Z transform as
		\begin{equation}
		\begin{aligned}
			Y & = \hat \G_2 U_2 = \hat \G_2 Y_1 = \hat \G_2 \hat \G_1 U_1 \\ & = \hat \G_2 \hat \G_1  U = \hat \G U
		\end{aligned}
		\end{equation}
		Note that in this case the order of the transfer function $\hat \G= \hat \G_2 \hat \G_1$ is important due to the multiplication of matrix; in fact in order to make the series connection possible we need to have that the length of the vector $y_1$ is equal to the one of $x_2$: this makes possible to have a matrix $\hat \G$ the can be multiplied by $U$ and returns a proper $Y$.
	
		Carefully representing this system in the space of the states, we obtain that
		\begin{align*}
			\delta x_1 & = \A_1 x_1 + \B_1 u \\
			y_1 & = \C_1 x_1 + \D_1 u = u_2 \\
			\delta x_2 & = \A_2 x_2 + \B_2 u_2 = \A_2 x_2 + \B_2\big( \C_1 x_1 + \D_1 u \big) \\
			y= y_2 & = \C_2 x_2 + \D_2 u_2 = \C_2 x_2 + \D_2 \big(\C_1 x_1 + \D_1 u\big)
		\end{align*}
		hence the \textit{big matrix} is
		\begin{equation}
			\mat{ \begin{array} {c c | c}
					\A_1 & 0 & \B_1 \\ 
					\B_2 \C_1 & \A_2 & \B_2 \D_1 \\ \hline 
					\D_2 \C_1 & \C_2 & \D_2\D_1
			\end{array} }
		\end{equation}
		
		\begin{figure}[bht]
			\centering
			\tikzfigure{series}
			\caption{black box representation of a series connection between two systems.} \label{fig:dyn:series}
		\end{figure}
	
	\subsection*{Feedback connection}	
		\begin{figure}[bt]
			\centering
			\tikzfigure{feedback}
			\caption{black box representation of a series feedback connection of a system.} \label{fig:dyn:feedback}
		\end{figure}
		
		A feedback connection (figure \ref{fig:dyn:feedback}) of a system is realised by subtracting to the input $u$ of the system it's output $y$. Performing the analysis in the frequency domain we have have in fact the solution
		\[ Y = \hat\G_1 U_1 = \hat\G_1 (U-Y) \]
		We see that this expression is implicit in $Y$, but we can still obtain the solution
		\begin{equation}
		\begin{split}
			Y + \hat\G_1 Y = Y(\I + \hat \G_1)& =\hat\G_1 U \\
			Y &= (\I + \hat \G_1)^{-1} \hat \G_1 U
		\end{split}
		\end{equation}
		This solution is obtained by inverting the matrix $\I + \hat \G_1$ and this operation is possible if the matrix is \textit{\de{well-posed}}, condition that happens every time the matrix $\I + \D_1$ is invertible. By performing the analysis in the states space we have in fact 
		\begin{align*}
			y & = \C_1 x_1 + \D_1 u_1 = \C_1 x_1 + \D_1 (u-y) \\
			(\I + \D_1) y & = \C_1 x_1 + \D_1 u \\
			y & = (\I + \D_1)^{-1}\C_1 x_1 +(\I + \D_1)^{-1} \D_1 u
		\end{align*}
		From this expression is possible to observe why the well-posedness condition is related to having $\I + \D_1$ invertible. Expliciting the variation of the states we have instead we have
		\begin{align*}
			\delta x & = \A_1 x_1 + \B_1 u_1 = \A_1x_1 + \B_1 (u-y) \\
			& = \A_1 x_1 + \B_1 u - \B_1 \left( (\I + \D_1)^{-1}\C_1 x_1 +(\I + \D_1)^{-1} \D_1 u \right) \\
			& = \left( \A_1 - \B_1 (\I+\D_1)^{-1} \C_1 \right) x_1 + \left( \B_1 - \B_1 (\I+\D_1)^{-1} \D_1 \right)u
		\end{align*}
		From this states space expansion we can define the \textit{big matrix} associated to a feedback connection as
		\begin{equation}
			\mat{ \begin{array} {c | c}
				\A_1 - \B_1(\I+\D_1)^{-1} \C_1 & \B_1\left(\I - (\I+\D_1)^{-1} \D_1\right) \\ \hline 
				(\I+\D_1)^{-1}\C_1 & (\I+\D_1)^{-1} \D_1
			\end{array} } = 
			\mat{ \begin{array} {c | c}
				\A_1 - \B_1(\I+\D_1)^{-1} \C_1 & \B_1 (\I+\D_1)^{-1} \\ \hline 
				(\I+\D_1)^{-1}\C_1 & \D_1 (\I+\D_1)^{-1} 
		\end{array} }
		\end{equation}
	
\section{Realization theory}
	Given a rational polynomial $g(s)$ so defined as the ratio between two polynomial $n(s)/d(s)$ (this expression indeed represent the case of a one dimensional transfer function), such function is said
	\begin{itemize}
		\item \textbf{strictly proper} if the degree $\#n(s)$ of the denominator is less then the degree $\#d(s)$ od the denominator; this means that
		\[ \lim_{s\rightarrow \infty} g(s) = 0 \]
		\item \textbf{proper} if in general we have $\#n(s) \leq \#d(s)$, but more specifically it has to be
		\[ \lim_{s\rightarrow \infty} g(s) \neq \infty \]
		\item every time we have $\#n(s) > \#d(s)$, the rational polynomial is defined as \textbf{improper} and has
		\[ \lim_{s\rightarrow \infty} g(s) = \infty \]
	\end{itemize}	
	A rational polynomial $G(s)$ is \textbf{biproper} if both $g$ and it's inverse $g^{-1}$ are propers. Extending this concepts to a transfer function $\hat \G(s)$ (that's in general a matrix), the system is (strictly) proper if all it's entries are (strictly) proper; if one entry is improper, then $\hat \G(s)$ is also improper.
	
	In general it's proven that for linear time-invariant systems the transfer function $\hat G(s)$ is composed by a strictly proper and a proper term as
	\begin{equation} \label{eq:dyn:temp2}
		\hat \G(s) = \underbrace{\C(s\I - \A)^{-1}\B}_\textrm{strictly proper} + \underbrace{\D}_\textrm{proper}
	\end{equation}
		
	\paragraph{Realization} Given a transfer function $\hat \G(s)$, we say that the linear time-invariant system in the form
	\[ \begin{cases}
		\delta x = \A x + \B u \qquad x(0) = x_0\\ y = \C x + \D u 
	\end{cases} \] 
	is a \de{realization} of the transfer function $\hat G$ if equation \ref{eq:dyn:temp2} holds. \\
	This states that a LTI system characterized by the matrices $\A,\B,\C,\D$ is the realization of $\hat G(s)$ if it happens that $\hat G(s) = \C(s\I - \A)^{-1}\B + \D$. We also define two realization as \de{zero state equivalent} if they realise the same transfer function $\hat G(s)$.
		
	\subsection*{Realizations and transfer functions}
		An important theorem states that a transfer function $\hat \G(s)$ can be \textbf{realised} by an LTI system in the \textit{standard form} \textbf{if and only if} the rational function $\hat \G(s)$ is \textbf{proper}.
		
		\paragraph{Proof pt. 1} We can prove the \textit{only if} conditions stating
		\[ \mat{\begin{array}{c|c}
				\A & \B \\ \hline \C & \D
		\end{array}} \mapsto \hat \G(s) \]
		the proof is straightforward: as \ref{eq:dyn:temp2} states, given the constant matrices $\A,\B,\C,\D$ the resulting transfer function is strictly proper for the term $\C(s\I-\A)^{-1}\B$ and becomes proper only in the case when $\D\neq 0$.
		
		\paragraph{Proof pt. 2} More complex is proving the \textit{if} condition that starting from the transfer function computes the \textit{characteristic} matrices of LTI systems:
		\[ \hat \G(s) \mapsto \mat{\begin{array}{c|c}
				\A & \B \\ \hline \C & \D
		\end{array}}  \]
		We want now to prove that if the transfer function $\hat G(s)$ is proper then it state space representation exists. Determining the non-strictly proper element $\D$ is straightforward, in fact it can be computed simply as the limit
		\[ \mathcal D = \lim_{s\rightarrow \infty} \hat \G(s) \]
		We can so build the strictly proper transfer function as
		\[ \hat \G_{sp}(s) = \B(s\I - \A)^{-1}\C = \Gs - \D\]
		Building the polynomial $d(s)$ as the least common denominator of all the entries in the matrix $\hat \G_{sp}(s)$ and writing him in a form
		\[ d(s) = s^n + \alpha_1 s^{n-1} + \alpha_2 s^{n-2}+ \dots + \alpha_{n-1} s + \alpha_n \]
		we can rewrite the strictly proper part of the transfer function as a combination of matrices $N_i$ in the following way:
		\[ \hat \G_{sp}(s) = \frac{N(s)}{d(s)} = \frac{N_1 s^{n-1} + N_2 s^{n-2} + \dots + N_{n-1} s + N_n }{ s^n + \alpha_1 s^{n-1} + \alpha_2 s^{n-2}+ \dots + \alpha_{n-1} s + \alpha_n} \]
		With that said we have all the elements to compute the state space representation of the system: it can be proven in fact that the LTI has a state space representation in the form
		\begin{equation} \label{eq:dyn:Gtostaterep}
			\mat{\begin{array}{ c c c c | c}
				-\alpha_1 \I_k & - \alpha_2 \I_k & \dots & - \alpha_n \I_k & \I_k \\
				\I_k & 0 & \dots & 0 & 0 \\
				0 & \ddots & & 0 & \vdots \\
				0 & 0& \I_k & 0 & 0 \\ \hline
				N_1 & N_2 & \dots & N_m & \D
			\end{array}}
		\end{equation}
		where $\I_k$ is the $k\times k$ identity matrix; this notation is called \de{controllable canonical form}.
		
		\begin{example}{: state representation from a transfer function}
			Given a system characterized by a transfer function
			\[ \Gs = \mat{\frac{4s-10}{s+1} & \frac{3}{s+2} \\ \frac{1}{s+2} & \frac{4}{s+1} } \]
			in order to compute one it's representation the first thing to do is extract the output matrix $\D$ as
			\[ \D = \lim_{s\rightarrow \infty} \Gs = \mat{4 & 0 \\ 0 & 0} \]
			And so we can compute the strictly  proper part as
			\begin{align*}
				\hat G_{sp}(s) & = \Gs - \D = \mat{\frac{-14}{s+1} & \frac{3}{s+2} \\ \frac{1}{s+2} & \frac{4}{s+1} } \\
				& = \frac 1 {(s+1)(s+2)} \mat{-14(s+2) & 3 (s+1) \\ 1(s+1) & 4(s+2)} = \frac 1{s^2 + 3s + 2} \mat{-14 s - 28 & 3s + 3 \\ s+1 & 4s + 8} \\
				& = \frac{\mat{-14 & 3 \\ 1 & 4}s + \mat{-28 & 3 \\ 1 & 8} }{s^2 + 3s +2}
			\end{align*}
			Using equation \ref{eq:dyn:Gtostaterep} we can compute the state representation of the system using the yet computed denominator and matrices $N_i$ providing the result
			\[ \mat{\begin{array}{c c c c | c c}
				-3 & 0 & -2 & 0 & 1 & 0 \\
				0 & -3 & 0 & -2 & 0 & 1 \\
				1 & 0 & 0 & 0 & 0 & 0 \\
				0 & 1 & 0 & 0 & 0 & 0 \\ \hline
				-14 & 3 & -28 & 3 & 4 & 0 \\ 
				1 & 4 & 1 & 8 & 0 & 0
			\end{array}} \]
			By computing $\C (s\I - \A)^{-1} \B + \D$ using the matrices yet obtain we can indeed see that, after some simplifications, that we obtain the original transfer function $\Gs$.
		\end{example}
		
		\paragraph{Proof pt. 3} All that's left now is to prove that equation \ref{eq:dyn:Gtostaterep} indeed is a realization of the proper transfer function $\Gs$. In order to do so we consider we consider the sub-matrix $z(s)$ defined as
		\[ z(s) = \mat{z_1^t & z_2^t & \dots & z_n^t} = (s\I - \A)^{-1} \B \qquad \Rightarrow \qquad (s\I-\A) z(s) = \B \]
		Considering the \textit{lower part} of the matrices $\A$ and $\B$ (the last 3 rows in equation \ref{eq:dyn:Gtostaterep}), the multiplication results in the set of equalities
		\[ s z_2 -z_1 = 0 \qquad sz_3 - z_2 = 0 \qquad \dots \qquad sz_n = z_{n-1} \]
		Considering so that $z_n = \frac 1 s z_{n-1}$ but also $z_{n-1} = \frac 1 s z_{n-2}$ up to $z_2 = \frac 1 2 z_1$ we can determine the generic vector $z_k$ as
		\begin{equation} \label{eq:dyn:temp3}
			z_k = \frac 1{s^{k-1}} z_1
		\end{equation}
		Considering so now the multiplication $(s\I - \A)z(s) = \B$ using the top row of $\A,\B$ respect to the formulation in equation \ref{eq:dyn:Gtostaterep}, we have that
		\[ (s+\alpha_1) z_1 + \alpha_2 z_2 + \dots + \alpha_n z_n = \I_k \]
		hence for relation \ref{eq:dyn:temp3}
		\[ \left( s + \alpha_1 + \frac{\alpha^2}{s} + \dots + \frac{\alpha_{n-1}}{s^{n-2}} + \frac{\alpha_n}{s^{n-1}} \right) z_1 = \I_k \]
		Since the polynomial in the left-hand side is given by the ratio $\frac{d(s)}{s^{n-1}}$ we can define the sub-matrix $z$ as
		\[ z(s) = \mat{z_1 \\ z_2 \\ \vdots \\ z_n} = \frac 1 {d(s)} \mat{s^{n-1}\I_k \\ s^{n-2}\I_k \\ \vdots \\ \I_k} \]
		hence
		\begin{align*}
			\C(s\I-\A)^{-1} \B & = \C z(s) = \frac 1{d(s)} \mat{N_1 & N_2 & \dots  & N_n} \mat{s^{n-1}\I_k \\ s^{n-2}\I_k \\ \vdots \\ \I_k} \\
			& = \hat \G_{sp}(s)
		\end{align*}
		
		\subsubsection{LTI SISO system case and multiple realizations} \label{sec:canonicalforms}
		 A SISO (single-input single-output) LTI system is characterized by a transfer function that's simply a rational polynomial in the form
		\[ \Gs = \frac{\beta_1 s^{n-1} + \beta_2 s^{n-2} + \dots + \beta_{n-1} s + \beta_ m}{s^n + \alpha_1 s^{n-1} + \alpha_2 s^{n-2}+ \dots + \alpha_{n-1} s + \alpha_n } \]
		Recalling the results of equation \ref{eq:dyn:Gtostaterep}, a realization of this kind of system is characterized by the matrices
		\[ \A = \mat{-\alpha_1 & - \alpha_2 & \dots & - \alpha_{n-1} & - \alpha_n \\
		1 & 0 & \dots & 0 & 0 \\
		0 & 1 & \dots & 0 & 0 \\ 
		\vdots & \vdots & \ddots & \vdots & \vdots \\
		0 & 0 & \dots & 1 & 0 } \qquad \B = \mat{1 \\ 0 \\ \vdots \\ 0 \\ 0} \qquad \C \mat{\beta_1 & \beta_2 & \dots & \beta_{n-1} & \beta_n} \]
		Figure \ref{fig:dyn:controllablecanonical} shows how a system described in a controllable canonical representation (as in this case) can be graphed.
		
		\begin{figure}[bht]
			\centering \resizebox{0.9\linewidth}{!}{\tikzfigure{controlform}}
				\caption{graph representation of an LTI SISO system expressed in the controllable canonical form; the $1/s$ blocks in practise are representing integrators of the input.} \label{fig:dyn:controllablecanonical}
		\end{figure}
		
		In general this is not the lonely allowable representation of the transfer function $\Gs$, but there exists infinitely many ones. Considering $T$ as a non-singular matrix, we can define a new vector of the states $\tilde x = T x$ (hence $x = T^{-1} \tilde x$) that allows us to determine a new state space representation of the system, in fact we can build
		\begin{equation*}
		\begin{cases}
			\delta \tilde x = T \, \delta x  = T \A x + T \B u = T A T^{-1} \tilde x + T\B u = \tilde \A \tilde x + \tilde \B u \\
			\tilde y = \C x + \D u = \C T^{-1}\tilde x + \D u = \tilde \C \tilde x + \D u
		\end{cases}
		\end{equation*}
		We can so state that the systems
		\begin{equation} \label{eq:dyn:algebraicequivalence}
			\mat{\begin{array}{c|c}
					\tilde \A & \tilde \B \\ \hline \tilde \C & \D
			\end{array}} = \mat{\begin{array}{c|c}
				T\A T^{-1} & T \B \\ \hline \C T^{-1} & \D
			\end{array}} \qquad \textrm{and} \qquad \mat{\begin{array}{c|c}
				\A & \B \\ \hline \C & \D
			\end{array}}
		\end{equation}
		are \de{algebraically equivalent} and are indeed representations of the same transfer function $\Gs$. In this case the non-singular matrix $T$ is referred as \textbf{\textit{similarity/equivalence transformation}} and allows to describe the system into a different set of states. This operation can be useful because it might allow to express a system in a more suitable way for numerical computations.
		
		An example of algebraically equivalent representation is the \de{observable canonical form} described as
		\begin{equation}
		\mat{\begin{array}{ c c c c | c}
			-\alpha_1 & 1 & 0 & 0 & \beta_1 \\
			\vdots & 0 & \ddots & 0 & \vdots  \\
			-\alpha_{n-1} & 0 & 0 & 1 & \beta_{n-1} \\
			-\alpha_n & 0 & \dots & 0 & \beta_n \\ \hline
			1 & 0 & \dots & 0 
		\end{array}}
		\end{equation}
		The graph representation of such system is shown in figure \ref{fig:dyn:observablecanonical}.
	
		\begin{figure}[bht]
			\centering \resizebox{0.9\linewidth}{!}{\tikzfigure{observableform}}
			\caption{graph representation of an LTI SISO system expressed in the controllable canonical form; the $1/s$ blocks in practise are representing integrators of the input.} \label{fig:dyn:observablecanonical}
		\end{figure}
	
\section{Solution of linear time-varying systems}
	\subsection*{Continuous-time linear time-variant systems}
	Given a continuous-time linear time-variant  system (CT-LTV), it's solution must be a function of the time $t$ and must be differentiable (in order to be the solution of the differential equations). In particular the zero input response of the system expressed as
	\[ \dot x = \A(t) x \hspace{2cm} x(t_0 ) = x_0 \]
	can be computed using the \de{Peano-Baker series}, a matrix $\Phi(t,t_0) \in \mathds R^{n\times n}$ defined as
	\begin{equation} \label{eq:dyn:ct-statetransition}
	\begin{split}
		\Phi(t,t_0) = \I + & \int_{t_0}^t \A(s_1)\, ds_1 + \int_{t_0}^t \A(s_1) \int_{t_0}^{s_1} \A(s_2)\, ds_2\, ds_1 + \\ & \hspace{1cm}+ \int_{t_0}^t \A(s_1) \int_{t_0}^{s_1} \A(s_2) \int_{t_0}^{s_2} \A(s_3)\, ds_3\, ds_2\, ds_1 + \dots
	\end{split}
	\end{equation}
	The matrix $\Phi$ is mostly referred as the \de{state transition matrix} and is characterized by the following property:
	\begin{enumerate}[\itshape i)]
		\item it has that $\Phi(t,t) = \I$ for any $t\in \mathds R$; this result come just by the definition of the state transition matrix: considering in fact that for the integrals $\int_a^a f(x)\, dx = 0$, then the evaluation for $t=t_0$ cancels all the terms unless the identity matrix;
		
		\item a second important property is associated to the derivation of $\Phi$; knowing that $\frac d{dt} \int_a^t f(\tau) \, d\tau = f(\tau)$, them we can show that
		\begin{align*}
			\frac d{dt} \Phi(t,t_0) & = 0 + \A(t) + \A(t) \int_{t_0}^t \A(s_2) \, ds_2 + \A(t) \int_{t_0}^{t} \A(s_2) \int_{t_0}^{s_2} \A(s_3)\, ds_3\, ds_2 + \dots \\
			& = \A(t) \Phi(t,t_0)
		\end{align*}	
	\end{enumerate}
	
	With this property states we can compute and prove that the zero input response solution of a continuous-time linear system is determined as
	\begin{equation} \label{eq:dyn:zeroinputres}
		x(t) = \Phi(t,t_0) x_0 \hspace{2cm} \forall t 
	\end{equation}
	This solution is also proven to be unique (but we won't prove that). Equation \ref{eq:dyn:zeroinputres} can be proven considering that $x(t_0)$ gives indeed $\Phi(t_0,t_0) x_0 = \I x_0 = x_0$ using property \textit{i)}. To generally prove that this formulation is also solution to the original state equation $\dot x(t) = \A(t) x(t)$ we can just take the derivative of the suggested solution $\Phi(t,t_0) x_0$ respect to time and using property \textit{ii)} we indeed have
	\[ \dot x(t) = \frac{d}{dt}\Big( \Phi(t,t_0) x_0 \Big) = \A(t) \phi(t,t_0) x_0 = \A(t) x(t) \]
	
	In order now to further extend the solution in case of non-zero inputs it's mandatory to define other 2 properties for the state transition matrix:	
	\begin{itemize}
		\item[\textit{iii)}] each $i$-th column of the matrix $\Phi(t,t_0)$ is the unique solution at the problem
		\[ \dot x = \A(t) x \qquad \textrm{with } x(t_0) = \vet{0 \\ \vdots \\ 0 \\ 1 \\ 0 \\ \vdots \\ 0} = e_i \] 
		where the \textit{one} is placed in the $i$-th position of the vector $x_0$, meaning that's the $i$-th canonical base $e_i$. This property is very useful for computation because it allow to decompose the study of the solution $x(t)$ by independently solving all the states;		
		
		\item[\textit{iv)}] it holds the \textbf{semi-group property} stating that 
		\[ \Phi(t,s) \Phi(s,\tau)= \Phi(t,\tau) \hspace{2cm} \forall t,s,\tau \in \mathds R \]
		We can in fact consider that, given the initial state $x_0$ at time $t_0$, the state at a second timespan $t_1$ is regarded as $x(t_1) = \Phi(t_1,t_0) x_0$. Considering now a second time $t_2$ we have that
		\begin{align*}
			x(t_2) = \Phi(t_2,t_1) x(t_1) & = \Phi(t_2,t_1) \Phi(t_1,t_0) x_0  \\
			& \Phi(t_2,t_0) x_0
		\end{align*}
		Note that the semi-group property doesn't set any requirement in the \textit{position} of the times $t,s,\tau$ that can in fact be also \textit{back in time}.
		
		\item[\textit{v)}] due to the semi-group property yet stated we have that the state transition matrix $\Phi(t,t_0)$ is always invertible (hence is non-singular) and it's inverse is equal to $\Phi(t,s) = \Phi(s,t)$. We can in fact see that 
		\[ \Phi(t,s) \Phi(s,t) = \Phi(t,t) = \I \hspace{2cm} \forall t,s \in \mathds R \] 
	\end{itemize}
	
	\paragraph{Addition of the inputs} The more general unique solution of a CT-LTV system defined by the state space representation
	\[ \begin{cases}
		\dot x = \A(t) x + \B(t) u \hspace{1.4cm} x(t_0) = x_0\\ 
		y = \C(t) x + \D(t) u
	\end{cases} \]
	is determined by equation
	\begin{equation} \label{eq:lin:varconstsants}
	\begin{split}
		x(t) & = \overbrace{\Phi(t,t_0) x_0}^\textrm{zero-input res.} + \overbrace{\int_{t_0}^t \Phi(t,\tau) \B(\tau) u(\tau) \, d\tau}^\textrm{zero-state response} \\
		y(t) & = \underbrace{\C(t) \Phi(t,t_0) x_0}_\textrm{homogeneous res.} + \underbrace{ \int_{t_0}^t \C(\tau) \Phi(t,\tau) \B(\tau) u(\tau)\, d\tau + \D(t) u(t) }_\textrm{forced response}
	\end{split}
	\end{equation}
	To prove that the provided solution works (we won't prove that's unique, but it happens so) we can simply derive in time the state solution and watch that matches the state equation $\dot x = \A(t) x + \B(t) u$. In order to do so we have to consider the following property in the derivation of composed function in integration, in fact
	\[ \frac{d}{dt} \int_{t_0}^t f(t,\tau)\, d\tau = f(t,t) + \int_{t_0}^t \frac{d}{dt}f(t,\tau)\, d\tau \]
	With this stated we have
	\begin{align*}
		\dot x(t) & = \frac{d}{dt} \left( \Phi(t,t_0) x_0 + \int_{t_0}^t \Phi(t,\tau) \B(\tau) u(\tau) \, d\tau \right) \\
		& = \A(t) \Phi(t,t_0) x_0 + \Phi(t,t) \B(t)u(t) + \int_{t_0}^t \frac{d\Phi}{dt} \B(\tau) u(\tau)\, d\tau \\
		& = \A(t)\Phi(t,t_0) x_0 \B(t) u(t) +  \int_{t_0}^t \A(t) \Phi(t,\tau) \B(\tau) u(\tau)\, d\tau \\
		& = \A(t) \underbrace{\left( \Phi(t,t_0) + \int_{t_0}^t \Phi(t,\tau) + \B(\tau) u(\tau)\, d\tau \right)}_{=x(t)} + \B(t)u(t)
	\end{align*}
	
	\subsection*{Discrete-time linear time-variant systems}
	Given a discrete-time linear time-variant DT-LTV system, the unique solution of the homogeneous term $x^+ = \A(t) x$ subject to an initial state $x(t_0) = x_0$ is determined by the expression
	\begin{equation}
		x(t) = \Phi(t,t_0) x_0 \hspace{2cm} x_0 \in \mathds R^n, \forall t \geq t_0
	\end{equation}
	\begin{note}
		In this case the solution is valid for time $t$ grater or equal to $t_0$; in the continuous time case, equation \ref{eq:dyn:zeroinputres} is valid also for $t < t_0$ due to the semi-group property; for discrete-time system the matrix $\Phi(t,t_0)$ can sometimes be singular, meaning that it cannot be inverted as it will be shown.
	\end{note} \noindent
	In this case $\Phi(t,t_0)$ is the \de{discrete-time state transition matrix} defined as
	\begin{equation} \label{eq:dyn:peanodiscrete}
		\Phi(t,t_0) = \begin{cases}
			\I & t = t_0 \\
			\A(t-1) \A(t-2) \dots \A(t_0+1) \A(t_0) \hspace{1cm} & t > t_0
		\end{cases}
	\end{equation}
	Such definition can be inductively retrieved: considering in fact that the state at the step $t_0+1$ can be computed as $x(t_0 + 1) + \A(t_0) x_0$ but also $x(t_0 + 2) = \A(t_0+1) x(t_1) = \A(t_0+2) \A(t_0+1) x_0$. Inductively it's proven how the discrete-time state transition matrix was defined. This formulation should be extended also to the \textit{backward times}, for $t < t_0$, in fact considering
	\[ x(\underbrace{t+1}_{=t_0}) = \A(t) x(t) \hspace{1.4cm} \Rightarrow \hspace{1.4cm} x(t) = \A^{-1}(t_0 - 1) x(t_0) \]
	This operation is feasible if and only if the matrix $\A(t_0)$ is non-singular that hence can be inverted, but this condition isn't stated a priori.
	
	As for the continuous time case, the general solution of the discrete-time linear time-variant systems of the form
	\[ \begin{cases}
		x^+ = \A(t) x + \B(t) u \hspace{1.4cm} x(t_0) = x_0\\ 
		y = \C(t) x + \D(t) u
	\end{cases} \]
	is determined by the functions
	\begin{equation}
	\begin{split}
		x(t) & = \Phi(t,t_0) x_0 + \sum_{\tau=0}^{t-1} \Phi(t,\tau+1) \B(\tau) u(\tau) \\
		y(t) & =  \C(t) \Phi(t,t_0) x_0 + \sum_{\tau=0}^{t-1} \C(\tau) \Phi(t,\tau+1) \B(\tau) u(\tau)  + \D(t)u(t)
	\end{split}
	\end{equation}

\section{Solutions to linear time-invariant systems}
	\subsubsection*{Discrete-time case}
	Starting with the case of discrete-time case, if the system is time invariant than it means that the matrices $\A,\B,\C,\D$ does not depends on time. Considering the definition of Peano-Bakers series for discrete-time systems (equation \ref{eq:dyn:peanodiscrete}), knowing that $\A(t)$ is constant (and is so regarded as $\A$), then it means that the state transition matrix can be regarded as
	\begin{equation}
		\Phi(t,t_0) = \A^{t-t_0} \hspace{2cm} \forall t\geq t_0
	\end{equation}
	where conventionally $\A^0 = \I$. With that we can rewrite the solutions of the DT-LTI system as
	\begin{equation}
	\begin{split}
		x(t) & = \A^{t-t_0} x_0 +  \sum_{\tau=0}^{t-1} \A^{t-\tau-1} \B u(\tau) \\
		y(t) & = \C \A^{t-t_0} + \sum_{\tau=0}^{t-1} \C \A^{t-\tau -1} \B u(\tau) + \D u(t)
	\end{split}
	\end{equation}
	We refer to $\A^t$ (with $t\in \mathds Z$) as the \de{matrix power} and it will be the goal in the definition of the solution is to identify a \textit{clever} way to compute it's value (because numerically it's not feasible to perform matrix multiplication at each iteration); such problem will be fixed later with the definition of the Jordan normal form for matrices.
	
	\subsubsection{Continuous-time case}
	Considering now the continuos-time case for LTI systems, the definition of the Peano-Baker series (equation \ref{eq:dyn:ct-statetransition}, page \pageref{eq:dyn:ct-statetransition}) can be simplified: the matrices $\A$ can in fact be taken outside the integral terms resulting in
	\begin{align*}
		\Phi(t,t_0) & = \I + \A(t-t_0) + \A^2 \frac{(t-t_0)^2}{2} + \A^3 \frac{(t-t_0)^3}{2\cdot 3} + \dots \\ & = \sum_{k=0}^{\infty} \A^k \frac{(t-t_0)^k}{k!}
	\end{align*}
	This definition resembles the exponential; considering in fact the Taylor series expansion of the exponential is $e^x = \sum_{k=0}^\infty \frac 1{k!}x^k$, we can determine the state transition matrix as the \de{exponential matrix}
	\begin{equation}
		\Phi(t,t_0) = e^{\A(t-t_0)}
	\end{equation}
	hence the unique solution of a CT-LTI system is provided by the expression
	\begin{equation}
	\begin{split}
		x(t) & = e^{\A(t-t_0)} x_0 + \int_{t_0}^t e^{\A(t-\tau)} \B u(\tau) \, d\tau \\
		y(t) & = \C e^{\A(t-t_0)} x_0 + \int_{t_0}^t \C e^{\A(t-t_0)} \B u(\tau)\, d\tau + \D u(t)
	\end{split}
	\end{equation}
	
	Relevant properties of the exponential matrix are
	\begin{enumerate}[\itshape i)]
		\item that each $i$-th column of the matrix $e^{\A t}$ is the unique solution of the problem
		\[ \dot x(t) = \A x(t) \hspace{2cm} \textrm{with } x(0)= e_i \]
		\item it still holds the semi-group property stating that $\Phi(t,\tau) \Phi(\tau,s) = \Phi(t,s)$ ($\forall t,s,\tau$); by an application point of view it means that for the exponential matrix
		\[ e^{\A t} e^{\A s} = e^{\A(t+s)} \]
		\item it's always invertible, in fact from the semi-group property we have that $e^{\A t} e^{-\A t} = e^{\A 0} = \I$ and so it means that $e^{\A t}$ is always non-singular (hence invertible) and its inverse is defined as
		\[ \big(e^{\A t} \big)^{-1} = e^{-\A t} \]
		\item for any matrix $\A \in \mathds R^{n\times n}$ it holds the following \textit{"commutative"} property:
		\[ \A e^{\A t} = e^{\A t} \A \hspace{2cm} \forall t \]
		
		\item there exists $n$ functions $\alpha_i(t)$ such that
		\[ e^{\A t} = \sum_{k=0}^{n-1} \alpha_k (t) \A^k \]
		
	\end{enumerate}
	
	\paragraph{Cayley-Hamilton theorem} The \textbf{Cayley-Hamilton theorem} states that { \itshape for each matrix $\A \in \mathds R^{n\times n}$ with characteristic polynomial
	\[ p_\A(s) = \det (s\I -\A) = s^n + a_1 s^{n-1} + \dots + a_{n-1} s + a_n \] then  $\A$ is a matrix solution of this polynomial, meaning that
	\[ \A^n + a_1 \A^{n-1} + \dots a_{n-1} \A + a_n \I = 0 \] }
	In this case $0$ is a $n\times n$ matrix of zeros. By reversing this quantity we can indeed see that
	\[ \A^n = - \big( a_1 \A^{n-1} + \dots a_{n-1} \A + a_n \I \big) \]
	and so by multiplying both sides of the equation by $\A$ allows to compute the exponential $\A^{n+1}$ as
	\begin{align*}
		\A^{n+1} & = -\A^n a_1 - a_2 \A^{n-1} - \dots - a_{n-1}\A^2 - a_n \A  \\
		& = a_1\big(a_1 \A^{n-1} + \dots + za_{n-1} \A + a_n \I \big) - a_2 \A^{n-1} - \dots - a_{n-1}\A^2 - a_n \A \\
		& = (a_1^2 - a_2) \A^{n-1} + (a_1 a_2 - a_3) \A^{n-2} + \dots + (a_1 a_{n-1} - a_n) \A + a_1 a_n \I - \dots -a_{n-1} \A^2
	\end{align*}
	In general this proposition can be extended and any exponential of a matrix $\A \in \mathds R^{n\times n}$ can be regarded as a linear combination of the first $n$ exponentials of matrix $\A$:
	\begin{equation} \label{eq:lin:cayley}
		\A^h = \sum_{k=0}^{n-1} c_{h,k} \A^k
	\end{equation}

	\subsection{Jordan normal form}
		As was show in equation \ref{eq:dyn:algebraicequivalence} (page \pageref{eq:dyn:algebraicequivalence}), using a transformation matrix $T$ a linear system characterized by a state space representation
		\[ \begin{cases}
			\delta x = \A x + \B u \\ y = \C x + \D u
		\end{cases} \]
		can be transformed in a algebraically equivalent representation that could have resulted in a easier computation model. From an ideal point of view the \textit{best} solution we can get after transformation is a \textbf{diagonal state matrix} in the form
		\[ T \A T^{-1} = \mat{ \lambda_1 & 0 & \dots & 0 \\
		0 & \lambda_2 & \ddots  & \vdots \\ 
		\vdots & \ddots & \ddots & 0 \\
		0 & \dots & 0 & \lambda_n } \]
		where $\lambda_i$ are the eigenvalues of the \textit{original} matrix $\A$ (values that are always shared in each algebraic equivalent representation of the same system).
		
		A problem of this kind of formulation is that in practise not all matrices $\A$ are diagonalizable, and in particular this happens when the geometric multiplicity doesn't match the algebraic multiplicity of the related eigenvalues.
		
		A solution of this problem is provided by Jordan with a theorem stating that { \itshape for each matrix $\A \in \mathds C^{n\times n}$ with eigenvalues $\lambda_1, \dots, \lambda_m \in \mathds C$ there exists an non-singular (hence invertible) matrix $T \in \mathds C^{n\times n}$ such that
		\begin{equation}
			\J = T \A T^{-1} = \mat{ J_1 & 0 & \dots & 0 \\
				0 & J_2 & \ddots  & \vdots \\ 
				\vdots & \ddots & \ddots & 0 \\
				0 & \dots & 0 & J_l } \quad \in \mathds C^{n\times n} 
		\end{equation} where $J_i$ are matrices in the form
		\begin{equation}
			J_i = \mat{\lambda_i & 1 & 0 & \dots & 0 \\
			0 & \lambda_i & 1 & \dots & 0 \\
			0 & 0 & \lambda_i & \dots & 0 \\
			\vdots & \vdots & \ddots & \vdots \\
			0 & 0 & 0 & \dots & \lambda_i } \quad \in \mathds C^{n_i\times n_i}
		\end{equation} }
	
		The matrix $\J$ is known as the \de{Jordan normal form} of $\A$ and is proven to be unique (upon rearrangement of the sub-matrices $J_i$). Note that in general sub-matrices $J_i,J_j$ (with $i\neq j$) might be constructed with the same eigenvalue $\lambda_k$ but what might change are their dimension ($n_i\neq n_j$). \\		
		The number of \textit{blocks} in the Jordan normal form are $l \leq n$ and are associated to each linearly independent eigenvectors of $\A$ (is proven that at least one eigenvector exists for each matrix $\A$ and there can be maximum $n$ linearly independent ones).
		
		A matrix is so called \textbf{semi-simple} (or \textbf{diagonalizable}) if its Jordan normal form is diagonal. In particular given a matrix $\A \in \mathds C^{n\times n}$ the 3 following statements are equivalents:
		\begin{itemize}
			\item $\A$ is semi-simple;
			\item $\A$ has $n$ linearly independent eigenvectors;
			\item $p(\A) \neq 0$ for all non-zero polynomials $p(s)$ having a degree less then $n$.
		\end{itemize}
	
	\subsubsection{Matrix power}
		The power matrix value can be \textit{easily} determined considering the Jordan normal form of the matrix on which we want to compute the power $\A^t$; in particular we can see that
		\[ \A^ t = \underbrace{ \overbrace{T^{-1} \J T}^{=\A}  T^{-1} \J T \dots T^{-1} \J T }_{t \textrm{ times } }  \]
		Observing that \textit{in the middle} the terms $T T^{-1}$ cancels out (they evaluate to the identity matrix $\I$) what we obtain is that
		\begin{equation} \label{eq:lin:powermatrix}
			\A^t = T^{-1} \J^t T
		\end{equation}
		where the power of a matrix in Jordan forms evaluates to
		\begin{equation} \label{eq:lin:Ji}
			\J^t = \mat{J_1^t \\ & J_2^t \\ && \ddots \\ &&& J_l^t}
		\end{equation} 
		where
		\begin{equation} J_i^t = \mat{ 
			\lambda_i^t & t \lambda_i^{t-1} & \frac{t! \lambda_i^{t-2 }}{(t-2)! 2!} & \frac{t! \lambda_i^{t-3 }}{(t-3)! 3!} & \dots & \frac{t! \lambda_i^{t-n_i+1}}{(t-n_i+1)!(n_i-1)!} \\
			0&\lambda_i^t & t \lambda_i^{t-1} &  \frac{t! \lambda_i^{t-2 }}{(t-2)! 2!} &  \dots & \frac{t! \lambda_i^{t-n_i+2}}{(t-n_i+2)!(n_i-2)!}\\
			\vdots & \ddots &\lambda_i^t &  t \lambda_i^{t-1} & \dots & \frac{t! \lambda_i^{t-n_i+3}}{(t-n_i+3)!(n_i-3)!} \\
			\vdots && \ddots & \ddots & \ddots & \vdots\\
			\vdots&&& \ddots &\lambda_i^t &  t \lambda_i^{t-1} &\\
			0& \dots& \dots & \dots& 0&\lambda_i^t &  \\			
		 } \end{equation}
		
		\paragraph{Region of convergence of the system} Considering that the homogeneous response of the discrete time system $x(t+1) = \A x(t)$ (with $x(0) = x_0$) is determines as
		\[ x(t)= \A^t x_0 \]
		In order to have a convergent solution we want to ensure that $\lim_{t\rightarrow\infty} \A^t$ tends to zero (or at least it doesn't diverge), in the sense that all it's entry are zero (or non-diverging). Considering that the matrix power $\A^t$ can be regarded as $T^{-1}\J T$, the limit is intrinsecally transmitted to the Jordan normal form matrix $\J$ for which we can state
		\[ \lim_{t\rightarrow\infty} \J^t = 0 \qquad \Leftrightarrow \qquad \lim_{t\rightarrow\infty} J_i^t = 0 \quad \forall i  \]
		Considering the formal definition of the sub-matrix power $J_i^t$ shown in equation \ref{eq:lin:Ji}, we observe that each entry of the matrix is made by the eigenvalue $\lambda_i$ to the power of $t$ and a polynomial of $t$. Considering that the exponential asymptotically grows/decreases faster then polynomial we observe that whenever $|\lambda_i| \leq 1$ then all entries converges to 0. We can so state that if all eigenvalues $\lambda_i$ of the state matrix $\A$ have a magnitude less then 1, then the system converges to a null solution.
		
		In contrary if we consider $|\lambda_i|>1$ the exponential $\lambda_i^t$ explodes to infinity for $t\rightarrow \infty$, hence the systems behaviour diverges. All that's left to analyse is the case where $\J^t$ (for $\rightarrow \infty$) doesn't diverge and this happens when $|\lambda_i| = 1$ (where $1^t \xrightarrow{t\rightarrow \infty} 1$), but in order to ensure that the matrix exponential doesn't diverge we have to impose that $n_i = 1$ (in this case we don't have entries with diverging polynomials). In general we can so state that
		\[ \lim_{t\rightarrow\infty} \A^t \neq \infty \quad \Leftrightarrow \quad \lim_{t\rightarrow\infty} J_i^t \neq \infty  \  \forall i \quad \Leftrightarrow \quad |\lambda_i| \leq 1 \textrm{ and if } |\lambda_i|=1  \Rightarrow n_i=1 \]
		
		\paragraph{Fibonacci sequence} Recalling the Fibonacci state representation reported in equation \ref{eq:dyn:temp4} (page \pageref{eq:dyn:temp4}), we have that the problem was described as
		\[ x^+ = \mat{1 & 1 \\ 1 & 0} x \hspace{2cm} y = \mat{1 & 0} x \hspace{2cm} x(0) = \vet{0 \\ 1} \]
		In this case the polynomial characteristic of the state matrix $\A \in \mathds R^{2\times 2}$ evaluates as $p_\A(s) = s^2-s-1$ whose roots (hence eigenvalues of $\A$) are $s_{1,2} = \frac{1\pm \sqrt 5}{2}$. Having two distinct eigenvalues this means that the Jordan normal form of $\A$ is the diagonal matrix
		\[ \J= \mat{\frac{1+\sqrt 5}{2} & 0 \\ 0 & \frac{1-\sqrt 5}{2}} \]
		In order now to determine the transformation matrix $T \in \mathds R^{2\times 2}$ we need to solve the linear system $T\A = \J T$, hence
		\[ \mat{t_1 & t_2 \\ t_3 & t_4} \mat{1 & 1 \\ 1 & 0} = \mat{\frac{1+\sqrt 5}{2} & 0 \\ 0 & \frac{1-\sqrt 5}{2}} \mat{t_1 & t_2 \\ t_3 & t_4} \qquad \Rightarrow \qquad \begin{cases}
			t_1 + t_2  & = \frac{1+\sqrt 5}{2} t_1 \\
			t_1  & = \frac{1+\sqrt 5}{2} t_2 \\
			t_3 + t_4 & = \frac{1-\sqrt 5}{2} t_3 \\
			t_3 & =  \frac{1-\sqrt 5}{2 } t_4
		\end{cases}   \]
		hence
		\[ t_1 = \frac{1+\sqrt 5}{2}t_2 \hspace{2cm} t_3 = \frac{1-\sqrt 5}{2} t_4 \hspace{2cm} \forall t_2,t_4 \]
		In order not to have a singular matrix we have to chose value $t_2,t_4 \neq 0$; picking for simplicity $t_2 = t_4 = 2$ the transformation matrix we obtain is
		\[ T = \mat{ 1 + \sqrt 5 & 2 \\ 1- \sqrt 5 & 2 } \hspace{1.2cm} \Rightarrow \hspace{1.2cm} T^{-1} = \frac{1}{4\sqrt 5} \mat{ 2 & - 2 \\ - (1-\sqrt 5) & 1 + \sqrt 5 } \]
		Using equation \ref{eq:lin:powermatrix} we can compute the matrix power $\A^t$ as $T^{-1} \J T$; to ease the computation we can see that $1+\sqrt 5 = 2 \lambda_1$ and $1-\sqrt 5 = 2 \lambda_2$ we can write the equation as
		\begin{align*}
			\A^t & = \frac{1}{4\sqrt 5} \mat{ 2 & - 2 \\ -2\lambda_2 & 2\lambda_1} \mat{ \lambda_1^t & 0 \\ 0 & \lambda_2^t}  \mat{ 2\lambda_1 & 2 \\ 2\lambda_2 & 2 } = \frac 1{\sqrt 5} \mat{ 1 & - 1 \\ -\lambda_2 & \lambda_1} \mat{ \lambda_1^t & 0 \\ 0 & \lambda_2^t}  \mat{ \lambda_1 & 1 \\ \lambda_2 & 1 } \\
			& = \frac 1{\sqrt 5} \mat{ 1 & - 1 \\ -\lambda_2 & \lambda_1} \mat{ \lambda_1^{t+1} & \lambda_1^t \\\lambda_2^{t+1} & \lambda_2^t } = \frac 1 {\sqrt 5} \mat{\lambda_1^{t+1} - \lambda_2^{t+1} & \lambda_1^t - \lambda_2^t \\ - \lambda_1^{t+1} \lambda_2 + \lambda_1 \lambda_2^{1+t} & - \lambda_1^t \lambda_2 + \lambda_1 \lambda_2^t}
		\end{align*}
		Computing the homogeneous solution of the system as $y(t) = \C\A^t x_0$, given the particular formulation of the matrices $\C$ and $x_0$ means that the output $y(t)$ is determined by the second entry in the first column, hence
		\begin{align*}
			y(t) & =  \mat{1 & 0} \frac 1 {\sqrt 5} \mat{\lambda_1^{t+1} - \lambda_2^{t+1} & \lambda_1^t - \lambda_2^t \\ - \lambda_1^{t+1} \lambda_2 + \lambda_1 \lambda_2^{1+t} & - \lambda_1^t \lambda_2 + \lambda_1 \lambda_2^t} \vet{0 \\ 1}  = \frac 1{\sqrt 5}\left(\lambda_1^t - \lambda_2^t\right) \\ & = \frac 1{\sqrt 5} \left[ \left( \frac{1 + \sqrt 5}{2} \right)^t - \left( \frac{1 - \sqrt 5}{2} \right)^t \right] 
		\end{align*}
			
	
	\subsubsection{Matrix exponential}
		If we want to compute the exponential $e^{\A t}$ of the matrix $\A$ with Jordan normal form $\J$ we can use the fact that the exponential of such Jordan form is
		\begin{equation} \label{eq:lin:jordanexponential}
			e^{\J t} = \mat{ e^{J_1 t} && 0\\ & \ddots \\ 0 & &  e^{J_lt} }
		\end{equation}
		where $e^{J_it}\in \mathds R^{n_i\times n_i}$ is defined as 
		\begin{equation}
			e^{J_i t} = e^{\lambda_i t} \mat{ 1 & t & \dots & \frac{t^{n_i-1}}{(n_i-1)!} \\
			& \ddots & \ddots & \vdots \\
			&& \ddots & t	\\ &&& 1	}
		\end{equation}
		
		Considering now the Cayley-Hamilton theorem (equation \ref{eq:lin:cayley}, page \pageref{eq:lin:cayley}) we have that the matrix exponential can be regarded as a linear combination of the first $n$ matrix powers as
		\[ e^{\A t} = \sum_{k=0}^{n-1} \alpha_k(t) \A^k \]
		Knowing so that $\A= T^{-1} \J T$ we can rewrite
		\begin{equation} \label{eq:lin:exponentialmatrix}
		\begin{split}
			e^{\A t} & = \sum_{k=0}^{n-1} \alpha_k(t) T^{-1} \J^k T = T^{-1} \left( \sum_{k=0}^{n-1} \J^k \right) T \\
			& = T^{-1} e^{\J t} T
		\end{split}
		\end{equation}
		on which we can compute the formal definition of the Jordan form exponential as shown in equation \ref{eq:lin:jordanexponential}.
		
		\paragraph{Region of convergence} Considering so that the exponential $e^{\J t}$ is the unique solution of the problem $\dot x(t) = \J x(t)$ with $x(0) = \I$, we can so analyse the asymptotic behaviour of the system (as was done for the discrete-time ones). In general the eigenvalues an be complex ($\lambda_i = \alpha_i + j \beta_i \in \mathds C$) and we can show that the sub-matrix exponentials $e^{J_it}$ converges for $t\rightarrow \infty$ if and only if the real part $\alpha_i$ of the eigenvalues is strictly negative; at contrary if $\alpha_i > 0$ what we obtain is that the term $e^{\lambda_i t}$ premultiplying the whole matrix diverges and hence $\J$.
		
		In the case where $\alpha_i = 0$ we have the case of discrete-time systems: if the dimension $n_i$ is unitary then $e^{J_i}$ is a constant (non-infinite) 1-dimensional matrix, while if $n_i>1$ the upper triangular terms polynomially explodes to $\infty$:
		\[ \lim_{t\rightarrow\infty} e^{\A t} \neq \infty \quad \Leftrightarrow \quad \lim_{t\rightarrow\infty} e^{J_i t} \neq \infty  \  \forall i \quad \Leftrightarrow \quad \Re{\lambda_i} \leq 0 \textrm{ and if } \Re{\lambda_i} = 0 \Rightarrow n_i=1 \]
		
		\paragraph{Example} Given the continuous time LTI system characterized by the state matrix
		\[ \A= \mat{ 5 & 3 \\ -6 & -4} \]
		in order to compute it's exponential $e^{\A t}$ we have to firstly determine it's Jordan normal form $\J$. This can be achieved by firstly determining the eigenvalues of the matrix; knowing that the characteristic polynomial is $p(\lambda) = (5 - \lambda)(-4-\lambda) + 6\cdot 3 = \lambda^2 - \lambda - 2$ we can compute it's roots and hence determine $\J$:
		\[ \lambda_{1,2} = \frac{1\pm \sqrt 9}{2} \hspace{1.2cm} \Rightarrow \hspace{1.2cm} \J= \mat{ 2 & 0 \\ 0 & -1} \]
		With this result we have to compute the transformation matrix $T$ by imposing that $\J T = T A$ and solving the related linear system in the unknowns entries of $T$:
		\[ \mat{ 2 & 0 \\ 0 & -1} \mat{t_1 & t_2 \\ t_3 & t_4} = \mat{t_1 & t_2 \\ t_3 & t_4} \mat{ 5 & 3 \\ -6 & -4} \]
		\[ \begin{cases}
			2 t_1 & = 5 t_1 - 6t_2 \\
			2 t_2 & = 3 t_1 - 4 t_2 \\
			-t_3 & = 5 t_3 - 6 t_4 \\
			-t_4 & = 3t_3 - 4 t_4
		\end{cases} \qquad \Rightarrow \quad \begin{cases}
			2t_2 = t_1 \\
			t_3 = t_4
		\end{cases} \]
		Choosing as free parameters $t_1 = t_3 = 1$ for simplicity we obtain the transformation matrix
		\[ T = \mat{2 & 1 \\ 1 & 1} \hspace{1.2cm} \Rightarrow \hspace{1.2cm} T^{-1} = \mat{  1 & - 1 \\ - 1 & 2} \]
		We can finally compute the exponential matrix $e^{\A t}$ as $T^{-1} e^{\J t} T$ (equation \ref{eq:lin:exponentialmatrix}):
		\begin{align*}
			e^{\A t} & = \mat{  1 & - 1 \\ - 1 & 2} \mat{e^{2t} & 0 \\ 0 & e^{-t} } \mat{2 & 1 \\ 1 & 1} = \mat{  1 & - 1 \\ - 1 & 2}  \mat{ 2 e^{2t} & e^{2t} \\ e^{-t} & e^{-t} } \\
			& = \mat{2 e^{2t} -e^{-t} & e^{2t} - e^{-t} \\ -2 e^{2t} + 2 e^{-t} & -e^{2t} + 2 e^{-to} }
		\end{align*}
		
\section{Internal / Lyapunov stability}
	\paragraph{Norms for vector} In order to start the discussion regarding the stability of linear systems, we have to recall the norms for vector and matrices. Given a vector $x\in \mathds R^n$, we define it's \de{norm} $x\mapsto |x|$ an operation that satisfy the following properties:
	\begin{align*}
		i) \qquad & |x|\geq 0 \ \forall x\in  \mathds R^n \quad \textrm{and} \quad |x|=0 \ \Leftrightarrow \ x = 0 \\
		ii) \qquad & |x+y| \leq |x|+|y| \qquad \forall x,y\in \mathds R^n \\
		iii) \qquad & |ax| =|a|\, |x| \qquad \forall x\in \mathds R^n,a\in \mathds R 
	\end{align*}
	Matching this definition we can define different kinds of norms:
	\begin{itemize}
		\item the \textit{one norm} $|\cdot|_1$ defined as
		\begin{equation}
			|x|_1 = \sum_{i=1}^{n} |x_i|
		\end{equation}
		\item the \textit{two norm} $|\cdot|_2$ (euclidean norm) defined as
		\begin{equation} \label{eq:lin:euclideannorm}
			|x|_2 = \sqrt{ \sum_{i=1}^n |x_i|^2 }
		\end{equation}
		\item the \textit{infinity norm} $|\cdot|_\infty$ defined as
		\begin{equation}
			|x|_\infty = \max_{i=1,\dots, n} |x_i|
		\end{equation}
		\item this previous concepts can be generalized by the concept of \textit{p norm} $|\cdot|_p$ defined as
		\begin{equation}
			|x|_p = \sqrt[p]{ \sum_{i=1}^n |x_i|^p}
		\end{equation}
	\end{itemize}	
	All this norms are \de{equivalents}: for any norm $p_1,p_2 \in [1,\infty)$ there exists two constants $c_1,c_2\in \mathds R$ such that
	\[ c_1 |x|_{p_1} \leq |x|_{p_2} \leq c_2 |x|_{p_1} \hspace{1cm} \forall x \in \mathds R^n \]
 
	\paragraph{Norms for matrix} The definition of norm can be extended also in case of matrices; in this case the 3 property of the norm must be respected but given a matrix $A \in \mathds R^{m\times n}$ with $m$ rows and $n$ columns we can compute its norm $\| A\|$ in different ways:
	\begin{itemize}
		\item \textit{one norm} $\|\cdot\|_1$ defined as
		\begin{equation}
			\|A\|_1 = \max_{j=1,\dots, n} \sum_{i=1}^m |a_{ij}|
		\end{equation}
		If we consider the matrix as the \textit{concatenation} of $n$ column vectors $v_i \in \mathds R^m$, then this norm can be regarded as the maximum one norm $|v_i|_1$ among such vectors.
		
		\item \textit{infinity norm} $\|\cdot\|_\infty$ defined as
		\begin{equation}
			\|A\|_\infty = \max_{i = 1,\dots,m} \sum_{j=1}^n |a_{ij}|
		\end{equation}
		\item \textit{two norm} $\|\cdot\|_2$ defined as
		\begin{equation}
			\|A\|_2 = \sigma_{\max}\{ A \} = \sqrt{\lambda_{max} \{ A^T A \}  }
		\end{equation}
		We denote with $\sigma\{A\}$ the \textbf{singular values} of the matrix $A$ and are computed as the square root of the eigenvalues of the square matrix $A^T A \in \mathds R^{n\times n}$; in particular the two norm corresponds to the maximum singular value of the matrix $A$. Considering that a vector can be regarded as a matrix $\mathds R^{m\times 1}$ this definitions coincide with the euclidean norm for vectors (equation \ref{eq:lin:euclideannorm}).
		
		\item \textit{Frobenius norm} $\|\cdot\|_F$ defined as
		\begin{equation}
			\|A\|_F = \sqrt{ \sum_{i=0}^m \sum_{j=0}^n a_{ij}^2 } = \sqrt{ \sum_{i=0}^n \sigma_i^2\{A\} }
		\end{equation} 
		Also in this case if we consider a vector as a matrix $\mathds R^{m\times 1}$ the computation of the Frobenius norm coincides with the euclidean/two norm (equation \ref{eq:lin:euclideannorm}) but in general for arbitrarily large matrices $\|A\|_2 \leq \|A\|_F$.
	\end{itemize}
	
	\begin{example}{: two vs Frobenius norm}
		Considering the matrix
		\[ A = \mat{ 2 & 0 \\ 0 & 1} \] 
		in order to determine both two and Frobenius norm we have to compute the singular values of the matrix, and to do so we have to compute the matrix $A^TA$ that results in
		\[ A^TA = \mat{4 & 0 \\ 0 & 1} \]
		Being this matrix diagonal, the two eigenvalues are $\lambda_1 = 4$ and $\lambda_2 = 1$ that, squared, results in the singular values $\sigma_1 = 2$, $\sigma_2 = 1$. We can so compute the two norms as
		\[ \| A\|_2 = \sigma_{max} = \sigma_1 = 2 \qquad \neq \qquad \|A\|_F = \sqrt{\sigma_1^2 + \sigma_2^2} = \sqrt{4 + 1} = \sqrt 5 \]
		We just showed with an example that in general Frobenius and two norms doesn't usually coincides for matrices of arbitrary dimensions.
	\end{example}
		
	Also matrix norms are \textbf{equivalent}, meaning that for any norm $p_1,p_2 \in \{ 1,2,F,\infty\}$ there exists two constants $c_1,c_2\in \mathds R$ such that
	\[ c_1 \|A\|_{p_1} \leq \|A\|_{p_2} \leq c_2 \|A\|_{p_1} \qquad \forall A \in \mathds V \]
	Matrix norms are also \textbf{sub-multiplicative}, meaning that
	\[ \|A B\|_p \leq \|A\|_p \|B\|_p \qquad \forall A,B\in \mathds V,\ \forall p \in \{1,2,F,\infty\} \]
	
	\paragraph{Induced norm} Given a matrix $A \in \mathds R^{m\times n}$ and a vector $x \in \mathds R^{n}$, the product $A x$ results in a $\mathds R^n$ vector on which we can compute the norm. Considering so the sub-multiplicative property we have that
	\[ | A x|_p \leq \|A\|_p |x|_p \qquad \Rightarrow \qquad \|A\|_p \geq \frac{|A x|_p}{|x|_p} \]
	For $p\in \{ 1,2,\infty\}$ we have that the associated norms are \textbf{subordinate}, meaning that we can compute the norm $\|A\|_p$ as
	\begin{equation}
		\|A\|_p = \max_{x\neq0} \frac{|Ax|_p}{|x|_p}
	\end{equation}
	Note that the Frobenius norm isn't subordinate, hence this relation can't be applied.
	
	\subsection*{Lyapunov stability for linear system}
	Given a linear system (both discrete/continuous-time, time-varying/invariant) described as
	\[ \begin{cases}
		\delta x = \A(t) x + \B(t) u \qquad x(t_0) = x_0 \\ y= \C(t) x + \D(t) u
	\end{cases} \]
	we say that
	\begin{itemize}
		\item is \de{Lyapunov} (or \textit{marginally}) \de{stable} if for each initial state $x_0$ there exists a constant $M\in \mathds R$ (that depends of course on $x_0$) such that the zero input response satisfies
		\[ |x(t)| \leq M \qquad \forall t\geq t_0,t_0 \geq 0 \]
		\item is (Lyapunov) \de{asymptotically stable} if it's Lyapunov stable and all solutions satisfy
		\[ \lim_{t\rightarrow \infty} |x(t)|=0 \qquad \forall x_0 \]
		\item is (Lyapunov) \de{exponentially stable} if it's asymptotically stable and exists constants $c>0, \lambda > 0, \mu \in [0,1)$ such that
		\begin{align*}
			& |x(t)| \leq ce^{-\lambda(t-t_0)} |x_0| \qquad \forall x_0,t_0 &&  \textrm{for continuous-time systems} \\
			& |x(t)| \leq c\mu^{t-t_0} |x_0| \qquad \forall x_0,t_0 &&  \textrm{for discrete-time systems} 
		\end{align*}
		\item a system is \de{unstable} if it's not Lyapunov stable.
	\end{itemize}
	We can see that this conditions are always more stringent, hence we can say that
	\[ \textrm{exponential stability} \quad \Rightarrow \quad \textrm{asymptotic stability} \quad \Rightarrow \quad \textrm{Lyapunov stability} \]
	From what concerns Lyapunov, stability is analysed regarding the homogeneous (zero input) response and, as was previously described, the solution can be described using the state transition matrix $\Phi$, hence we can consider:
	\[ |x(t)| = |\Phi(t,t_0)x_0| \leq \|\Phi(t,t_0)\|\, |x_0| \qquad \forall x_0 \]
	From what concerns the Lyapunov stability this means that the norm of the state transition matrix must be bounded, mathematically $\|\Phi(t,t_0)\| \leq M_\Phi$; the same idea holds for both the asymptotic ($\lim_{t\rightarrow \infty} \|\Phi(t,t_0) = 0$) and exponential stability, where $\|\Phi(t,t_0)\| \leq c e^{-\lambda(t-t_0)}$ (for continuous-time systems, but the same can be transliterated for discrete-time ones).
	
	We indeed see that \textbf{Lyapunov stability} only depends on the state transition matrix $\Phi(t,t_0)$, and not the initial condition $x_0$, hence is an \textbf{intrinsic property of the system} independently on the inputs and can be ensured by the analysis of the behaviour of the state transition matrix.
	
	\subsubsection{Linear time invariant systems}
		Considering continuous-time time-invariant linear systems, we proved that the state transition matrix $\Phi(t,t_0)$ can be computed as the matrix exponential $e^{\A(t-t_0)}$ and, as was described, we showed that such matrix converged if $\Re{\lambda_i} < 0 \forall i$. In particular for continuous-time LTI systems we have 
		\begin{align*}
			\textrm{asymptotic stability} \quad & \Leftrightarrow \quad \Re{ \lambda_i\{\A\} } < 0 \ \forall i \\
			\textrm{exponential stability} \quad & \Leftrightarrow \quad \Re{ \lambda_i\{\A\} } < 0 \ \forall i \\
			\textrm{Lyapunov stability} \quad & \Leftrightarrow \quad \Re{ \lambda_i\{\A\} } \leq 0 \ \forall i \textrm{ and if } \Re{\lambda_j} = 0 \ \Rightarrow \ n_j = 0
		\end{align*} 
		We see so that for linear time-invariant systems asymptotic and exponential stability are \textit{coincident} concepts, meaning that they both happen at the same time.\\
		Considering instead discrete-time LTI systems the state transition matrix $\Phi(t,t_0)$ is computed as the power $\A^{t-t_0}$ and, as was previously discussed, we can conclude that
		\begin{align*}
			\textrm{asymptotic stability} \quad & \Leftrightarrow \quad |\lambda_i\{\A\} < 1 \ \forall i \\
			\textrm{exponential stability} \quad & \Leftrightarrow \quad |\lambda_i\{\A\} < 1 \ \forall i \\
			\textrm{Lyapunov stability} \quad & \Leftrightarrow \quad |\lambda_i\{\A\} \leq 1 \ \forall i \textrm{ and if } |\lambda_j| = 1 \ \Rightarrow \ n_j = 0
		\end{align*} 
	
	\subsection{Conditions for exponential stability}
		\paragraph{Positive and negative definite matrices} In order to make assertions regarding the exponential stability of system, a recall of definition from linear algebra is required. Given a square matrix $Q\in \mathds R^{n\times n}$ \textbf{symmetric} (so such that $Q=Q^T$), we say that it is
		\begin{itemize}
			\item \de{positive definite}, noted as $Q>0$, if  $x^TQx > 0$ $\forall x\neq 0$;
			\item \de{negative definite} ($Q<0$) if  $x^TQx < 0$ $\forall x\neq 0$;
			\item \textbf{semi-positive definite} ($Q\geq0$) if  $x^TQx \geq 0$ $\forall x \in \mathds R^n$;
			\item \textbf{semi-negative definite} ($Q\leq0$) if  $x^TQx \leq 0$ $\forall x \in \mathds R^n$.
		\end{itemize}
		\begin{note}
			This definition is set for symmetric matrices $Q$, however in the more general case a matrix $Q\in \mathds R^{n\times n}$ is definite regarding only is symmetric part. Recalling the property for matrices calculations $(ABC)^T = C^TB^TA^T$ and observing that $x^T Q x$ is a scalar hence it's symmetric, we can see that
			\[ x^TQx = \frac 1 2 \left( x^TQx + (x^TQx)^T \right) = \frac 1 2 \left( x^T Qx + x^T Q^t x \right) = x^T \frac{Q+ Q^T}{2} x = x^T Q_{symm} x \]
			where $Q_{symm} = \frac{Q+Q^T}{2}$ is the symmetrical part of $Q$; the matrix $Q$ can in fact be consider as the sum $Q_{symm} + Q_{skew}$ where $Q_{skew} = \frac{Q-Q^T}2$ is the skew-symmetric part of $Q$ that results always in a zero-definite part.
		\end{note}
	
		As a convention for any symmetric matrices $Q_1,Q_2$, with the notation $Q_1 > Q_2$ we imply that $Q_1-Q_2 >0$ or $Q_2-Q_1 < 0$. A results from linear algebra for any symmetric matrix $Q= Q^T\in \mathds R^{n\times n}$ the following statements are equivalents:
		\begin{itemize}
			\item $Q$ is positive definite;
			\item all eigenvalues of $Q$ are positive and real evaluated;
			\item the determinant of all the principle minors (the upper-left sub-matrices) are all positive definite. This is referred as the Sylvester criterion;
			\item there exists a non-singular matrix $H\in \mathds R^{n\times n}$ such that $Q= H^TH$ as for the Cholesky upper-triangular factorization. This also implied that exists the root $K=\sqrt Q$, so $\exists K$ such that $Q = K K$.
		\end{itemize}
		As a consequence for \textbf{quadratic functions} (in the form $x^TQx$) it holds
		\begin{equation} \label{eq:lin:sandwich}
			\lambda_{min}\{ Q \} |x|^2_2 \leq x^T Q x \leq \lambda_{max} \{Q\} |x|_2^2 \qquad \forall x \in \mathds R^n
		\end{equation}
		
		\paragraph{Theorem} For a continuous-time LTI system in the form $\dot x =\A x$ (hence only the zero input part) the following statements are equivalent:
		\begin{enumerate}[\itshape (i)]
			\item it is asymptotically stable;
			\item it is exponentially stable;
			\item the matrix $\A$ is \textit{Hurwitz}, meaning that all the eigenvalues have negative real part ($\Re{\lambda_i(A) } < 0$ $\forall i$);
			\item for each symmetric positive definite matrix $Q=Q^T>0 \in \mathds R^{n\times n}$ there exists a symmetric matrix $P\in \mathds R^{n\times n}$ such that $P^T = P > 0$ and that satisfy the \de{Lyapunov equality}
			\begin{equation} \label{eq:lin:lyapunovequality}
				\A^T P + P\A = -Q 
			\end{equation}
			Moreover (as an extra fact), such matrix $P$ is also unique;
			\item there exists a symmetric matrix $P = P^T > 0 \in \mathds R^{n\times n}$ that satisfies the \de{Lyapunov inequality}
			\[ \A^T P + P\A < 0 \]
		\end{enumerate}
		From a computational point of view the last statement \textit{(v)}, associated to the so called \textbf{\textit{linear matrix inequality}}, somehow certifies the exponential stability. Lyapunov equality \textit{(iv)} is a stronger definition but more complex to compute and the inequality is usually preferred to assert the internal stability of the system.
		
		In general the implications \textit{(i)} $\Leftrightarrow$ \textit{(ii)} $\Leftrightarrow$ \textit{(iii)} were already previously discussed; relating \textit{(iv)} $\Rightarrow$ \textit{(v)} is somewhat trivial (while the reverse operation is mathematically impossible and requires to pass through \textit{(i)} of \textit{(ii)}), but in the next paragraph an analytical proof connecting \textit{(ii)} $\Rightarrow$ \textit{(iv)} and \textit{(v)} $\Rightarrow$ \textit{(ii)} will be presented.
		
		\paragraph{Proof 1: exponential stability implies the Lyapunov equality} We can show that one solution (and we will show that's also unique) of the system is characterized by the matrix $P$ defined as
		\[ P = \int_0^\infty e^{\A^T t} Q e^{\A t}\, dt \]
		This matrix solves the Lyapunov equality and so we have to show that's finite; in order to to so we can compute it's norm obtaining
		\[ \|P\| = \left\| \int_0^\infty e^{\A^T t} Q e^{\A t}\, dt \right\| \leq \int_0^\infty \left\| e^{\A^T t} Q e^{\A t} \right\|\, dt \]
		Using the sub-multiplicative property of the matrix norm we moreover obtain
		\[ \|P\| = \int_0^\infty \left\| e^{\A^T t} \right\| \|Q\| \left\| e^{\A t}\right\|\, dt = \|Q\|\int_0^\infty \|e^{\A t}\|^2\, dt \]
		As a property for any square matrix $A\in \mathds R^{n\times n}$, denoting with $\mu\{A\} = \max_i \Re{\lambda_i(A)}$ it's \textbf{spectral abscissa}, we have that $\forall \lambda > \mu\{A\}$ there exists a constant $k\in \mathds R$ such that $\|e^{At}\| \leq k e^{\lambda t}$ $\forall t>0$. We can so rewrite the norm inequality as
		\[ \|P\| \leq \|Q\| \int_0^\infty k e^{2\lambda t}\, dt \]
		Considering that we started with the assumption that the system is exponentially stable, then it means that all the eigenvalues $\lambda_i$ have a real negative part (hence $\lambda$ in the reported integral is negative) meaning that the integral (and so the norm) converges to a finite number.
		
		We can now show that the so computed matrix $P$ is the solution of the Lyapunov equality: by plugging such definition in equation \ref{eq:lin:lyapunovequality} what we obtain is
		\begin{align*}
			\A^T P + P \A & = \int_0^\infty \A^T e^{\A^T t} Q e^{\A t}\, dt + \int_0^\infty e^{\A^T t} Q e^{\A t} \A \, dt \\
			& = \int_0^\infty \left( \A^T e^{\A^T t} Q e^{\A t} + e^{\A^T t} Q e^{\A t} \A \right)\, dt
		\end{align*}
		Considering that $e^{\A^T t}$ can be regarded as $\left(e^{\A t} \right)^T$, the whole term in the parenthesis can be interpreted as the derivative $\frac d{dt} \left( e^{\A^Tt} Q e^{\A t}\right)$ then the evaluation of the integrals reduces to
		\begin{align*}
			\A^T P + P\A & = \left[  e^{\A^Tt} Q e^{\A t} \right] \Bigg|_{t=0}^\infty = 0 - \I Q \I \\
			& = -Q
		\end{align*}
	
		That concluded the proof that such matrix $P$ is indeed the solution of the Lyapunov equality but we also have to show that $P$ is positive definite; this can be considered computing the product
		\[ z^T P z = \int_0^\infty z^T e^{\A^T t} Q e^{\A t} z\, dt \hspace{2cm} \forall z\in \mathds R^{n}/\{0\}  \]
		Observing that the product $e^{\A t} z$ evaluates always in a non-zero vector $w(t)$ we can interpret the previous expression as
		\[ z^TPz = \int_0^\infty w^T(t) Q  w(t)\, dt \geq \lambda_{min}\{Q\} \int_0^1 |w(t)|^2 \, dt > 0 \]
		where the inequality has been obtained considering that $w^T Q w(t)$ is a quadratic function and hence we can apply the \textit{sandwich inequality} (equation \ref{eq:lin:sandwich}).
		
		Lastly we can show that the solution $P$ is unique; if we assume that $P_1,P_2$ are two solution of the Lyapunov equation this means that they both satisfy
		\[ \begin{cases}
			\A^T P_1 + P_1 \A = - Q \\ \A^T P_2 + P_2 \A = -Q
		\end{cases} \]
		Subtracting the second equation to the first evaluates to $\A^T(P_1-P_2) + (P_1-P_2) \A= 0$; pre-multiplying by a factor $e^{\A^T t} \neq 0$ and post-multiplying by $e^{\A t} \neq 0$ both terms we return to the case
		\begin{align*}
			e^{\A^T t} \A^T(P_1-P_2)e^{\A t} +	e^{\A^T t} (P_1-P_2) \A e^{\A t}  & = 0 \\
			\A^T e^{\A^T t} (P_1-P_2)e^{\A t} +	e^{\A^T t} (P_1-P_2) \A e^{\A t}  & = \\
			\frac{d}{dt} \left( e^{\A^T t} (P_1-P_2) e^{\A t} \right) & = 
		\end{align*}
		This means that the product $e^{\A^Tt}(P_1-P_2) e^{\A t}$ is constant; knowing that for $t\rightarrow \infty$ the exponential $e^{\A t}$ evaluates to zero, in order to ensure that the term remains constant we need to have $P_1 - P_2 = 0$, hence the two solutions $P_1,P_2$ must coincide.
		
		\paragraph{Proof 2: Lyapunov inequality implies exponential stability} To show that a system withstand the Lyapunov inequality (so $\A^T P + T \A < $ and $P>0$) we have to use the \de{Lyapunov function} $\lyap(x)$ defined as
		\begin{equation}
			\lyap (x) = x^T P x \quad \in \mathds R \qquad \forall x\in \mathds R^n
		\end{equation}
		Observing that the result is a scalar, we can apply the \textit{sandwich} relation (equation \ref{eq:lin:sandwich}) to such formulation hence
		\[ \lambda_{min}\{P\} |x|^2 \leq x^T P x \leq \lambda_{max}\{P\} |x|^2 \]
		rewritten as
		\[ c_1 |x|^2 \leq \lyap(x) \leq c_2 |x|^2 \qquad \forall x \in \mathds R^n \]
		If we now take the directional derivative of the Lyapunov function with direction $x$ what we obtain is
		\[ \frac{d\lyap (x)}{dt} = \frac{\partial \lyap}{\partial x} \frac{dx(t)}{dt} = \frac{\partial \lyap}{\partial x} \dot x = \frac{\partial \lyap}{\partial x} \A x = \nabla \lyap^T(x)  \A x = \left\langle \nabla \lyap(x), \A x \right\rangle \]
		it's shown that the gradient $\nabla V = \nabla x^TPx$ is equals to $2P x$ and hence we can say
		\begin{align*}
			\left\langle \nabla \lyap (x), \A x \right\rangle & = 2 P_x \A x = x^T P \A x + x^T P \A x = x^T P \A x + x^T \A^T P^T x \\
			&= x^T (P\A + \A^T P) x = -x^T \Sigma x
		\end{align*}
		Knowing that for \textit{(v)} the matrix $\A^T P + P \A$ is negative definite for the Lyapunov inequality, then it means that the matrix $\Sigma= - (P\A + \A^T P)$ is positive definite. Having from the \textit{sandwich} inequality that $\lambda_{min}\{\Sigma\} |x|^2 \leq x^T\Sigma x$ then it's also true that $- \lambda_{min}\{\Sigma\} |x|^2 \geq - x^T\Sigma x$: we can finally state that
		\[ \left\langle \nabla \lyap(x), \A x \right\rangle \leq -\lambda_{min}\{\Sigma\} |x|^2 \leq -\lambda_{min}\{\Sigma\} \frac 1 {c_2} \lyap(x) =- c_3 \lyap^2(x) \]
		where this relationships are commonly referred as \de{\textit{flow inequalities}}. In order to complete the proof we have to use the \textbf{comparison theorem} stating {\itshape given a scalar function $v(t)$ that's differentiable and such that $\dot v(t) \leq -\mu v(t)$ for all $t\geq t_0$ where $\mu \in \mathds R$ is a constant, then it happens that $v(t) \leq e^{-\mu(t-t_0)} v(t_0)$ for all $t\geq t_0$.} We can so state that for each solution $x(t)$ (for $t\geq t_0$) we can define the scalar function $v(t) = \lyap\big(x(t)\big)$ what we obtain from the flow inequality is that
		\[ \dot v(t) = \big\langle \nabla \lyap\big(x(t)\big), \dot x(t) \big\rangle = \big\langle \nabla \lyap\big(x(t)\big), \A x(t) \big\rangle \leq - c_3 \lyap\big(x(t)\big)  \]
		Applying the comparison theorem to such result we can express
		\[ \lyap\big(x(t)\big) = v(t)\leq e^{-c_3(t-t_0)} v(t_0) = e^{-c_3(t-t_0)} \lyap \big(x(t_0)\big) \]
		From the initial \textit{sandwich} of the proof we furthermore obtain
		\[ c_1 |x(t)|^2 \leq \lyap\big(x(t)\big) \leq e^{-c_3 t} \lyap\big(x(t_0)\big) \leq e^{-c_3( t- t_0)} c_2 |x(t_0)|^2 \]
		We can so prove the exponential decay of the solution by making explicit the inequality respect to it's norm:
		\[ |x(t)|^2 \leq \frac{c_2}{c_1} e^{-c_3(t-t_0)} |x(t_0)|^2 \qquad \Rightarrow \qquad |x(t)| \leq \sqrt{\frac{c_2}{c_1}} e^{-\frac {c_3}2 (t-t_0)} |x(t_0)| \]
		
		\paragraph{Theorem for discrete-time system} For discrete-time LTI systems in the form $x^+ = \A x$ we can express similar equivalent statements as for the continuous-time one:
		\begin{enumerate}[\itshape (i)]
			\item it is asymptotically stable;
			\item it is exponentially stable;
			\item all the eigenvalues of $\A$ have a magnitude strictly lower then one ($|\lambda_i\{\A\}|\leq 1$ $\forall i$);
			\item for each symmetric positive definite matrix $Q=Q^T>0 \in \mathds R^{n\times n}$ there exists a symmetric matrix $P\in \mathds R^{n\times n}$ such that $P^T A = PA > 0$ and that satisfy the \de{Lyapunov equality}
			\[\A^T P \A - P = -Q\]
			\item there exists a symmetric matrix $P = P^T > 0 \in \mathds R^{n\times n}$ that satisfies the \de{Lyapunov inequality}
			\[ \A^T P \A - P < 0 \]
			
		\end{enumerate}
	

\section{Linear quadratic regulator}
	Assuming a continuous-time LTI system where all the staates can be measured, the best control system that we can use it the \de{linear quadratic regulator} that it's based on the minimization of the functional
	\begin{equation}
		\J := \int_0^\infty y^T(\tau) Q y(t) + u^T(\tau) R u(\tau)\, d\tau
	\end{equation}
	where $Q,R$ are two symmetric positive definite matrices. In particular the linear quadratic regular is based on the solution of the following optimal control problem known as the \textbf{\textit{Bayes Rule}}:
	\begin{equation}
		\J^* = \min_{u(t), \, t\geq 0} \J = \min_{u(t), \, t\geq 0} \int_0^\infty y^T(\tau) Q y(t) + u^T(\tau) R u(\tau)\, d\tau
	\end{equation}
	The matrices $Q,R$ are used respectively to evaluate (and penalize) the \textit{output} and \textit{control signal energy} of the system. \textbf{DA RICHIEDERE BENE QUESTA PARTE IL PERCHE}
	
	With the problem as here state, we can (partially) prove that the solution of the optimal control problem is characterized by the input
	\begin{equation} \label{eq:lin:lqrsol}
		u^*(t) = - K x(t)
	\end{equation}
	where $K$ is a matrix that has to be \textit{tuned} according to the chosen matrices $Q,R$ (but also on $\A,\B,\C,\D$ characterizing the system); note that the solution implies that all the states can be measured and used as input in the system.
		
	\paragraph{Feedback invariant} In order to prove that equation \ref{eq:lin:lqrsol} is indeed the argument minimizing the cost functional $\J$ we have to introduce the functional
	\begin{equation} \label{eq:lin:feedbackinvariant}
		\H\big(x(\cdot), u(\cdot)\big) := -\int_0^\infty \Big(\A x(t) + \B u(t)\Big)^T P x(t) + x^T(t) P \Big( \A x(t) + \B u(t)\Big)\, dt
	\end{equation}
	that is a \textbf{feedback invariant} property of the system (and depends only on the initial state) as long it holds that $\lim_{t\rightarrow \infty}(x) = 0$.
	
	We can prove that such functional is invariant substituting $\A x + \B u = \dot x$	and showing that
	\begin{align*}
		\H\big(x(\cdot), u(\cdot)\big) & = - \int_0^\infty \dot x^T P x +x^T P \dot x \, dt = - \int_0^\infty \frac d{dt} \left( x^T P x \right)\, dt = - \Big[ x^T(t) P x(t) \Big]_0^\infty \\ 
		&  = -\cancel{x^T(\infty) P x(\infty)} + x^T(0) P x(0)  = x_0^T P x_0
	\end{align*}
	where the quadratic term $x^T(\infty) P x(\infty)$ has been cancelled due to the preliminary assumption that $x(\infty) = 0$.
	
	\paragraph{Proof} To prove that the solution $u^*(t) = -K x(t)$ is indeed the solution of the optimal control problem, we consider a system whose output depends only on the system (hence $\D$ is identically null for simplicity in calculation). Assuming that the functional $\J$ can be rewritten as
	\[ \J = \H\big(x(\cdot), u(\cdot)\big) + \int_0^\infty \Lambda\big(x(t),u(t)\big) \, dt \]
	where $\Lambda$ is a function characterized by having
	\[ \min_u \Lambda\big(x,u\big) = 0 \qquad \forall x \]	
	To show that this rewrite of the functional $\J$ is equal to the original statement we can add and subtract from the original definition the functional $\H$:
	\[ \J = \int_0^\infty y^T(\tau) Q y(t) + u^T(\tau) R u(\tau)\, d\tau + \H(x,u) - \H(x,u) \]
	Taking in the integral the expansion of $-\H$ and knowing that $y = \C x$ (that transposed evaluates to $(\C x)^T = x^T\C^T$) then we can rewrite
	\begin{align*}
		\J & = \int_0^\infty x^T\C^T Q \C x + u^TRu + \big( \A x + \B u \big)^T P x + x^T P\big(\A x+ \B u\big) \, d\tau + \H(x,u) \\
		& = \int_0^\infty x^T\big(\C^T Q \C + \A^T P + PA \big) x + 2 x^T P \B u + u^T R u\, dt + \H(x,u)
	\end{align*}
	Knowing that the expected solution is in the form $ u + K x$, performing it's quadratic form respect to $R$ evaluates to
	\[ \Lambda(x,u) = \big(u^T + x^T K^T\big) R\big(u + Kx\big) = u^T R u + x^TK^T R K x + 2 x^T K^T R u \]
	Matching the \textit{mixed} quadratic form involving $x^T\cdot u$ in both $\J$ and $\Lambda$ allows to compute the value of the matrix $K$ as
	\[ P\B = K^T R \qquad \Rightarrow \quad K^T = P\B R^{-1} \qquad \Rightarrow \quad K = \big(P\B R^{-1}\big)^T = \big(R^{-1}\big)^T \B^T P^T = R^{-1} \B^T P \]
	With such choice we can rewrite the functional as
	\begin{align*}
		\J & = \H(x,u) + \int_0^\infty x^T\big(\C^T Q \C + \A^T P + PA \big) x + 2 x^T P \B u + u^T R u\, dt \\
		& = \H(x,u) + \int_0^\infty x^T\big(\C^T Q \C + \A^T P + PA + P \B R^{-1} \B^TP - P \B R^{-1} \B^TP \big) x + 2 x^T P \B u + u^T R u\, dt
	\end{align*}
	We can so see that $x^T P \B R^{-1} \B^TP x$ matches the expansion of $\Lambda$ evaluated at $x^T K^T R K x$. With that in order to have a functional $\J$ in the form $\H(x,u) + \int_0^\infty \Lambda(x,u)\, dt$ we have to impose the condition on the quadratic term $x^T\cdot x$ determining the  \textbf{algebraic} \de{Riccati equation}:
	\begin{equation}
		\A^T P + P \A + \C^T Q \C - P \B R^{-1} \B^T P = 0
	\end{equation}
	The problem now is finding a symmetric positive definite matrix $P$ that satisfies such equation and if we ensure that $\lim_{t\rightarrow\infty}x(t) = 0$ then the solution $u^* = - Kx$ is the solution of the linear quadratic regulator; it also happens that the optimal value $\J^+$ evaluates to $H(x,u) = x_0^T P x_0$.		
	
	\textbf{RICONTROLLARE}
	
\section{Controllability}	
	The main idea behind the \de{\textit{controllability}} is to determine what the inputs $u$ can \textit{accomplish} in the output depending on the initial states. Recalling that the whole response of a continuous-time linear time-varying system is made by a free and a forced term in the form
	\[ x(t_1) = \Phi(t_1,t_0) x(t_0) + \int_{t_0}^{t_1} \Phi(t_1,\tau) \B (\tau) u(\tau) \, d\tau \]
	
	In particular given two times $t_1 > t_0 \geq 0$, we define with $\mathscr R[t_0,t_1]$ the \de{reachable set} (that's indeed a linear subspace) defined as
	\begin{equation}
		\mathscr R[t_1,t_2] = \left\{ x_1 \in \mathds R^n \ : \ \exists u \textrm{ such that } \int_{t_0}^{t_1} \Phi(t_1,\tau) \B(\tau) u(\tau) \, d\tau = x_1 \right\}
	\end{equation}
	The idea of the reachable set is to know which states the system can reach in the interval $[t_0,t_1]$ starting from a zero initial state $x(t_0) = 0$. Conversely the \de{controllable set} (that's a linear subspace) $\mathscr C[t_0,t_1]$ as the set of all the initial states $x_0 = x(t_0)$ resulting in a zero final state $x(t_1) = 0$:
	\begin{equation}
	\begin{aligned}
		\mathscr C[t_1,t_2] & = \left\{ x_0 \in \mathds R^n \ : \ \exists u \textrm{ such that } \int_{t_0}^{t_1} \Phi(t_0,\tau) \B(\tau) \big(-u(\tau)\big) \, d\tau = x_1 \right\} \\
		& = \left\{ x_0 \in \mathds R^n \ : \ \exists u \textrm{ such that } \int_{t_0}^{t_1} \Phi(t_0,\tau) \B(\tau) v(\tau) \, d\tau = x_1 \right\}
	\end{aligned}
	\end{equation}
		
	Such integral (slightly different from the reachable set) has been obtained pre-multiplying the response by the inverse $\Phi(t_1,t_0)^{-1} = \Phi(t_0,t_1)$ what we obtain is
	\begin{align*}
		x_1 = 0 & =  \cancel{\Phi(t_0,t_1) \Phi(t_1,t_0)} x_0 + \int_{t_0}^{t_1} \Phi(t_0,t_1) \Phi(t_1,\tau) \B (\tau) u(\tau) \, d\tau = x_0 + \int_{t_0}^{t_1} \Phi(t_0,\tau) \B(\tau) u(\tau)\, d\tau \\
		-x_0 & = \int_{t_0}^{t_1} \Phi(t_0,\tau) \B(\tau) u(\tau)\, d\tau
	\end{align*}
	
	\paragraph{Linear algebra recall} In order to ease the following definition of \textit{Gramians} a recall on concept of linear algebra is required. Given a matrix $W\in \mathds R^{m\times n}$ we define as \textit{range} or, preferably, \textbf{image} of $W$ the set
	\[ \textrm{Im} W = \left\{ y \in \mathds R^m\, : \  y = W x \textrm{ with } x\in \mathds R^n \right\} \subseteq \mathds R^m \]
	The dimension of the image of $W$ coincides with its \textbf{rank}, meaning the number of independent columns (and also rows) composing the matrix. The \textbf{kernel} (also referred as \textit{null space}) of the matrix is defined by the set
	\[ \ker W = \left\{ x \in \mathds R^n \ : \ W x = 0 \right\} \subseteq \mathds R^n\]
	The dimension $\dim\{\ker W\}$ is sometimes referred as \textit{nullity}.
	
	An important theorem that we have to always keep in mind is the \textbf{fundamental theorem of linear algebra} stating that for any matrix $W \in \mathds R^{m\times n}$ we have that
	\[ \dim\{ \textrm{Im} W \} + \dim\{\ker W \} = n \]
	Given a linear space $\mathcal V \subseteq \mathds R^n$ we can determine its \textbf{orthogonal complement} $\mathcal V^\perp$ the set defines as
	\[ \mathcal V^\perp = \left\{ x \in \mathds R^n \textrm{ such that } x^T z = 0 \ \forall z \in \mathcal V \right\} \]
	
	\paragraph{Singular value decomposition} The \de{singular value decomposition} \textbf{SVD} is a particular transformation of any matrix $W \in \mathds R^{m\times n}$ that's defined as
	\begin{equation}
		W = U S V^T
	\end{equation}
	where $U,V$ are special orthogonal matrices (so such that $U^TU = UU^T = \I$ and so the vectors composing such matrices determines an orthonormal base of the space they are representing) and $S$ is a matrix containing diagonally the singular values $\sigma_i = \sqrt{\lambda_i\{W^TW\}}$ of the matrix $W$:
	\[ P = \begin{bmatrix} \sigma_1 \\ & \ddots \\ & & \sigma_p \\ &&& 0 \\ &&&& \ddots \\ &&&&& 0 \end{bmatrix} \]
	\textbf{MANCANO DA SCRIVERE DELLE CONSIDERAZIONI}
	
	\paragraph{Gramians} Given two times $t_1 > t_0 \geq 0$, the \de{Gramian} \textbf{reachability} and \textbf{controllability} of a continuous-time linear time-varying system are described by the symmetric matrices
	\begin{equation} \label{eq:lin:gramians}
	\begin{aligned}
		W_\reach[t_0,t_1] & = \int_{t_0}^{t_1} \Phi(t_1,\tau) \B(\tau) \B^T(\tau) \Phi^T(t_1,\tau) \, d\tau \qquad \in \mathds R^{n\times n} \\
		W_\control[t_0,t_1] & = \int_{t_0}^{t_1} \Phi(t_0,\tau) \B(\tau) \B^T(\tau) \Phi^T(t_0,\tau) \, d\tau \qquad \in \mathds R^{n\times n} 
	\end{aligned}
	\end{equation}
	We in fact relate such matrices respectively with the reachable and controllable set as
	\begin{equation} \label{eq:lin:gramiam}
		\reach[t_0,t_1] = \textrm{Im} W_\reach[t_0,t_1] \hspace{2cm}
		\control[t_0,t_1] = \textrm{Im} W_\control[t_0,t_1]
	\end{equation}
	Moreover denoting with $x_1 = W_\reach[t_0,t_1] \eta_1$ the resulting reachable state (similarly for controllability $x_0 = W_\control[t_0,t_1] \eta_0$) we have that the optimal input $u(t)$ that gets to such value of the states is the one determined by
	\begin{equation} \label{eq:lin:gramoptimalsolution}
		u^*(t) = \B^T(t) \Phi^T(t_1,t)\eta_1 \hspace{1.2cm} \forall t \in [t_0,t_1]
	\end{equation}
	Similarly for the controllable set the optimal output is defined as $u^*(t) = - \B^T(t) \Phi^T(t_0,t) \eta_0$.
		
	\paragraph{Proof} To prove the Gramian theorem (associated to equation \ref{eq:lin:gramiam}) we can firstly check that $\textrm{Im} W_\reach \subseteq \reach$ (the proof is made for the reachability of the system, but similar calculations can be performed for the observability) and then observing that $\reach \subseteq \textrm{Im} W_\reach$: this two conditions happening simultaneously indeed implies that $\reach = \textrm{Im} W_\reach$.	
		
	\begin{enumerate}[\itshape i)]
		\item Taking $x_1 \in \textrm{Im} W_\reach$ a state belonging to the image of $W_\reach$, using the optimal control solution presented in equation \ref{eq:lin:gramoptimalsolution} in the variation of constant formula (equation \ref{eq:lin:varconstsants} at page \pageref{eq:lin:varconstsants} where, for the reachability, $x_0$ is assumed to be zero):
		\begin{align*}
			x(t_1) & = \int_{t_0}^{t_1} \Phi(t_1,\tau) \B(\tau) \B^T(\tau) \Phi^T(t_1,\tau) \eta_1 \, d\tau = \int_{t_0}^{t_1} \Phi(t_1,\tau) \B(\tau) \B^T(\tau) \Phi^T(t_1,\tau) \, d\tau \  \eta_1 \\
			& = W_\reach[t_0,t_1] \eta_1 = x_1
		\end{align*}
	
		\item Considering now that for any state $x_1 \in \reach[t_0,t_1]$ there exists at least one control input $u$ such that $x_1 = \int_{t_0}^{t_1} \Phi(t_1,\tau) \B(\tau) u(\tau) \, d\tau$ (still from the variation of constants formula), then we can show that
		\[ x_1 \in \textrm{Im} W_\reach = \big( \ker W_\reach^T \big)^\perp = \big(
		 \ker W_\reach \big)^\perp \]
		For all vectors $\eta_{1k} \in \ker W_\reach$ we can prove that $x_1^T \eta_{1k} = 0$; expanding the the definition of $x(t)$ from the variation of constants we have
		\[ x^T \eta_{1k} = \int_{t_0}^{t_1} \Phi(t_1,\tau) \B(\tau) \B^T(\tau) \Phi^T(t_1,\tau) \eta_{1k} \, d\tau \]
		Since $\eta_{1k}$ belongs to the kernel we have that $( W_\reach \eta_{1k} )^T = \eta_{1k}^T W_\reach$, meaning
		\begin{align*}
			\eta_{1k}^T W_\reach[t_0,t_1] \eta_{1k} & = \int_{t_0}^{t_1} \eta_{1k} \Phi(t_1,\tau)\Phi(t_1,\tau) \B(\tau) \B^T(\tau) \Phi^T(t_1,\tau) \eta_{1k} \, d\tau  \\
			\eta_{1k}^T 0 & = \int_{t_0}^{t_1} \big\| \B^T(\tau) \Phi^T(t_1,\tau) \eta_{1k} \big\|^2 \, d\tau = 0			
		\end{align*}
		Knowing that the element inside the integral is non-negative, in order to have a zero integral it must have $\B^T(\tau) \Phi^T(t_1,\tau) \eta_{1k} = 0$.
	\end{enumerate}
	
	\paragraph{Theorem} Given two times $t_1 > t_0 \geq 0$:
	\begin{itemize}
		\item for each state $x_1 \in \reach[t_0,t_1]$ in the reachable set, the control input $u^*$ described in equation \ref{eq:lin:gramoptimalsolution} transforms the state from $x_0 = 0$ to $x_1 = W_\reach \eta_1$ with the minimum energy
		\[ \int_{t_0}^{t_1} |u(\tau)|^2\, d\tau \]
		Moreover it has that $\min_u \int_{t_0}^{t_1} |u^*(\tau)|^2\, d\tau = \eta_1^T W_\reach \eta_1$.
		\item similarly for every initial state $x_0 \in \control[t_0,t_1]$ the control $u^*(t) = -\B^T(t)\Phi^T(t_0,t)\eta_0$ transforms the initial state $x_0 \neq 0$ to the final state $x_1 = 0$ with the minimum energy and the minimum value is $\eta_0^T W_\control \eta_0$.
	\end{itemize}
	The main idea behind this theorem is that the matrices $W_\reach,W_\control$ can be used to estimate the energy required for \textit{moving the states}.
		
	\paragraph{Proof} We can prove that $u^*$ minimizes the energy required for moving the states from $x_0$ to $x_1 \in \reach[t_0,t_1]$ (similar proof can be written for the controllability); considering $\tilde u(t)$ one other input that allows to \textit{achieve the goal}, so such that $\int_{t_0}^{t_1} \Phi \B \tilde u\, d\tau = x_1$, we can compute the difference between such solution and the optimal one as
	\[ 0 = \int_{t_0}^{t_1} \Phi(t_1,\tau) \B(\tau) \underbrace{\big( u^*(\tau) - \tilde u(\tau) \big)}_{= v(z)} \, d\tau \]
	Writing with $v(z)$ the difference $u^* - \tilde u$ between the optimal input and another valid one. Studying the minimization of the energy of the alternative input $\tilde u$, we have that
	\begin{align*}
		\int_{t_0}^{t_1} |\tilde u(\tau)|^2\, d \tau & = \int_{t_0}^{t_1} \tilde u^T(\tau) \tilde u(\tau)\, d\tau = \int_{t_0}^{t_1}\big(u^*(\tau) - v(\tau)\big)^T\big( u^*(\tau) - v(\tau) \big) \, d\tau \\
		& = \int_{t_0}^{t_1} u^{*^T} u^*\, d\tau + \int_{t_0}^{t_1} v^T v\, d\tau + \int_{t_0}^{t_1} 2 u^{*^T} v\, d\tau
	\end{align*}
	Knowing that the ideal input $u^*$ is in the form $\B^T \Phi^T \eta_1$ (equation \ref{eq:lin:gramoptimalsolution}) we can expand the definition of the energy of the alternative input $\tilde u$ as
	\begin{align*}
		\int_{t_0}^{t_1} |\tilde u|\, d\tau & = \eta_1^T \int_{t_0}^{t_1} \Phi \B \B^T \Phi\, d\tau\ \eta_1 + \int_{t_0}^{t_1} |v|^2\, d\tau - 2 \eta_1^T \cancelto{0}{\int_{t_0}^{t_1} \Phi\B v \, d\tau} \\
		& = \eta_1^T W_\reach \eta_1 + \int_{t_0}^{t_1} |v|^2\, d\tau
	\end{align*}
	The mixed terms has been driven to zero considering the initial condition on the proof; considering that $|v|$ is a non-negative quantity, it means that integrating over time results in an increment on the energy spent and the optimal solution $u^*$ minimizes the energy to the minimum value $\eta_1^T W_\reach \eta_1$.
	
	\subsubsection{Continuous-time linear time-invariant case}
		Considering the case of continuous-time LTI systems the definitions of the Gramian reachability (equation \ref{eq:lin:gramians}) can be simplified: considering in fact that $\Phi(t_1,\tau) = e^{\A(t_1-\tau)}$ we have that
		\begin{equation}
			W_\reach[t_0,t_1] = \int_{t_0}^{t_1} e^{\A(t_1-\tau)} \B \B^T e^{\A^T(t_1-\tau)}\, d\tau \quad \xrightarrow{t_1-\tau = t} \quad \int_0^{t_1-t_0} e^{\A t} \B \B^T e^{\A^T t}\, dt
		\end{equation}
		Similarly for the controllability Gramian we obtain
		\[ W_\control[t_0,t_1] = \int_{t_0}^{t_1} e^{\A(t_0-\tau)} \B \B^T e^{\A^T(t_0-\tau)}\, d\tau \quad \xrightarrow{t_0-\tau = - t} \quad \int_0^{t_1-t_0} e^{-\A t} \B \B^T e^{-\A^T t}\, dt \]	
		To solve the dynamic of such system we can use the so called \de{controllability matrix} $R\in \mathds R^{n\times (kn)}$ defined as the \textit{concatenation} of matrices in the form
		\begin{equation} \label{eq:lin:controllabilitymatrix}
			R = \mat{ \B &\A\B &\A^2\B &\dots & \A^{n-1}\B }
		\end{equation}
		As consequence a theorem states that for any two times $t_1 > t_0 \geq 0$ for a continuous-time linear time-invariant system we have that
		\[ \reach [t_0,t_1] = \textrm{Im} W_\reach[t_0,t_1] = \textrm{Im} R = \textrm{Im} W_\control[t_0,t_1] = \control[t_0,t_1] \]
		meaning that,in this case, the controllability and reachability subspaces are the same, they coincides. As consequent property we can consider the \textit{time reversibility}, meaning that a state is controllable if and only if its reachable, and also the \textit{time scaling}, meaning that reachability/controllability does not depend on the time difference $t_1-t_0$ (in fact there always exists a input $u(t)$ that allows to reach the desired state in a \textit{arbitrarily small time}).
		
		\paragraph{Proof} To prove that controllability and reachability set are coincident, we can prove that $\reach \subseteq \textrm{Im} R$ as a first step and then prove that $\textrm{Im} R \subseteq \textrm{Im} W_\reach$ (considering that is already proven that $\reach = \textrm{Im} W_\reach$); similarly it can be proven that $\textrm{Im} R = \textrm{Im} W_\control = \control$.
		\begin{enumerate}[\itshape i)]
			\item considering a state $x_1 \in \reach[t_0,t_1]$ means that exists an input $u(t)$ such that
			\[ x_1 = \int_{t_0}^{t_1} e^{\A(t_1-\tau)} \B u(\tau)\, d\tau \]
			Using the Cayley-Hamilton theorem (equation \ref{eq:lin:cayley}, page \pageref{eq:lin:cayley}) we can rewrite this expression as
			\[ x_1 = \int_{t_0}^{t_1} \sum_{i=0}^{n-1} \alpha_i(t_1-\tau)\A^i \B u(\tau)\, d\tau = \sum_{i=0}^{n-1} \A^i \B \int_{t_0}^{t_1} \alpha_i\big(t_1-\tau\big) u (\tau)\, d\tau  \]
			This expression can be rewritten as a linear combination of the form
			\[ x_1 = \underbrace{\mat{ \B  & \A\B & \dots & \A^{n-1}\B }}_R \underbrace{\vet{\int_{t_0}^{t_1} \alpha_0(t_1-\tau) u(\tau)\, d\tau \\ \int_{t_0}^{t_1} \alpha_1(t_1-\tau) u(\tau)\, d\tau \\ \vdots \\ \int_{t_0}^{t_1} \alpha_{n-1}(t_1-\tau) u(\tau)\, d\tau  }}_\nu  \]
			Having $\nu$ a vector we indeed have that $x_1$ is in the image set of $R$ because it can be regarded as $R\nu$.
			
			\item it has been previously proven that $\B^T\Phi^T(t_1,\tau) \eta_{1k} = 0$ for any vector $\eta_{1k} \in \ker W_\reach[t_0,t_1]$ and $\forall \tau \in [t_0,t_1]$ in the time-variant case, considering a LTI system we have that
			\[ (a) \qquad \B^T e^{\A^T(t_1-\tau)} \eta_{1k} = 0 \]
			Deriving with respect to time this expression transposed evaluates to $\frac{d}{dt} (a)^T = -\eta_{1k}^T \A e^{\A(t_1-\tau)} \B$ and more generally we can express
			\[ \frac{d^i}{dt^i} (a)^T = (-1)^i \eta_{1k}^T \A^i e^{\A(t_1-\tau)} \B = 0 \]
			Evaluating this derivatives for $\tau = t_1$ results in $\eta_{1k}^T \A^i \B = 0$ $\forall i$; for any state $x_1 = R\nu \in \textrm{Im} W_\reach[t_0,t_1]$ we so have that
			\begin{align*}
				x_1 ^T \eta_{1k} & = \eta_{1k}^T x_1 = \eta_{1k}^T R\nu = \eta_{1k}^T \mat{ \B  & \A\B & \dots & \A^{n-1}\B } \nu \\
				& = \mat{ \cancelto{0}{\eta_{1k}^T \B}  & \cancelto{0}{\eta_{1k}^T \A \B} & \dots & \cancelto{0}{\eta_{1k}^T \A^{n-1} \B} } \nu
			\end{align*}
			This means that $x_1 \perp \eta_{1k}$ for any vector $\eta_{1k} \in \ker W_\reach$, meaning that $x_1$ belongs to the orthogonal subspace $(\ker W_\reach)^\perp$. Knowing that in general $\textrm{Im} W = (\ker W^T)^\perp$, but in this case $W_\reach$ is symmetric we have that $\textrm{Im} W_\reach = (\ker W_\reach)^\perp$ and so we have that $\textrm{Im} R \subset \textrm{Im} W_\reach$.
		\end{enumerate}
		We can so state that in general the reachable/controllable subspaces, for LTI systems, are properties of the system itself and are determined only by the matrices $\A$ and $\B$.
		
	\subsubsection{Discrete-time systems}
		\paragraph{Discrete-time LTV} With the linear continuous-time system analysed (both time-variant and invariant), the same concepts regarding the controllability (reachability) of system can be described also for the discrete-time case. Given the state-space representation $x^+ = \A(t) x + \B(t) u$, from the variation of constant formula we have 
		\[ x(t_1) = \Phi(t_1,t_0) x_0 + \sum_{\tau=t_0}^{t_1-1} \Phi(t,\tau+1) \B(\tau) u(\tau) \]
		Given so 2 times $t_1 > t_0 \geq 0$, we can define the $[t_0,t_1]$-reachable subspace and the $[t_0,t_1]$-controllable one the sets described as
		\begin{align*}
			\reach[t_0,t_1] & = \left\{ x_1 \textrm{ such that } \exists u(t) \textrm{ with } t = t_0,t_0+1,\dots, t_1-1 \ : \ x(t_1) = \sum_{\tau=t_0}^{t_1-1} \Phi(t,\tau+1) \B(\tau) u(\tau) \right\} \\
			\control[t_0,t_1] & = \left\{ x_0 \textrm{ such that } \exists u(t) \textrm{ with } t = t_0,\dots, t_1-1 \ : \ 0 = \Phi(t_1,t_0) x_0 + \sum_{\tau=t_0}^{t_1-1} \Phi(t,\tau+1) \B(\tau) u(\tau) \right\}
		\end{align*}
		Moreover if the matrix $\A$ is non-singular $\forall t \in [t_0,t_1]$, then the state transition matrix $\Phi$ can be inverted and we can re-define the controllable set as
		\[ \control[t_0,t_1] = \left\{ x_0 \textrm{ such that } \exists v(t) = -u(t) \ : \ x_0 = \sum_{\tau = t_0}^{t_1-1} \Phi(t_0,\tau+1) \B(\tau) v(\tau) \right\}  \]
		The \textbf{discrete-time Gramians} of the system are defined as
		\[ W_\reach = \sum_{\tau = t_0}^{t_1 - 1} \Phi(t_1,\tau+1) \B(\tau) \B^T(\tau) \Phi^T(t_1,\tau+1) \qquad W_\control = \sum_{\tau = t_0}^{t_1 - 1} \Phi(t_0,\tau+1) \B(\tau) \B^T(\tau) \Phi^T(t_0,\tau+1) \]
		Observe that the controllable Gramian $W_\control$ is valid only if the matrix $\A$ is non-singular in the domain $[t_0,t_1]$ (as for the second definition of the set itself).
		
		It still holds that $\textrm{Im} W_\reach[t_0,t_1] = \reach[t_0,t_1]$ and $\textrm{Im} W_\control[t_0,t_1] = \control[t_0,t_1]$ for any times $t_1 > t_0 \geq 0$ and, moreover, if $x_1 = W_\reach \eta_1 \in \textrm{Im} W_\reach$ is a state in the reachable set, then the control $u^*(t) = \B^T(t) \Phi^T(t_1,t+1) \eta_1$ moves $x_0 = 0$ to $x_1\neq 0$ with the minimum energy; similarly for the control set if we have $x_0 = W_\control \eta_0$, them the optimal control is $u^*(t) = - B^T(t)\Phi^T(t_0,t+1)\eta_0$.
		
		\paragraph{Discrete-time LTI} Considering a system with state-space representation $x^+ = \A x + \B u$, then if $\A$ is invertible and knowing that $\Phi(t_1,\tau) = \A^{t_1-\tau}$ performing some change of variables we can rewrite the Gramians of discrete-time LTI systems as
		\[ W_\reach = \sum_{s=0}^{t_1-t_0-1} \A^s \B \B^T \big(\A^T\big)^s \hspace{2cm} W_\control = \sum_{s=0}^{t_1-t_0-1} \A^{-s-1} \B \B^T \big(\A^T\big)^{-s-1} \]
		Also in this case we can say that for any $t_1 > t_0 \geq 0$ 
		\[ \reach [t_0,t_1] = \textrm{Im} W_\reach[t_0,t_1] = \textrm{Im} R = \textrm{Im} W_\control[t_0,t_1] = \control[t_0,t_1] \]
		if we assume that $\A$ is invertible (for the controllability property) and $t_1 \geq t_0 +n $ (this is due to the fact that \textit{we cannot scale the input in time as much as we want}).
	
	\subsection{State feedback and single input eigenvalue assignment}
		Given a linear time-invariant system that's so characterized by a state-space response $x^+/\dot x = \A x + \B u$, if we can measure all the states then we can assign as input $u$ a linear combination of them, hence $u = - K x$ (where $K$ is a matrix): this means that the states variations corresponds to a form
		\[ \dot x = \big(\A- \B K\big) x \]
		
		\paragraph{Single input eigenvalue assignment theorem} Considering a controllable (meaning that $\reach = R^n$) single input LTI system characterized by the pair of matrices $\A,\B$, then for each set $\lambda_1,\dots,\lambda_n$ of desired eigenvalues there always exists a matrix $K \in \mathds R^{k\times n}$ such that
		\begin{equation}
			\A_{cl} = \A - \B K \qquad \textrm{has eigenvalues } \lambda_1,\dots, \lambda_n
		\end{equation}
		where $\A_{cl}$ is usually referred as the \de{\textit{closed loop matrix}}. The goal is now to find a \textit{procedure} to determine such matrix $K$ that allows us to determined the desired eigenvalues.
		
		\vspace{3mm}
		Recalling the standard controllable form of a system (seen in page \pageref{sec:canonicalforms}) that's characterized by the matrices
		\[ \A_{ctr} = \mat{\begin{array} {c | c c c} & \\
			0 & & \I \\ & \\  \hline
			-\alpha_0 & -\alpha_1 & \dots & -\alpha_{n-1}				
		\end{array}} \qquad \B_{ctr} = \mat{0 \\ \vdots \\ 0 \\ 1} \]
		where the coefficients $\alpha_i$ are the coefficient of the characteristic polynomial of the original matrix $\A$ in the form
		\[ p_\A(s) = s^n + \alpha_{n-1} s^{n-1} + \dots + \alpha_1 s + \alpha_0 \]
		Having in this case a single input system, it means that the desired matrix $K$ is a row vector in the form $K = \mat{k_0 & \dots & k_{n-1}}$. In order to compute such matrix, we can start off by building the characteristic polynomial associated to the desired eigenvalues of the closed loop matrix in the form
		\[ p_{\A_{cl}}(s) = (s-\lambda_1) \dots  (s - \lambda_2) = s^n + \beta_{n-1} s^{n-1} + \dots + \beta_1 s + \beta_0 \]
		By equating the controllable form of the closed loop matrix with the one obtained by $\A_{ctr} - \B_{ctr}K$ we have that
		\[ \mat{\begin{array} {c | c c c} & \\
				0 & & \I \\ & \\  \hline
				-\beta_0 & -\beta_1 & \dots & -\beta_{n-1}				
		\end{array}} = \mat{\begin{array} {c | c c c} & \\
			0 & & \I \\ & \\  \hline
			-\alpha_0 - k_0 & -\alpha_1 - k_1 & \dots & -\alpha_{n-1} - k_{n-1}				
		\end{array}}  \]
		meaning that the coefficients $k_i$ of the matrix $K$ can be evaluates as $k_i = \beta_i - \alpha_i$.
		
		In the more general case where $\A,\B$ are not in controllable form but are \textit{generic} matrices, we can show that if $R$ is non-singular, then the product $R^{-1}\A R$ evaluates to the matrix $\A_{ob}$ in observable form and $R^{-1} \B = \B_{ob}$ (where $R$ is the controllability matrix seen at page \pageref{eq:lin:controllabilitymatrix}) described as
		\[ \A_{ob} = \mat{\begin{array} {c c c | c} 
			& 0 & & -\alpha_0 \\ \hline
			&&& -\alpha_1 \\ & \I & & \vdots \\ &&& -\alpha_{n-1}				
		\end{array}} \qquad \B_{ob} = \mat{1 \\ 0 \\ \vdots \\ 0} \]
		This relations can be proven considering that
		\begin{align*}
			R \A_{ob} & = \mat{ \B &\A\B &\dots & \A^{n-1}\B } \mat{\begin{array} {c c c | c} 
				& 0 & & -\alpha_0 \\ \hline
				&&& -\alpha_1 \\ & \I & & \vdots \\ &&& -\alpha_{n-1}				
			\end{array}} \\
			& = \mat{ \begin{array}{c c c  | c}
				&&& \\
				\A \B & \dots &  \A^{n-1}\B & x \\ &&&
			\end{array} }
		\end{align*}
		where $x$ is a vector determines as the linear combination $x = -\alpha_0 \B - \alpha_1 \A \B - \dots - \alpha_{n-1} \A^{n-1}\B$ (where note that all the products $\A^k\B$ are column vectors). Collecting $\B$ we have that
		$x = \big(-\alpha_0 \I - \alpha_1 \A  - \dots - \alpha_{n-1} \A^{n-1} \big) \B$ and recalling the Cayley-Hamilton theorem (equation \ref{eq:lin:cayley}, page \pageref{eq:lin:cayley}) we can consider that the term in the parenthesis exactly as $\A^n$, hence
		\[ R \A_{ob} = \mat{ \A \B & \dots &  \A^{n-1}\B & \A^n \B } = \A \mat{ \B &\A\B &\dots & \A^{n-1}\B } = \A R \]
		The proof of $R^{-1}\B=\B_{ob}$ is straightforward and can be obtained considering that $R \B_{ob}$ is indeed $\B$. We can so prove that for each observable form there exists a matrix $M$ such that $M^{-1} \A_{ob} M = \A_{ctr}$ and $M^{-1}\B_{ob} = \B_{ctr}$ that allows to determine the canonical form. In particular $M$ is the symmetric matrix defines as
		\[ M = \mat{ \alpha_1 & \alpha_2 & \dots & \alpha_{n-1} & 1 \\
		\alpha_2 & \reflectbox{$\ddots$} & & \reflectbox{$\ddots$} \\
		\vdots & & \reflectbox{$\ddots$} \\
		\alpha_{n-1} & \reflectbox{$\ddots$}\\
		1 } \]
	
		We can in fact show that
		\[ M \A_{ctr} = \mat{\begin{array} {c c c | c} 
				& \alpha^T & & 1\\ \hline
				&&& 0 \\
				& M_{21} & & \vdots \\
				&&& 0
		\end{array}} \mat{\begin{array} {c | c c c} 0 & \\
			\vdots & & \I \\ 0 & \\  \hline
			-\alpha_0 & & -\alpha^T			
		\end{array}} = \mat{\begin{array} {c | c c c}
			-\alpha_0  & 0 & \dots & 0 \\ \hline
			0 & \\ 
			\vdots & & M_{21} \\
			0 & 
		\end{array}} \]
		where $M_{21}$ is a symmetric submatrix of $M$ and $\alpha$  is the column vector $(\alpha_1,\dots,\alpha_{n-1})$. Observing that the resulting matrix is symmetric we have that
		\[ M \A_{ctr} = \big(M \A_{ctr}\big)^T = \A_{ctr}^T M^T = \A_{ob} M \]
		
		\vspace{3mm}
		With all this proofs we have the ability now to explicit the general procedure that allows to determine the matrix $K$ that determines the closed loop matrix $\A_{cl} = \A -\B K$ whose eigenvalues are the desired $\lambda_1,\dots, \lambda_n$. Choosing in particular $T= RM$ what we have is that
		\[ T^{-1} \A T = \A_{ctr} \qquad \textrm{and} \qquad T^{-1} \B = \B_{ctr} \]
		Determined the controllable forms of both $\A$ and $\B$ we can compute the matrix $K_{ctr}$ by computing the characteristic polynomials $p_{\A_{ctr}}(s)$ and $p_{\A_{cl}}(s)$ and by matching the coefficients we determine the entries of $K_{ctr}$ as $k_i = \beta_i - \alpha_i$. In particular $K$ associated to the initial matrices $\A,\B$ evaluates to
		\[ K = K_{ctr} T^{-1} \]
		
		\paragraph{Ackerman} Another formula used to compute such matrix $K$ in order to assign the eigenvalues to $\A - \B K$ is by applying the \textbf{Ackerman equation}
		\[ K = \mat{0 & \dots & 0 & 1} R^{-1} p_{\A_{cl}} (\A) \]
		
		
		