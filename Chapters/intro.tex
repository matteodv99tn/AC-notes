\chapter{Dynamical systems}

	This course will treat the \de{automatic control} of \de{dynamical systems}, hence it's necessary to firstly understand what a \textit{dynamical system} is and how it's described. In general such systems can be regarded as \textit{black box} that for a given \textbf{input} $u(t)$ determines an \textbf{output} $y(t)$; systems can be \textbf{continuous} or \textbf{discrete-time} depending on the type of \textit{time axis scale}, but also \textbf{hybrid} systems exists (as it will be studied).
	
	\paragraph{Rabbits} In order to understand behaviours of dynamical system, let's consider the example of pairs of rabbit. If we describe with $y(t)$ the total number of rabbits couple that one person has (where $t$ is the time expressed in months), we can consider that initially a person start with no animals ($y(0) = 0$) and goes at $t=1$ at an animal shop to buy a couple of small rabbits (hence $y(1) = 1$). In the first month of life they grow up and from \textit{babies} they become \textit{adults}, but the person still has $y(2) = 1$ couple of rabbit. In the next month they procreate a new couple of rabbits and so we have $y(3) = 2$ rabbit pairs. Assuming that no rabbit dies, we can describe the evolution of the system as
	\begin{equation} \label{eq:dyn:rabbitdiscrete}
		y(n) = y(n-2) + y(n-1)
	\end{equation}
	where the term $y(n-2)$ relates to the elder population and $y(n-1)$ describes the newly borne couple from each pair of elder pair. Equation \ref{eq:dyn:rabbitdiscrete} is a \textbf{finite difference equation} that allows to recursively define the number of rabbit pairs depending on the time $t$ assuming to know the \textbf{initial conditions} $y(0)$ and $y(1)$.
	
	For such system we can define it's \de{states} $x_1(t) = y(t)$ and $x_2 = y(t-1)$ determining the vector $x(t)$:
	\begin{equation}
		x(t) = \begin{pmatrix}
			x_1(t) \\ x_2(t)
		\end{pmatrix}
	\end{equation}
	Given so the states $x(t)$, we can compute their evolution $x(t+1)$ as
	\[ x(t+1) = \vet{x_1(t+1) \\ x_2(t+1)} = \vet{y(t+1) \\ y(t)} = \vet{y(t-1) + y(t) \\ y(t)} = \vet{x_2(t) + x_1(t) \\ x_1(t)}\]
	Using linear algebra the evolution of the state can be regarded as a linear combination of the actual states, in fact
	\begin{equation}
		x(t+1) = \mat{1 & 1 \\ 1 & 0} \vet{x_1(t) \\ x_2(t)} = \A x(t)
	\end{equation}
	Having the eigenvalues of the matrix $\A$ equal to $\lambda_{1,2} = \frac{1 \pm \sqrt 5}{2}$ we will prove that the continuous explicit function that express the rabbit pairs $y$ as function of $t$ is
	\[ y(t) = \frac1{\sqrt{5}} \left[ \left(\frac{1 + \sqrt 5}{2}\right)^t - \left(\frac{1 - \sqrt 5}{2}\right)^t \right] \]
	
	\paragraph{Classification of system} This example allowed to intrude the first classification of system: they in fact can be \de{linear} if the states variation and output can be regarded as linear combination of the actual states and inputs, meaning that they can be expressed as
	\begin{equation} \label{eq:dyn:temp1}
	\begin{cases}
		x(t+1) = \A x(t) + \B u(t) \\ y(t) = \C x(t) + \D u(t)
	\end{cases}
	\end{equation}
	where $\A$ is the \textbf{dynamic matrix}, $\B$ the \textbf{input} m., $\C$ the \textbf{output} m. and $\D$ is the \textbf{feed-through} (instantaneous) m.; both $u,x$ and $y$ are in general vectors. To simplify the notation usually the dependency on the time $t$ is dropped and the increment for \textbf{discrete-time} systems (where $t\in \mathds Z$) is described as $x^+$ (values of the state at the next step), hence equation \ref{eq:dyn:temp1} is rewritten as
	\begin{equation} \label{eq:dyn:LTIdiscrete}
		\begin{cases}
			x^+ = \A x + \B u \\ y = \C x + \D u \\ x(t_0) = x_0
		\end{cases}
	\end{equation}
	For \textbf{continuous-time} systems (where $t\in \mathds R$) the concept of increment is represented by the derivative $\dot x = \frac{dx}{dt}$, hence
	\begin{equation} \label{eq:dyn:LTIcontinuous}
		\begin{cases}
			\dot x = \A x + \B u \\ y = \C x + \D u \\ x(t_0) = x_0
		\end{cases}
	\end{equation}
	Note that for all the system is very important to define the \textbf{initial condition} $x(t_0) = x_0$ of the states because the response of the output is strictly related to them.
	
	System that don't exhibits a linear behaviour are therefore called \de{non-linear} and the input/state/output relations are described by functions:
	\begin{equation}
	\begin{cases}
		\delta x = f(x,u) \\
		y = h(x,y) \\
		x(t_0) = x_0
	\end{cases}
	\end{equation}
	\begin{note}
		the term $\delta x$ is the generalization of $\dot x$ for continuous-time or $x^+$ for discrete-time systems in order to make the notation more generalized and will be used throughout the whole book.
	\end{note}
	
	Until now the notation were referred to \de{time-invariant} systems where the matrix $\A,\B\,\C,\D$ (for linear sys.) or the functions $f,h$ (for non-linear sys.) were not dependent on time, however \de{time-variant} systems also exists that exhibiting an explicit relation with time, hence can be regarded as
	\begin{equation}
		\begin{cases}
			\delta x = \A(t) x + \B(t) u \\ y = \C(t) x + \D(t) u \\ x(t_0) = x_0
		\end{cases}
		\qquad \textrm{or} \qquad 
		\begin{cases}
			\delta x = f(x,u,t) \\
			y = h(x,y,t) \\
			x(t_0) = x_0
		\end{cases}
	\end{equation}
	for both linear and non-linear case.\\
	By a mathematical point of view a system if it satisfy that
	\[ \begin{cases}
		x(t_0 + T) = x_0 \\ u(t-T) 
	\end{cases} \qquad \Rightarrow \quad y(t-T) \hspace{2cm} \forall t \geq t_0 + T \]
	
	\textbf{ESEMPI DEI DIVERSI TIPI DI SISTEMI}
	
	\begin{example}{: linear time-variant dynamical system}
		Let's consider the model of a 3 year course of an university with the goal of study the behaviour of students. Considering the graphs that follows, we denote as $u$ the number of students registering at the first year, while $\alpha_i \in [0,1]$ is the relative value of students that passes to the next year/graduates; $y$ finally represent the number of student graduated. In this case the states $x_i$ are representing the number of student in each year of  the course. This models assumes that no student drops the course (more complex models can indeed consider such phenomena).
		\begin{center}
			\tikzfigure{university}
		\end{center}
		
		We can see that this dynamical system is discrete-time (the system is considered to evolved each year at the start of the academic year); analysing the graph we observe that each next year the number of student in each \textit{class} ($x_i$) depends on the number of the one passed from the previous course ($\alpha_i x_{i-1}$) and the one that didn't passed ($(1-\alpha_i)x_i$), hence we have that
		\[\begin{cases}
			x_1^+ & = (1-\alpha_1)x_1 + u \\
			x_2^+ & = (1-\alpha_2)x_2 + \alpha_1 x_1 \\
			x_3^+ & = (1-\alpha_3)x_3 + \alpha_2 x_2 \\
			y & = \alpha_3 x_3
		\end{cases}\]	
		In this case if we assume that $\alpha_i$ are constant value, the model that we obtain doesn't suit the real behaviour of the course where problems might happen (for example a \textit{evil teacher} reduces the rate of passed students one year, reducing $\alpha_i$),	hence we have a system that's time-variant ($\alpha_i$ depends on time $t$) while the model is still linear, because it can be expressed as
		\begin{align*}
			x^+ = \vet{x_1^+ \\ x_2^+ \\ x_3^+} & = \mat{1-\alpha_1(t) & 0 & 0 \\ \alpha_1(t) & 1-\alpha_2(t) & 0 \\ 0 & \alpha_2(t) & 1-\alpha_3(t)} \vet{x_1 \\ x_2 \\ x_3} + \vet{1 \\0 \\0 } u = \A(t)x + \B(t)u \\
			y & = \mat{0 & 0 & \alpha_3(t)} \vet{x_1 \\ x_2 \\ x_3} + \mat{0} u = \C(t)x + \D(t) u
		\end{align*}
	\end{example}
	
\section{Linear time-invariant systems and transfer functions}
	As equations \ref{eq:dyn:LTIdiscrete} and \ref{eq:dyn:LTIcontinuous} stated, \de{linear time-invariant} \textbf{LTI} systems can be described in matrix form as
	\[ \begin{cases}
		\delta x = \A x + \B u \qquad x(0) = x_0\\ y = \C x + \D u 
	\end{cases} \]
	where the matrix $\A,\B,\C,\D$ are constant (hence independent from time due to the time-invariancy requirement). Having the system time invariant the initial condition can be expressed as $x(0)$ (because it doesn't depend on the choice of $t_0$, but on any general initial condition).
	
	Continuous-time LTI systems can be easily analysed in the domain of the complex variable $s$ using the \textbf{Laplace transform} $\mathscr L$, an operator defined
	\[ X(s) = \laplace{x(t)} = \int_0^\infty x(t) e^{-st}\, dt \hspace{2cm} s \in \mathds C \]
	An important property of such operator is that converts differential equations into algebraic ones in the variable $s$, in fact we can observe that
	\[ \dot x(t) \mapsto s X(s) - x(0) \]
	The Laplace transform has also the useful property of being linear, hence the input-output relation of LTI system can be transformed according to Laplace, hence
	\begin{equation}
		(eq.\ref{eq:dyn:LTIcontinuous}) \mapsto \begin{cases}
			sX(s) - x(0) = \A X(s) + \B U(s) \\ Y(s) = \C X(s) + \D U(s)
		\end{cases}
	\end{equation}
	The first equation can be solved in order to explicit the transform of the states $X(s) = \laplace{x(s)}$ as function of the transform of the input $U(s) = \laplace{u(t)}$ determining
	\begin{align*}
		sX(s) - \A X(s) & = \B U(s) + x(0) \\
		(s\I-\A) X(s) & = \B U(s) + x(0) \\
		X(s) & = (s\I - \A)^{-1} \B U(s) + (sI- \A)^{-1} x(0) 
	\end{align*}
	\begin{note}
		In the first step in order to collect $sX(s)$ and $\A X(s)$ has been necessary to multiply $s$ by the identity matrix $\I$: $s$ is in fact a scalar while $\A$ is a matrix and no operation is compatible between this elements (addition is defined only for matrix with same sizes).
	\end{note} \noindent
	With this result obtained we can compute the transform of the output $Y(s) = \laplace{y(t)}$ as function of the lonely $U(s)$ as 
	\begin{equation}
		Y(s) = \underbrace{\left(\C (s\I - \A)^{-1} \B + \D\right)}_{= \hat \G(s)} U(s) + \underbrace{\C(s\I-\A)^{-1} \D}_{\hat \psi(s)} x(0)
	\end{equation}
	In this equation $\hat \G(s)$ represent the \de{transfer function} of the system (however more generally it's a matrix). Observing that the output in the Laplace domain can be so simplified to the form $Y(s) = \hat G(s) U(s) + \hat \psi(s) x(0)$ and so by using property of the (inverse) Laplace transform we can rewrite the output as the convolution of the input sequence and the \de{impulse response} $G(t) = \mathscr L^{-1}\{\hat G(s)\}$ of the system:
	\begin{equation}
		y(t) = (G*x)(t) + \psi(t) x(0)
	\end{equation}
	where the convolution is defined as
	\[ (G*x)(t) = \int_0^t G(t)x(t-\tau)\, d\tau \]
	
	Considering the case of discrete-time LTI  systems instead of the Laplace we have to use the \textbf{Z transform} $\Z$ that maps discrete time sequences into a domain in the complex variable $z$; knowing that $X(z) = \ztransf{x(t)}$ (with $t\in \mathds Z$) the analogous property of differentiation in time is that $x(t+1) = x^+ \mapsto z X(z) - z x(0)$, hence
	\begin{equation}
		(eq.\ref{eq:dyn:LTIdiscrete} )\mapsto \begin{cases}
			z X(z) - z x(0) = \A X(z) + \B U(z) \\ Y(z) = \C X(z) + \D U(z)
		\end{cases}
	\end{equation}
	As in the continuous case it's possible to compute the transform of the output $Y(z) = \ztransf{y(t)}$ as function of the spectrum $U(z) = \ztransf{x(t)}$ as
	\begin{equation}
		Y(z) = \underbrace{\left(\C (z\I - \A)^{-1} \B + \D\right)}_{= \hat \G(z)} U(z) + \underbrace{\C(s\I-\A)^{-1} \D z}_{\hat \psi(z)} x(0)
	\end{equation}

	\paragraph{Theorem} A fundamental theorem for continuous-time linear time-invariant systems states that \textit{the \textbf{transfer function} $\hat \G(s)$ and so the \textbf{impulse response} $\G(t)$ of such systems is computed as}
	\begin{equation}
		\hat \G(s) = \left(\C (s\I - \A)^{-1} \B + \D\right) \hspace{2cm} \G(s) = \antilaplace{\left(\C (s\I - \A)^{-1} \B + \D\right)}
	\end{equation}
	
\section{Interconnected systems}
	The control of dynamical systems is based on \de{interconnections} between, and such connection can be summarized as: in parallel, in series, in feedback. In this section a proof of transfer function of interconnected system is given in the case of linear time-invariant system (where the concept of transfer function is \textit{well defined}). Such system represented as 
	\[ \begin{cases}
		\delta x = \A x + \B u \qquad x(0) = x_0\\ y = \C x + \D u 
	\end{cases} \]
	are so characterized by the 4 matrix $\A,\B,\C,\D$ by the \textit{big matrix notation}
	\[ \vet{\delta x \\ \hline y} = \begin{bmatrix}
		\begin{array}{c | c}
			\A & \B \\ \hline \C & \D
		\end{array} 
	\end{bmatrix} \vet{x \\ u }\]
	
	\subsection*{Parallel connection}
		\begin{figure}[b!]
			\centering
			\tikzfigure{parallel}
			\caption{black box representation of a parallel connection between two systems.} \label{fig:dyn:parallel}
		\end{figure}
		Considering the parallel connection of the systems $\Sigma_1,\Sigma_2$ (figure \ref{fig:dyn:parallel}), the overall transfer function of the system $\Sigma$ can be computed considering that the output $Y(\cdot)$ in the Laplace/Z domain is the sum of the contribution of the single stages that can be computed so independently
		\begin{equation}
		\begin{aligned}
			Y &  = Y_1 + Y_2 = \hat \G_1 U_1 + \hat \G_2 U_2 = \hat \G_1 U + \hat G_2 U \\
			& = \underbrace{\big(\hat \G_1  + \hat \G_2\big)}_{=\hat \G} U
		\end{aligned}
		\end{equation}
		
		While connection two or more systems we have that the overall states $x$ of the system $\Sigma$ is the union of the individual systems $x_i$, hence in this case
		\[ x = \vet{x_1 \\ x_2} \]
		More formally the we have that the variation of the states $\delta x$ of the system $\Sigma$ can be regarded as
		\[ \delta x = \A_1 x_1 + \B_1 u_1 + \A_2 u_2 + \B_2 u_2 = \mat{\A_1 & 0 \\ 0 & \A_2} \vet{x_1 \\ x_2} + \mat{\B_1 \\ \B_2} u \]
		and similarly the output in the time domain
		\[ y = \C_1 x_1 + \D_1 u_1 + \C_2 x_2 + \D_2 x_2 = \mat{\C_1 & \C_2} \vet{x_1 \\ x_2} + \vet{D_1+D_2} u \]
		The associated \textit{big matrix} is so
		\begin{equation} 			
			\vet{\delta x_1 \\ \delta x_2 \\ y} = \mat{\begin{array}{c c | c}
					\A_1 & 0 & \B_1 \\ 0 & \A_2 & \B_2 \\ \hline \C_1 & \C_2 & \D_1 + \D_2
			\end{array}} \vet{x_1 \\ x_2 \\ u}
		\end{equation}
	
	\subsection*{Series (cascade) connection}
		A series connection (figure \ref{fig:dyn:series}) between two system is implemented by feeding the input of $\Sigma_2$ with the output of $\Sigma_1$; in this case the input-output of the overall system can be regarded in the Laplace/Z transform as
		\begin{equation}
		\begin{aligned}
			Y & = \hat \G_2 U_2 = \hat \G_2 Y_1 = \hat \G_2 \hat \G_1 U_1 \\ & = \hat \G_2 \hat \G_1  U = \hat \G U
		\end{aligned}
		\end{equation}
		Note that in this case the order of the transfer function $\hat \G= \hat \G_2 \hat \G_1$ is important due to the multiplication of matrix; in fact in order to make the series connection possible we need to have that the length of the vector $y_1$ is equal to the one of $x_2$: this makes possible to have a matrix $\hat \G$ the can be multiplied by $U$ and returns a proper $Y$.
	
		Carefully representing this system in the space of the states, we obtain that
		\begin{align*}
			\delta x_1 & = \A_1 x_1 + \B_1 u \\
			y_1 & = \C_1 x_1 + \D_1 u = u_2 \\
			\delta x_2 & = \A_2 x_2 + \B_2 u_2 = \A_2 x_2 + \B_2\big( \C_1 x_1 + \D_1 u \big) \\
			y= y_2 & = \C_2 x_2 + \D_2 u_2 = \C_2 x_2 + \D_2 \big(\C_1 x_1 + \D_1 u\big)
		\end{align*}
		hence the \textit{big matrix} is
		\begin{equation}
			\mat{ \begin{array} {c c | c}
					\A_1 & 0 & \B_1 \\ 
					\B_2 \C_1 & \A_2 & \B_2 \D_1 \\ \hline 
					\D_2 \C_1 & \C_2 & \D_2\D_1
			\end{array} }
		\end{equation}
		
		\begin{figure}[bht]
			\centering
			\tikzfigure{series}
			\caption{black box representation of a series connection between two systems.} \label{fig:dyn:series}
		\end{figure}
	
	\subsection*{Feedback connection}	
		\begin{figure}[bt]
			\centering
			\tikzfigure{feedback}
			\caption{black box representation of a series feedback connection of a system.} \label{fig:dyn:feedback}
		\end{figure}
		
		A feedback connection (figure \ref{fig:dyn:feedback}) of a system is realised by subtracting to the input $u$ of the system it's output $y$. Performing the analysis in the frequency domain we have have in fact the solution
		\[ Y = \hat\G_1 U_1 = \hat\G_1 (U-Y) \]
		We see that this expression is implicit in $Y$, but we can still obtain the solution
		\begin{equation}
		\begin{split}
			Y + \hat\G_1 Y = Y(\I + \hat \G_1)& =\hat\G_1 U \\
			Y &= (\I + \hat \G_1)^{-1} \hat \G_1 U
		\end{split}
		\end{equation}
		This solution is obtained by inverting the matrix $\I + \hat \G_1$ and this operation is possible if the matrix is \textit{\de{well-posed}}, condition that happens every time the matrix $\I + \D_1$ is invertible. By performing the analysis in the states space we have in fact 
		\begin{align*}
			y & = \C_1 x_1 + \D_1 u_1 = \C_1 x_1 + \D_1 (u-y) \\
			(\I + \D_1) y & = \C_1 x_1 + \D_1 u \\
			y & = (\I + \D_1)^{-1}\C_1 x_1 +(\I + \D_1)^{-1} \D_1 u
		\end{align*}
		From this expression is possible to observe why the well-posedness condition is related to having $\I + \D_1$ invertible. Expliciting the variation of the states we have instead we have
		\begin{align*}
			\delta x & = \A_1 x_1 + \B_1 u_1 = \A_1x_1 + \B_1 (u-y) \\
			& = \A_1 x_1 + \B_1 u - \B_1 \left( (\I + \D_1)^{-1}\C_1 x_1 +(\I + \D_1)^{-1} \D_1 u \right) \\
			& = \left( \A_1 - \B_1 (\I+\D_1)^{-1} \C_1 \right) x_1 + \left( \B_1 - \B_1 (\I+\D_1)^{-1} \D_1 \right)u
		\end{align*}
		From this states space expansion we can define the \textit{big matrix} associated to a feedback connection as
		\begin{equation}
			\mat{ \begin{array} {c | c}
				\A_1 - \B_1(\I+\D_1)^{-1} \C_1 & \B_1\left(\I - (\I+\D_1)^{-1} \D_1\right) \\ \hline 
				(\I+\D_1)^{-1}\C_1 & (\I+\D_1)^{-1} \D_1
			\end{array} } = 
			\mat{ \begin{array} {c | c}
				\A_1 - \B_1(\I+\D_1)^{-1} \C_1 & \B_1 (\I+\D_1)^{-1} \\ \hline 
				(\I+\D_1)^{-1}\C_1 & \D_1 (\I+\D_1)^{-1} 
		\end{array} }
		\end{equation}
	
\section{Realization theory}
	Given a rational polynomial $g(s)$ so defined as the ratio between two polynomial $n(s)/d(s)$ (this expression indeed represent the case of a one dimensional transfer function), such function is said
	\begin{itemize}
		\item \textbf{strictly proper} if the degree $\#n(s)$ of the denominator is less then the degree $\#d(s)$ od the denominator; this means that
		\[ \lim_{s\rightarrow \infty} g(s) = 0 \]
		\item \textbf{proper} if in general we have $\#n(s) \leq \#d(s)$, but more specifically it has to be
		\[ \lim_{s\rightarrow \infty} g(s) \neq \infty \]
		\item every time we have $\#n(s) > \#d(s)$, the rational polynomial is defined as \textbf{improper} and has
		\[ \lim_{s\rightarrow \infty} g(s) = \infty \]
	\end{itemize}	
	A rational polynomial $G(s)$ is \textbf{biproper} if both $g$ and it's inverse $g^{-1}$ are propers. Extending this concepts to a transfer function $\hat \G(s)$ (that's in general a matrix), the system is (strictly) proper if all it's entries are (strictly) proper; if one entry is improper, then $\hat \G(s)$ is also improper.
	
	In general it's proven that for linear time-invariant systems the transfer function $\hat G(s)$ is composed by a strictly proper and a proper term as
	\begin{equation} \label{eq:dyn:temp2}
		\hat \G(s) = \underbrace{\C(s\I - \A)^{-1}\B}_\textrm{strictly proper} + \underbrace{\D}_\textrm{proper}
	\end{equation}
		
	\paragraph{Realization} Given a transfer function $\hat \G(s)$, we say that the linear time-invariant system in the form
	\[ \begin{cases}
		\delta x = \A x + \B u \qquad x(0) = x_0\\ y = \C x + \D u 
	\end{cases} \] 
	is a \de{realization} of the transfer function $\hat G$ if equation \ref{eq:dyn:temp2} holds. \\
	This states that a LTI system characterized by the matrices $\A,\B,\C,\D$ is the realization of $\hat G(s)$ if it happens that $\hat G(s) = \C(s\I - \A)^{-1}\B + \D$. We also define two realization as \de{zero state equivalent} if they realise the same transfer function $\hat G(s)$.
		
	\subsection*{Realizations and transfer functions}
		An important theorem states that a transfer function $\hat \G(s)$ can be \textbf{realised} by an LTI system in the \textit{standard form} \textbf{if and only if} the rational function $\hat \G(s)$ is \textbf{proper}.
		
		\paragraph{Proof pt. 1} We can prove the \textit{only if} conditions stating
		\[ \mat{\begin{array}{c|c}
				\A & \B \\ \hline \C & \D
		\end{array}} \mapsto \hat \G(s) \]
		the proof is straightforward: as \ref{eq:dyn:temp2} states, given the constant matrices $\A,\B,\C,\D$ the resulting transfer function is strictly proper for the term $\C(s\I-\A)^{-1}\B$ and becomes proper only in the case when $\D\neq 0$.
		
		\paragraph{Proof pt. 2} More complex is proving the \textit{if} condition that starting from the transfer function computes the \textit{characteristic} matrices of LTI systems:
		\[ \hat \G(s) \mapsto \mat{\begin{array}{c|c}
				\A & \B \\ \hline \C & \D
		\end{array}}  \]
		We want now to prove that if the transfer function $\hat G(s)$ is proper then it state space representation exists. Determining the non-strictly proper element $\D$ is straightforward, in fact it can be computed simply as the limit
		\[ \mathcal D = \lim_{s\rightarrow \infty} \hat \G(s) \]
		We can so build the strictly proper transfer function as
		\[ \hat \G_{sp}(s) = \B(s\I - \A)^{-1}\C = \Gs - \D\]
		Building the polynomial $d(s)$ as the least common denominator of all the entries in the matrix $\hat \G_{sp}(s)$ and writing him in a form
		\[ d(s) = s^n + \alpha_1 s^{n-1} + \alpha_2 s^{n-2}+ \dots + \alpha_{n-1} s + \alpha_n \]
		we can rewrite the strictly proper part of the transfer function as a combination of matrices $N_i$ in the following way:
		\[ \hat \G_{sp}(s) = \frac{N(s)}{d(s)} = \frac{N_1 s^{n-1} + N_2 s^{n-2} + \dots + N_{n-1} s + N_n }{d(s)} \]
		With that said we have all the elements to compute the state space representation of the system: it can be proven in fact that the LTI has a state space representation in the form
		\begin{equation} \label{eq:dyn:Gtostaterep}
			\mat{\begin{array}{ c c c c | c}
				-\alpha \I_k & - \alpha_2 \I_k & \dots & - \alpha_n \I_k & \I_k \\
				\I_k & 0 & \dots & 0 & 0 \\
				0 & \ddots & & 0 & \vdots \\
				0 & 0& \I_k & 0 & 0 \\ \hline
				N_1 & N_2 & \dots & N_m & \D
			\end{array}}
		\end{equation}
		where $\I_k$ is the $k\times k$ identity matrix; this notation is called \de{controllable canonical form}.
		
		\begin{example}{: state representation from a transfer function}
			Given a system characterized by a transfer function
			\[ \Gs = \mat{\frac{4s-10}{s+1} & \frac{3}{s+2} \\ \frac{1}{s+2} & \frac{4}{s+1} } \]
			in order to compute one it's representation the first thing to do is extract the output matrix $\D$ as
			\[ \D = \lim_{s\rightarrow \infty} \Gs = \mat{4 & 0 \\ 0 & 0} \]
			And so we can compute the strictly  proper part as
			\begin{align*}
				\hat G_{sp}(s) & = \Gs - \D = \mat{\frac{-14}{s+1} & \frac{3}{s+2} \\ \frac{1}{s+2} & \frac{4}{s+1} } \\
				& = \frac 1 {(s+1)(s+2)} \mat{-14(s+2) & 3 (s+1) \\ 1(s+1) & 4(s+2)} = \frac 1{s^2 + 3s + 2} \mat{-14 s - 28 & 3s + 3 \\ s+1 & 4s + 8} \\
				& = \frac{\mat{-14 & 3 \\ 1 & 4}s + \mat{-28 & 3 \\ 1 & 8} }{s^2 + 3s +2}
			\end{align*}
			Using equation \ref{eq:dyn:Gtostaterep} we can compute the state representation of the system using the yet computed denominator and matrices $N_i$ providing the result
			\[ \mat{\begin{array}{c c c c | c c}
				-3 & 0 & -2 & 0 & 1 & 0 \\
				0 & -3 & 0 & -2 & 0 & 1 \\
				1 & 0 & 0 & 0 & 0 & 0 \\
				0 & 1 & 0 & 0 & 0 & 0 \\ \hline
				-14 & 3 & -28 & 3 & 4 & 0 \\ 
				1 & 4 & 1 & 8 & 0 & 0
			\end{array}} \]
			By computing $\C (s\I - \A)^{-1} \B + \D$ using the matrices yet obtain we can indeed see that, after some simplifications, that we obtain the original transfer function $\Gs$.
		\end{example}
		
		\paragraph{Proof pt. 3} All that's left now is to prove that equation \ref{eq:dyn:Gtostaterep} indeed is a realization of the proper transfer function $\Gs$. In order to do so we consider we consider the sub-matrix $z(s)$ defined as
		\[ z(s) = \mat{z_1^t & z_2^t & \dots & z_n^t} = (s\I - \A)^{-1} \B \qquad \Rightarrow \qquad (s\I-\A) z(s) = \B \]
		Considering the \textit{lower part} of the matrices $\A$ and $\B$ (the last 3 rows in equation \ref{eq:dyn:Gtostaterep}), the multiplication results in the set of equalities
		\[ s z_2 -z_1 = 0 \qquad sz_3 - z_2 = 0 \qquad \dots \qquad sz_n = z_{n-1} \]
		Considering so that $z_n = \frac 1 s z_{n-1}$ but also $z_{n-1} = \frac 1 s z_{n-2}$ up to $z_2 = \frac 1 2 z_1$ we can determine the generic vector $z_k$ as
		\begin{equation} \label{eq:dyn:temp3}
			z_k = \frac 1{s^{k-1}} z_1
		\end{equation}
		Considering so now the multiplication $(s\I - \A)z(s) = \B$ using the top row of $\A,\B$ respect to the formulation in equation \ref{eq:dyn:Gtostaterep}, we have that
		\[ (s+\alpha_1) z_1 + \alpha_2 z_2 + \dots + \alpha_n z_n = \I_k \]
		hence for relation \ref{eq:dyn:temp3}
		\[ \left( s + \alpha_1 + \frac{\alpha^2}{s} + \dots + \frac{\alpha_{n-1}}{s^{n-2}} + \frac{\alpha_n}{s^{n-1}} \right) z_1 = \I_k \]
		Since the polynomial in the left-hand side is given by the ratio $\frac{d(s)}{s^{n-1}}$ we can define the sub-matrix $z$ as
		\[ z(s) = \mat{z_1 \\ z_2 \\ \vdots \\ z_n} = \frac 1 {d(s)} \mat{s^{n-1}\I_k \\ s^{n-2}\I_k \\ \vdots \\ \I_k} \]
		hence
		\begin{align*}
			\C(s\I-\A)^{-1} \B & = \C z(s) = \frac 1{d(s)} \mat{N_1 & N_2 & \dots  & N_n} \mat{s^{n-1}\I_k \\ s^{n-2}\I_k \\ \vdots \\ \I_k} \\
			& = \hat \G_{sp}(s)
		\end{align*}
		
		\subsubsection{LTI SISO system case and multiple realizations}		
		 A SISO (single-input single-output) LTI system is characterized by a transfer function that's simply a rational polynomial in the form
		\[ \Gs = \frac{\beta_1 s^{n-1} + \beta_2 s^{n-2} + \dots + \beta_{n-1} s + \beta_ m}{s^n + \alpha_1 s^{n-1} + \alpha_2 s^{n-2}+ \dots + \alpha_{n-1} s + \alpha_n } \]
		Recalling the results of equation \ref{eq:dyn:Gtostaterep}, a realization of this kind of system is characterized by the matrices
		\[ \A = \mat{-\alpha_1 & - \alpha_2 & \dots & - \alpha_{n-1} & - \alpha_n \\
		1 & 0 & \dots & 0 & 0 \\
		0 & 1 & \dots & 0 & 0 \\ 
		\vdots & \vdots & \ddots & \vdots & \vdots \\
		0 & 0 & \dots & 1 & 0 } \qquad \B = \mat{1 \\ 0 \\ \vdots \\ 0 \\ 0} \qquad \C \mat{\beta_1 & \beta_2 & \dots & \beta_{n-1} & \beta_n} \]
		Figure \ref{fig:dyn:controllablecanonical} shows how a system described in a controllable canonical representation (as in this case) can be graphed.
		
		\begin{figure}[bht]
			\centering \resizebox{0.9\linewidth}{!}{\tikzfigure{controlform}}
				\caption{graph representation of an LTI SISO system expressed in the controllable canonical form; the $1/s$ blocks in practise are representing integrators of the input.} \label{fig:dyn:controllablecanonical}
		\end{figure}
		
		In general this is not the lonely allowable representation of the transfer function $\Gs$, but there exists infinitely many ones. Considering $T$ as a non-singular matrix, we can define a new vector of the states $\tilde x = T x$ (hence $x = T^{-1} \tilde x$) that allows us to determine a new state space representation of the system, in fact we can build
		\begin{equation*}
		\begin{cases}
			\delta \tilde x = T \, \delta x  = T \A x + T \B u = T A T^{-1} \tilde x + T\B u = \tilde \A \tilde x + \tilde \B u \\
			\tilde y = \C x + \D u = \C T^{-1}\tilde x + \D u = \tilde \C \tilde x + \D u
		\end{cases}
		\end{equation*}
		We can so state that the systems
		\begin{equation}
			\mat{\begin{array}{c|c}
					\tilde \A & \tilde \B \\ \hline \tilde \C & \D
			\end{array}} = \mat{\begin{array}{c|c}
				T\A T^{-1} & T \B \\ \hline \C T^{-1} & \D
			\end{array}} \qquad \textrm{and} \qquad \mat{\begin{array}{c|c}
				\A & \B \\ \hline \C & \D
			\end{array}}
		\end{equation}
		are \de{algebraically equivalent} and are indeed representations of the same transfer function $\Gs$. In this case the non-singular matrix $T$ is referred as \textbf{\textit{similarity/equivalence transformation}} and allows to describe the system into a different set of states. This operation can be useful because it might allow to express a system in a more suitable way for numerical computations.
		
		An example of algebraically equivalent representation is the \de{observable canonical form} described as
		\begin{equation}
		\mat{\begin{array}{ c c c c | c}
			-\alpha_1 & 1 & 0 & 0 & \beta_1 \\
			\vdots & 0 & \ddots & 0 & \vdots  \\
			-\alpha_{n-1} & 0 & 0 & 1 & \beta_{n-1} \\
			-\alpha_n & 0 & \dots & 0 & \beta_n \\ \hline
			1 & 0 & \dots & 0 
		\end{array}}
		\end{equation}
	
	
		\begin{figure}[bht]
			\centering \resizebox{0.9\linewidth}{!}{\tikzfigure{observableform}}
			\caption{graph representation of an LTI SISO system expressed in the controllable canonical form; the $1/s$ blocks in practise are representing integrators of the input.} \label{fig:dyn:observablecanonical}
		\end{figure}
		